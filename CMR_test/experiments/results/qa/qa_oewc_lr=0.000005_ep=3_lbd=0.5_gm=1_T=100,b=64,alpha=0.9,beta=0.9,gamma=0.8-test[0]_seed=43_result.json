{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_upstream_model/best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=0.000005_ep=3_lbd=0.5_gm=1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8-test[0]_seed=43', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=0.5, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-06, max_grad_norm=0.1, num_epochs=3.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=25, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=2, result_file='experiments/results/qa/qa_oewc_lr=0.000005_ep=3_lbd=0.5_gm=1_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8-test[0]_seed=43_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8-test.json', task_name='mrqa', train_batch_size=2, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4239, "online_eval_results": [{"timecode": 0, "UKR": 0.861328125, "KG": 0.4234375, "before_eval_results": {"predictions": ["the ease with which people, youth in particular, can obtain controlled substances", "\u20ac27,814", "serious depopulation and permanent change in both economic and social structures", "increased", "Yellow River", "Infrastructure", "Climate fluctuations", "best teachers", "the Episcopal (United States) Calendar of Saints", "ten", "England", "Walt Disney", "Sky+ PVR", "Pannerdens Kanaal", "lost in the 5th Avenue laboratory fire of March 1895", "the comprehensive institutions of the Great Yuan", "Sir Isaac Newton", "1421", "Ghazan Khan", "two Japanese research teams", "design build", "the basis for most separation results of complexity classes", "113 research centers", "DTIME(f(n)", "the Kuz Laksh curve", "proteolysis", "a suite of network protocols created by Digital Equipment Corporation", "Stephen Kemble", "NASA immediately convened an accident review board", "decline of organized labor", "Coptic Cathedral", "energy", "24 August \u2013 3 October 1572", "prep schools", "Matthew 16:18", "Science", "late 2008", "NFL Mobile", "ABC", "NBC", "18 of 26", "1879", "Henry Cole", "5K", "Donald Davies", "The Premier", "the AKS primality test", "ink", "Wahhabism", "Mohawk Chief Hendrick", "efficiency", "Spanish", "charter schools", "Europe itself", "the devolved competencies", "53,000", "the public interest", "Manned Spacecraft Center", "1521", "a nationwide network", "1978", "Jim Nantz and Phil Simms", "beat", "the original force"], "metric_results": {"EM": 0.890625, "QA-F1": 0.9169642857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1685", "mrqa_squad-validation-7889", "mrqa_squad-validation-7379", "mrqa_squad-validation-3932", "mrqa_squad-validation-2386", "mrqa_squad-validation-8719", "mrqa_squad-validation-4731"], "SR": 0.890625, "CSR": 0.890625, "EFR": 0.7142857142857143, "Overall": 0.8024553571428572}, {"timecode": 1, "before_eval_results": {"predictions": ["11", "Benjamin Lamme", "Off-Off Campus", "1857", "VideoGuard pay-TV", "Nintendo", "three", "advanced research and education networking in the United States", "De Marcus Ware", "Anabaptists", "1954", "2006", "international recognition", "eight-year", "a cubic interpolation formula", "the League of Nations", "1500 and 1850", "Michael Eisner", "otter, beaver and hundreds of bird species", "fucoxanthin dinophyte", "inertia", "physical", "English", "a much larger conflict between France and Great Britain", "Beyonc\u00e9 and Bruno Mars", "widespread education", "architect", "poverty, the lack of access to education and weak government institutions", "Lunar Roving Vehicle (LRV)", "June 6, 1951", "prevent growth by limiting aggregate demand", "the New York Times", "Oh, what joy", "1726", "King of England", "North Carolina and New Mexico", "various disciplines of pharmacy", "Danny Lane", "Public-Private Partnering", "magazines and journals", "antisemitic", "7.8%", "Sir William Henry Bragg and William Lawrence Bragg", "the father of the house", "1936", "shamanist, Buddhist or Christian", "corrosion", "polynomial-time", "1269", "Because everyday clothing from previous eras has not generally survived", "15 May 1525", "Samarkand", "1962", "external", "15 miles", "Drogo", "Cadeby", "seventh", "6 and \"cxxii\" means 122", "Germany", "toad", "Sheryl Crow", "an ancient optical illusion toy", "Ku - Klip"], "metric_results": {"EM": 0.78125, "QA-F1": 0.803125}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1355", "mrqa_squad-validation-809", "mrqa_squad-validation-5547", "mrqa_squad-validation-2485", "mrqa_squad-validation-673", "mrqa_squad-validation-7430", "mrqa_squad-validation-2242", "mrqa_squad-validation-6779", "mrqa_squad-validation-2525", "mrqa_squad-validation-1764", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-3348", "mrqa_naturalquestions-validation-2698"], "SR": 0.78125, "CSR": 0.8359375, "EFR": 0.5, "Overall": 0.66796875}, {"timecode": 2, "before_eval_results": {"predictions": ["Search the Collections", "ten years", "to coordinate the response to the embargo", "Fran\u00e7ois, Duc d'Alen\u00e7on", "luxurious parks and royal gardens", "more than $45,000", "a licensing deal", "Brock Osweiler", "USSR", "cellular respiration", "Priestley", "1989", "December 3", "third largest", "Art Library", "Gini index", "Archbishop of Trier", "Albert C. Outler", "factorial primes", "branching", "\u00a3304m", "monthly subscription", "banned the growing of coffee", "monophyletic", "graze", "William of Orange", "2007", "James Gamble", "\u221211.7 \u00b0C", "Shi Bingzhi", "Islam", "MCA Inc.", "negative correlation between spatial and spin variables", "1130", "Richard Trevithick and, separately, Oliver Evans", "fundamental rights recognised and protected in the constitutions of member states", "151 votes", "west of Kashgar", "economic", "mouth and pharynx", "Civil disobedience", "captured", "vacuum swing adsorption", "black caiman", "detrimental", "six", "starch", "workforce consultation in businesses", "The Literary and Philosophical Society of Newcastle upon Tyne", "power", "flammable", "90%", "blue", "Louis Le Vau", "Leonard Rossiter", "the Great Chicago Fire", "Sarajevo", "Republic of Fiji", "James Murdoch", "Lehman Bros International (Europe)", "Terrence Malick", "Missouri", "Phidias", "The soup Nazi"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8643914473684211}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1312", "mrqa_squad-validation-8982", "mrqa_squad-validation-8288", "mrqa_squad-validation-10452", "mrqa_squad-validation-3332", "mrqa_squad-validation-3676", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-4089", "mrqa_triviaqa-validation-2701"], "SR": 0.828125, "CSR": 0.8333333333333334, "EFR": 0.36363636363636365, "Overall": 0.5984848484848485}, {"timecode": 3, "before_eval_results": {"predictions": ["Christopher Hay and Douglas Coyne", "11,000", "violence", "Commission v Austria", "1985", "Mongols and Semuren", "head of state and head of government", "clinical pharmacists", "Marek Belka", "internet", "platyctenids", "1869", "multiple revisions", "Nepali", "MPEG-4", "three", "double membrane with an intermembrane space and phycobilin", "public (government) funding", "University of Chicago campus", "Sky Three", "Roman", "algae", "color confinement", "East Asia", "Pannerdens Kanaal", "land-based reinforcements", "Kevin Harlan", "Home Improvement", "microorganisms", "closed", "2\u20137 \u00b0C (4\u201313 \u00b0F)", "cholera", "1487", "1560", "the assassination of US President John F. Kennedy", "$414 million", "the level of the top tax rate", "double", "V&A Museum of Childhood", "red brick and Portland stone", "Western", "environmental determinism", "John D. Rockefeller", "the connection id in a table", "3,837", "Uighur King of Qocho", "34,000", "light pencil detector", "Hartford", "international", "The Comitium", "bison", "Rudolf Hess", "David Bowie", "St Moritz", "red eye logo", "\"Upside Down\"", "President Garfield", "Bachelor of Arts", "Nunc dimittis", "CAR DEL (1945), ALLEGRO (1947),", "Arizona", "Virgin America", "$7.8 million"], "metric_results": {"EM": 0.75, "QA-F1": 0.79296875}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4310", "mrqa_squad-validation-7896", "mrqa_squad-validation-8593", "mrqa_squad-validation-6891", "mrqa_squad-validation-10273", "mrqa_squad-validation-3131", "mrqa_squad-validation-2908", "mrqa_squad-validation-8246", "mrqa_triviaqa-validation-591", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-6370"], "SR": 0.75, "CSR": 0.8125, "EFR": 0.4375, "Overall": 0.625}, {"timecode": 4, "before_eval_results": {"predictions": ["had never received a formal education", "actions-oriented", "Hunters Hill", "France", "18,000", "one hundred", "Dolby Digital", "Leonardo da Vinci", "Clair Cameron Patterson", "student-teacher relationships", "Pierre-Auguste Renoir", "Prejin grace", "1823", "75,000 to 100,000", "governmental entities", "Ice Ages", "the 2004 Treaty establishing a Constitution for Europe", "1887", "Ismailiyah, Egypt", "the increase in tea drinking", "Apollo 8", "VHF channel 7", "to reduce costs and maximize profits", "infrequent rain", "England", "3,837", "13", "11 points", "technology", "western European", "1815", "3, 4, & 5", "31 October 1517", "primes", "Philip Roth", "poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies", "3\u20132.7 billion years ago", "Sava Kosanovi\u0107", "Chagatai and Jochi", "uncertainty", "1929", "$5 million", "Cupid", "Harriet Tubman", "Knickerbocker Glory", "semitones", "Jonathan Swift", "tax collector", "Greek", "Felix Mendelssohn", "Tractors", "Hedonismbot", "1948", "Benedict meaning", "the skull", "hung-over", "salty", "Guinea", "ghee - Sesli S\u00f6zl\u00fck", "American History X", "Chamberlain holds the single - game scoring record, having scored 100 in game in 1962.", "March 23, 2017", "Roman Catholic", "MASTERPIECE THEATRE"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7442708333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.13333333333333336, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1256", "mrqa_squad-validation-7020", "mrqa_squad-validation-9803", "mrqa_squad-validation-3985", "mrqa_squad-validation-937", "mrqa_squad-validation-260", "mrqa_squad-validation-1880", "mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-4449", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-4253", "mrqa_naturalquestions-validation-6668", "mrqa_searchqa-validation-12335"], "SR": 0.71875, "CSR": 0.79375, "EFR": 0.4444444444444444, "Overall": 0.6190972222222222}, {"timecode": 5, "before_eval_results": {"predictions": ["six", "HD", "50-yard line", "females", "one hundred", "drowned", "26", "1920", "3D printing technology", "internal", "Santa Clara Marriott", "15", "1636", "ten", "Zhu Yuanzhang", "June 4, 2014", "Woodward Park", "seizures", "Galileo Galilei", "1961", "Shiphrah and Puah", "NL and NC", "November 28, 1995", "Egyptians", "Planet of Giants", "seven", "2p \u2212 1", "individual state laws", "European Parliament and the Council of the European Union", "hard-to-fill", "Bukhara", "the Castle Church in Wittenberg", "power of expulsion", "attacks on Jews", "the Romantic Rhine", "San Diego", "palm", "Queen bees", "Arctic Monkeys", "David Hockney", "Maine", "Gambia", "Ireland", "Conservative and Liberal Democrat coalition", "Sir Henry Neville", "Australia", "PHYSICS", "Canada", "sheep", "Canada", "watchmaking", "Tswalu Kalahari", "Chepstow", "dragonflies", "Bristol", "nine", "1959", "Charlotte Bronte", "hopeful expectation", "Hellenismos", "Sweden, Norway and Denmark", "two", "1999", "Nelson"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7609375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4572", "mrqa_squad-validation-7112", "mrqa_squad-validation-614", "mrqa_triviaqa-validation-2503", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-3570", "mrqa_naturalquestions-validation-4903", "mrqa_hotpotqa-validation-5415", "mrqa_newsqa-validation-3590", "mrqa_searchqa-validation-13950", "mrqa_hotpotqa-validation-3843"], "SR": 0.734375, "CSR": 0.7838541666666666, "EFR": 0.5882352941176471, "Overall": 0.6860447303921569}, {"timecode": 6, "before_eval_results": {"predictions": ["Anglican", "Turner Broadcasting System", "1920", "more efficient solutions", "2,000", "by experience", "specific catechism", "all the normal forms of parental discipline", "Sainte Foy in Quebec", "illegal boycotts", "several years", "England", "three hundred sixty", "Roone Arledge", "10 o'clock", "Missy", "That Special feeling", "6.04 milliliters", "the oxygen cycle", "Paul Marin de la Malgue", "high", "a place where justice resides", "overinflated", "2006", "Business Connect", "Brock Osweiler", "1990s", "tablets", "two", "25", "250,000 feet", "environmental factors", "Seattle Seahawks", "1982", "Mad Hatter", "Fenn Street School", "Wild Atlantic Way", "Jimmy Perry and David Croft", "abacus", "Dionysus", "Lowestoft", "Jackson Pollock", "Toronto", "Isotopes", "Picasso", "pianissimo", "Easter", "James Van Allen", "April", "Help", "Egypt", "U.S. Marshals", "\u201cEATME\u201d", "Tasmania", "motorway junction J33", "Bruce Wayne", "Biafra", "Fluorine", "Patricia Rom", "2016\u201317", "A Whiter Shade of Pale", "Christmas Eve", "Las Vegas", "Daniel Paul Johns"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7817708333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-6848", "mrqa_squad-validation-7818", "mrqa_squad-validation-2263", "mrqa_squad-validation-523", "mrqa_squad-validation-4762", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7577", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-131", "mrqa_naturalquestions-validation-989", "mrqa_hotpotqa-validation-5723", "mrqa_searchqa-validation-8377"], "SR": 0.734375, "CSR": 0.7767857142857143, "EFR": 0.29411764705882354, "Overall": 0.5354516806722689}, {"timecode": 7, "before_eval_results": {"predictions": ["attacked the British column", "higher returns", "three", "the Supreme Court of the United Kingdom", "the Los Angeles Times", "al-Gama'a al-Islamiyya", "1413", "imperialism", "Emmerich Rhine Bridge", "296", "inhlogisticated air", "all numbers up to n = 2 \u00b7 1017", "Festival of Britain", "1636", "\"Christ and His salvation\"", "electric current", "Lake Balkanian", "15th century", "Litton's Weekend Aventure", "photosynthesis", "fossil sequences", "Lucas\u2013Lehmer test", "Ruhr", "587,000 square kilometres", "uncertainty", "Thomas Edison", "Soviet Union", "kinematic measurements", "King George's War", "presidential representative democratic republic", "agricola", "\"The Blind Side\"", "microwave oven", "Corvidae", "Killer whale", "balustrade", "Thom Yorke", "love", "aradne\u2019s Dancing-Floor", "the end of the second month in the uterus", "dragonflies", "Soham", "Polovtsian March", "actress and a former fashion model", "Anne Elliot", "Argentina", "ky", "bach ist der Vater, wir sind die Buben", "Bellamy", "Roshi", "\"Fur elise\"", "Paddington Bear", "Rocky Marciano", "The Raj Quartet", "Philadelphia Eagles", "William Plomer", "Jimmy Carter", "Nathan Hale", "vigorish", "40", "longest-running educational promotion", "Daniel Espinosa", "NFL single-season touchdown reception record", "1982"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7278645833333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3573", "mrqa_squad-validation-6135", "mrqa_squad-validation-6197", "mrqa_squad-validation-4256", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-1847", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-7657", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-5478", "mrqa_hotpotqa-validation-2092", "mrqa_searchqa-validation-12187", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-4806"], "SR": 0.6875, "CSR": 0.765625, "EFR": 0.15, "Overall": 0.4578125}, {"timecode": 8, "before_eval_results": {"predictions": ["electric lighting", "hundreds", "a problem", "2003", "same-gender marriages", "exlposion", "high risk preparations and some other compounding functions", "civil disobedience", "2009", "second", ", while both civil disobedience and civil rebellion are justified by appeal to constitutional defects, rebellion is much more destructive", "lactobacilli", "the A1 (Gateshead Newcastle Western Bypass)", "heterokontophyte", "45\u201360 nanometers across", "Robert Maynard Hutchins", "photons", "chantrey, John Gibson, Edward Hodges Baily, Lord Leighton, Alfred Stevens, Thomas Brock, Alfred Gilbert, George Frampton, and Eric Gill", "CBS", "primary law, secondary law and supplementary law", "graph isomorphism problem", "August 10, 1948", "modern", "13 June 1525", "lunar geology training", "many celebrated seasons", "the \"blurring of theological and confessional differences in the interests of unity.\"", "Super Bowl Opening Night", "Betelgeuse", "Rev", "Ecuador", "Lab\u00e8que", "Dead Belgians Don\u2019t Count", "Vinegar Joe", "Stan Butler", "buried alive", "Abraham Lincoln Grover Cleveland", "green, red, white and black", "samovar", "birds such as flock, colony, fleet, parcel and dissimulation", "Nigeria", "the Assassin", "Brian Blessed", "willow", "\"Cruisin'\"", "Forrest Gump", "Straits of Tiran", "Eva Braun", "Little Women", "Papua New Guinea", "bronze medal", "denier", "Woolton pie", "Denmark", "Kopassus", "Tuesday", "Belfast", "is an outlaw motorcycle club with many charters in the United States as well as overseas", "Anandji Virji Shah", "world number 24 Rezai", "eared seals", "the Joint Committee on the Organization of Congress", "eardrum", "used to scare people"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6837222013993003}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4166666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1379310344827586, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3478", "mrqa_squad-validation-6828", "mrqa_squad-validation-6799", "mrqa_squad-validation-5589", "mrqa_squad-validation-2275", "mrqa_squad-validation-3899", "mrqa_triviaqa-validation-7252", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-3473", "mrqa_naturalquestions-validation-7845", "mrqa_newsqa-validation-3283", "mrqa_searchqa-validation-8824", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-3776", "mrqa_searchqa-validation-4602"], "SR": 0.640625, "CSR": 0.7517361111111112, "EFR": 0.17391304347826086, "Overall": 0.462824577294686}, {"timecode": 9, "before_eval_results": {"predictions": ["clapping their lobes", "Paul Revere", "Apollo 8", "Oxygen", "1973", "Magnetic stratigraphers", "Archduke Sigismund of Austria", "election of the UK Labour Party to government", "a + bi", "Superintendent", "Immediately after Decision Time", "powerful", "24 March 1879", "money from foreign Islamist banking systems", "the Song dynasty", "Rijn", "2.8%", "time and memory", "curved", "average teacher salaries", "five million", "deficit", "Hugues hypothesis", "social and political action", "set of triples", "Chris Salmon", "JLS", "abacus", "King County Executive", "Yakutat, Alaska", "Urania", "conclave", "Mallard", "Snowdonia", "Branson", "Buckinghamshire", "1974", "Benedict XVI", "Hindi", "France", "white", "architectural splendor", "Old Betsy", "Myanmar", "Goldie Hawn", "Italy", "George Orwell", "chilies", "six month", "Richard Marx", "breathless", "Rita Hayworth", "beer", "El Loco", "The West Wing", "Bundesliga Fu\u00dfball-Liga or DFL", "habitat", "Alex Skuby", "Heather Elizabeth Langenlighting", "200,167", "Haleigh Kushner", "safety issues", "son of the Legion", "enna Was Here"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6625}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3731", "mrqa_squad-validation-1223", "mrqa_squad-validation-6968", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1149", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3185", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-2440", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-1719", "mrqa_hotpotqa-validation-2639", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-247", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-1429"], "SR": 0.640625, "CSR": 0.740625, "EFR": 0.2608695652173913, "Overall": 0.5007472826086956}, {"timecode": 10, "UKR": 0.841796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4898", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-986", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-6668", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1356", "mrqa_newsqa-validation-2153", "mrqa_newsqa-validation-237", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-778", "mrqa_searchqa-validation-1109", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-13950", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-3776", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-8824", "mrqa_squad-validation-10026", "mrqa_squad-validation-10084", "mrqa_squad-validation-10103", "mrqa_squad-validation-10110", "mrqa_squad-validation-10155", "mrqa_squad-validation-10160", "mrqa_squad-validation-10166", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10217", "mrqa_squad-validation-10273", "mrqa_squad-validation-10288", "mrqa_squad-validation-10291", "mrqa_squad-validation-10336", "mrqa_squad-validation-10339", "mrqa_squad-validation-10363", "mrqa_squad-validation-10376", "mrqa_squad-validation-10452", "mrqa_squad-validation-1053", "mrqa_squad-validation-108", "mrqa_squad-validation-113", "mrqa_squad-validation-1207", "mrqa_squad-validation-1223", "mrqa_squad-validation-1256", "mrqa_squad-validation-1312", "mrqa_squad-validation-1319", "mrqa_squad-validation-1348", "mrqa_squad-validation-1355", "mrqa_squad-validation-1424", "mrqa_squad-validation-147", "mrqa_squad-validation-1506", "mrqa_squad-validation-1566", "mrqa_squad-validation-1587", "mrqa_squad-validation-1611", "mrqa_squad-validation-1621", "mrqa_squad-validation-1650", "mrqa_squad-validation-1666", "mrqa_squad-validation-1685", "mrqa_squad-validation-1764", "mrqa_squad-validation-1768", "mrqa_squad-validation-1774", "mrqa_squad-validation-1779", "mrqa_squad-validation-1790", "mrqa_squad-validation-1815", "mrqa_squad-validation-1881", "mrqa_squad-validation-1950", "mrqa_squad-validation-198", "mrqa_squad-validation-2014", "mrqa_squad-validation-2052", "mrqa_squad-validation-2076", "mrqa_squad-validation-2076", "mrqa_squad-validation-2078", "mrqa_squad-validation-2107", "mrqa_squad-validation-2110", "mrqa_squad-validation-2122", "mrqa_squad-validation-2134", "mrqa_squad-validation-2192", "mrqa_squad-validation-2196", "mrqa_squad-validation-2232", "mrqa_squad-validation-2235", "mrqa_squad-validation-2242", "mrqa_squad-validation-2250", "mrqa_squad-validation-2263", "mrqa_squad-validation-2286", "mrqa_squad-validation-2318", "mrqa_squad-validation-2332", "mrqa_squad-validation-2346", "mrqa_squad-validation-2347", "mrqa_squad-validation-2386", "mrqa_squad-validation-241", "mrqa_squad-validation-2472", "mrqa_squad-validation-2525", "mrqa_squad-validation-2539", "mrqa_squad-validation-2546", "mrqa_squad-validation-2594", "mrqa_squad-validation-260", "mrqa_squad-validation-2622", "mrqa_squad-validation-2636", "mrqa_squad-validation-2642", "mrqa_squad-validation-2653", "mrqa_squad-validation-2656", "mrqa_squad-validation-2666", "mrqa_squad-validation-2712", "mrqa_squad-validation-2719", "mrqa_squad-validation-2738", "mrqa_squad-validation-2742", "mrqa_squad-validation-2802", "mrqa_squad-validation-2853", "mrqa_squad-validation-2908", "mrqa_squad-validation-2915", "mrqa_squad-validation-2925", "mrqa_squad-validation-2956", "mrqa_squad-validation-296", "mrqa_squad-validation-2970", "mrqa_squad-validation-2993", "mrqa_squad-validation-3000", "mrqa_squad-validation-3040", "mrqa_squad-validation-3094", "mrqa_squad-validation-3118", "mrqa_squad-validation-3146", "mrqa_squad-validation-3159", "mrqa_squad-validation-323", "mrqa_squad-validation-3294", "mrqa_squad-validation-3305", "mrqa_squad-validation-3332", "mrqa_squad-validation-3335", "mrqa_squad-validation-3357", "mrqa_squad-validation-3366", "mrqa_squad-validation-340", "mrqa_squad-validation-344", "mrqa_squad-validation-3461", "mrqa_squad-validation-3478", "mrqa_squad-validation-3520", "mrqa_squad-validation-3532", "mrqa_squad-validation-3552", "mrqa_squad-validation-3555", "mrqa_squad-validation-3563", "mrqa_squad-validation-3568", "mrqa_squad-validation-3573", "mrqa_squad-validation-3600", "mrqa_squad-validation-3632", "mrqa_squad-validation-3643", "mrqa_squad-validation-3681", "mrqa_squad-validation-3728", "mrqa_squad-validation-3731", "mrqa_squad-validation-3757", "mrqa_squad-validation-3809", "mrqa_squad-validation-3858", "mrqa_squad-validation-3899", "mrqa_squad-validation-3932", "mrqa_squad-validation-3952", "mrqa_squad-validation-396", "mrqa_squad-validation-3969", "mrqa_squad-validation-3985", "mrqa_squad-validation-4023", "mrqa_squad-validation-4033", "mrqa_squad-validation-4053", "mrqa_squad-validation-4056", "mrqa_squad-validation-4103", "mrqa_squad-validation-4128", "mrqa_squad-validation-4194", "mrqa_squad-validation-425", "mrqa_squad-validation-4256", "mrqa_squad-validation-43", "mrqa_squad-validation-4310", "mrqa_squad-validation-4512", "mrqa_squad-validation-4551", "mrqa_squad-validation-456", "mrqa_squad-validation-4572", "mrqa_squad-validation-4646", "mrqa_squad-validation-4673", "mrqa_squad-validation-468", "mrqa_squad-validation-4702", "mrqa_squad-validation-4711", "mrqa_squad-validation-4731", "mrqa_squad-validation-4762", "mrqa_squad-validation-4781", "mrqa_squad-validation-4797", "mrqa_squad-validation-4826", "mrqa_squad-validation-4847", "mrqa_squad-validation-4882", "mrqa_squad-validation-4915", "mrqa_squad-validation-5008", "mrqa_squad-validation-5035", "mrqa_squad-validation-5080", "mrqa_squad-validation-5136", "mrqa_squad-validation-5187", "mrqa_squad-validation-520", "mrqa_squad-validation-5206", "mrqa_squad-validation-5207", "mrqa_squad-validation-5235", "mrqa_squad-validation-5256", "mrqa_squad-validation-5272", "mrqa_squad-validation-5310", "mrqa_squad-validation-5331", "mrqa_squad-validation-5354", "mrqa_squad-validation-5371", "mrqa_squad-validation-5428", "mrqa_squad-validation-5443", "mrqa_squad-validation-5470", "mrqa_squad-validation-5491", "mrqa_squad-validation-5540", "mrqa_squad-validation-5547", "mrqa_squad-validation-5581", "mrqa_squad-validation-5584", "mrqa_squad-validation-5589", "mrqa_squad-validation-5602", "mrqa_squad-validation-564", "mrqa_squad-validation-5666", "mrqa_squad-validation-5680", "mrqa_squad-validation-5688", "mrqa_squad-validation-5728", "mrqa_squad-validation-5771", "mrqa_squad-validation-5780", "mrqa_squad-validation-5785", "mrqa_squad-validation-5800", "mrqa_squad-validation-5806", "mrqa_squad-validation-5827", "mrqa_squad-validation-5841", "mrqa_squad-validation-5890", "mrqa_squad-validation-5899", "mrqa_squad-validation-5902", "mrqa_squad-validation-5903", "mrqa_squad-validation-591", "mrqa_squad-validation-6067", "mrqa_squad-validation-612", "mrqa_squad-validation-612", "mrqa_squad-validation-6127", "mrqa_squad-validation-6134", "mrqa_squad-validation-6135", "mrqa_squad-validation-6139", "mrqa_squad-validation-614", "mrqa_squad-validation-6165", "mrqa_squad-validation-6197", "mrqa_squad-validation-6307", "mrqa_squad-validation-6355", "mrqa_squad-validation-6360", "mrqa_squad-validation-6362", "mrqa_squad-validation-6366", "mrqa_squad-validation-6423", "mrqa_squad-validation-6428", "mrqa_squad-validation-645", "mrqa_squad-validation-6505", "mrqa_squad-validation-6554", "mrqa_squad-validation-659", "mrqa_squad-validation-6608", "mrqa_squad-validation-6717", "mrqa_squad-validation-6728", "mrqa_squad-validation-673", "mrqa_squad-validation-6746", "mrqa_squad-validation-6750", "mrqa_squad-validation-6799", "mrqa_squad-validation-6804", "mrqa_squad-validation-6891", "mrqa_squad-validation-6898", "mrqa_squad-validation-6905", "mrqa_squad-validation-6918", "mrqa_squad-validation-6920", "mrqa_squad-validation-6968", "mrqa_squad-validation-6994", "mrqa_squad-validation-702", "mrqa_squad-validation-7020", "mrqa_squad-validation-7055", "mrqa_squad-validation-7065", "mrqa_squad-validation-7076", "mrqa_squad-validation-7077", "mrqa_squad-validation-7107", "mrqa_squad-validation-7112", "mrqa_squad-validation-7141", "mrqa_squad-validation-7166", "mrqa_squad-validation-7186", "mrqa_squad-validation-7274", "mrqa_squad-validation-7430", "mrqa_squad-validation-7434", "mrqa_squad-validation-7455", "mrqa_squad-validation-749", "mrqa_squad-validation-7491", "mrqa_squad-validation-7519", "mrqa_squad-validation-7550", "mrqa_squad-validation-7582", "mrqa_squad-validation-7634", "mrqa_squad-validation-7660", "mrqa_squad-validation-7745", "mrqa_squad-validation-7849", "mrqa_squad-validation-7889", "mrqa_squad-validation-7941", "mrqa_squad-validation-7945", "mrqa_squad-validation-795", "mrqa_squad-validation-7950", "mrqa_squad-validation-7976", "mrqa_squad-validation-7984", "mrqa_squad-validation-8012", "mrqa_squad-validation-8045", "mrqa_squad-validation-805", "mrqa_squad-validation-8073", "mrqa_squad-validation-8077", "mrqa_squad-validation-8129", "mrqa_squad-validation-8140", "mrqa_squad-validation-8151", "mrqa_squad-validation-8188", "mrqa_squad-validation-8232", "mrqa_squad-validation-8246", "mrqa_squad-validation-8250", "mrqa_squad-validation-8288", "mrqa_squad-validation-831", "mrqa_squad-validation-8316", "mrqa_squad-validation-8343", "mrqa_squad-validation-8426", "mrqa_squad-validation-8516", "mrqa_squad-validation-8561", "mrqa_squad-validation-8576", "mrqa_squad-validation-8625", "mrqa_squad-validation-8636", "mrqa_squad-validation-865", "mrqa_squad-validation-865", "mrqa_squad-validation-8707", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8726", "mrqa_squad-validation-8828", "mrqa_squad-validation-8910", "mrqa_squad-validation-8920", "mrqa_squad-validation-895", "mrqa_squad-validation-8981", "mrqa_squad-validation-8982", "mrqa_squad-validation-8987", "mrqa_squad-validation-8999", "mrqa_squad-validation-9040", "mrqa_squad-validation-9080", "mrqa_squad-validation-9114", "mrqa_squad-validation-9154", "mrqa_squad-validation-9205", "mrqa_squad-validation-9205", "mrqa_squad-validation-921", "mrqa_squad-validation-9258", "mrqa_squad-validation-9306", "mrqa_squad-validation-9317", "mrqa_squad-validation-9336", "mrqa_squad-validation-9366", "mrqa_squad-validation-937", "mrqa_squad-validation-9397", "mrqa_squad-validation-9461", "mrqa_squad-validation-9501", "mrqa_squad-validation-9507", "mrqa_squad-validation-9519", "mrqa_squad-validation-9545", "mrqa_squad-validation-9550", "mrqa_squad-validation-960", "mrqa_squad-validation-9614", "mrqa_squad-validation-9649", "mrqa_squad-validation-9650", "mrqa_squad-validation-9691", "mrqa_squad-validation-9754", "mrqa_squad-validation-9803", "mrqa_squad-validation-9873", "mrqa_squad-validation-9889", "mrqa_squad-validation-9970", "mrqa_squad-validation-9985", "mrqa_triviaqa-validation-1008", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1036", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1118", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-1146", "mrqa_triviaqa-validation-1149", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-1519", "mrqa_triviaqa-validation-1519", "mrqa_triviaqa-validation-1571", "mrqa_triviaqa-validation-1594", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1685", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1847", "mrqa_triviaqa-validation-1854", "mrqa_triviaqa-validation-1913", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2143", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2503", "mrqa_triviaqa-validation-2600", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-2708", "mrqa_triviaqa-validation-2753", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-3213", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-3463", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-3776", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-4027", "mrqa_triviaqa-validation-4089", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4308", "mrqa_triviaqa-validation-4368", "mrqa_triviaqa-validation-4449", "mrqa_triviaqa-validation-4472", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5130", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5796", "mrqa_triviaqa-validation-5815", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6090", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-6357", "mrqa_triviaqa-validation-6370", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6667", "mrqa_triviaqa-validation-6691", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-707", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-7119", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7395", "mrqa_triviaqa-validation-7418", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7544", "mrqa_triviaqa-validation-7577", "mrqa_triviaqa-validation-7626", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-7674", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-7756", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-7761", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-928"], "OKR": 0.830078125, "KG": 0.43359375, "before_eval_results": {"predictions": ["Vince Lombardi Trophy", "2006", "Dutch Cape Colony", "27 June", "Orange County", "Private Bill Committees", "\"yes\" or \"no\"", "45", "Ali Shariati", "1994", "seven", "Apollo", "Bermuda 419 turf", "April 4, 1968", "treatment", "disturbances, including a revolt by the Augustinian friars against their prior, the smashed of statues and images in churches, and denunciations of the magistracy", "Corliss steam engine", "organisms", "private", "the Rhine and its downstream extension", "Manuel Blum", "African-American", "lactic acid", "Mamma Mia!", "Kofi Annan", "The Woodentops", "Hill House School", "Nicola Adams", "Leicester", "James Blunt", "Steve Miller", "Standard Oil Company", "J.P. Pennington", "Parker Pyne", "Laurie Lee", "Germany", "Reims", "2000", "skull", "Red Fox", "Frank Darabont", "Hugh Gaitskell", "Bloody Sunday", "Eric Blair", "the Crystal Palace", "heartburn", "Dr Hawley Harvey Crippen", "Richard Seddon", "\"The Celts: Blood, Iron and Sacrifice\"", "the cornfield", "Colombia", "failure", "the official score(s) of the specified batsman", "Seymour Hersh", "caractacus Potts", "Tartar sauce", "Chris Rea", "The season will consist of 24 episodes", "Sarah Winnemucca", "the extraterrestrial hypothesis", "the motherless cub defended by Elphaba", "Lee Myung-Bak says he would donate his salary to help the underprivileged.", "Paul Yves Roch Gil- bert du Motier", "U.S.A."], "metric_results": {"EM": 0.65625, "QA-F1": 0.7038690476190477}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.8333333333333333, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1772", "mrqa_squad-validation-2301", "mrqa_squad-validation-9434", "mrqa_triviaqa-validation-359", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-7257", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-4099", "mrqa_naturalquestions-validation-525", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1607", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-3686", "mrqa_searchqa-validation-15122"], "SR": 0.65625, "CSR": 0.7329545454545454, "EFR": 0.5, "Overall": 0.6676846590909091}, {"timecode": 11, "before_eval_results": {"predictions": ["Thomas Commerford Martin", "shortening the cutoff", "a form of starch", "by conquering the other state's lands", "the Henry Cole wing", "a visitation of the church", "private universities", "Doctorin' the Tardis", "24", "celibacy", "MCI Telecommunications", "rationing", "as part of a rule connected with civil disobedience", "1294", "Ralph Nelson", "Chicago Bears", "Yangzi River basin", "self and non-self", "Protestant", "East Asian", "osmium", "M.E. Watson", "elytra", "kitsunes", "the people you know and love", "silica sand, soda ash, dolomite and limestone", "2011", "Argentina", "Son of Sam", "Red", "1912", "Wyoming", "St. Louis", "Ulysses S. Grant", "Novak Djokovic", "Dangerous Minds", "four inches", "raw beef", "kuyk", "Gillis Grafstr\u00f6m", "sturgeon", "\u00ef\u00bf\u00bd", "euthenasia", "DMC-12", "Bloodaxe", "The Rocketeer", "James Mason", "UK Singles Chart", "Syria", "four", "Raggety", "cabbage", "12", "Haiti", "one", "Wyatt and Dylan Walters", "orange", "Newfoundland and Labrador", "Leonarda Cianciulli", "Mexican soldiers", "five", "golden", "eirksson", "Jodie Prenger"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5703125}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2483", "mrqa_squad-validation-2754", "mrqa_squad-validation-6722", "mrqa_triviaqa-validation-6668", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-172", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-2175", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1207", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-6038", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-5450", "mrqa_triviaqa-validation-1723", "mrqa_naturalquestions-validation-1577", "mrqa_newsqa-validation-3551", "mrqa_searchqa-validation-16074", "mrqa_searchqa-validation-4107"], "SR": 0.546875, "CSR": 0.7174479166666667, "EFR": 0.3448275862068966, "Overall": 0.6335488505747127}, {"timecode": 12, "before_eval_results": {"predictions": ["via a D loop mechanism", "private universities", "Hulu", "scalar quantities", "John Dobson", "NDS", "Bell Northern Research", "Republic of Kenya", "William Ellery Channing and Ralph Waldo Emerson", "detrimental", "a strange odor", "HIV", "smaller, weaker swimmers", "ABC Studios", "Jordan Norwood", "The Time of the Doctor", "non-Mongol physicians", "Philo", "Discretion", "Peter Principle", "HobbyA hobby is a regular activity that is done for enjoyment", "Florence", "Jesus", "aurochs", "standard", "Phar Lap", "lactic acid", "Lorelei", "Spain", "the iris", "raw animal hides", "Brian Kraft", "first wedding anniversary", "the Continental Army", "Clinton", "Madison Square Garden", "a Rh\u00f4ne Grape Varietal Grown at Tablas Creek Vineyard", "pi\u00f1a colada", "eyes", "Colette", "Brisbane River", "aeoline", "Montezuma", "Eric Morley", "mead", "toilet", "Turkey", "Kevin Painter", "Fifth", "La Boh\u00e8me", "Kermadec Islands", "Montpelier", "Indonesia", "Newcastle United", "Manchester", "the United States Office of Management and Budget ( OMB )", "in the east", "1993", "conservative", "pirates", "Oprah Winfrey", "Sir Humphry Davy", "jam", "Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7298888305322129}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4488", "mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-865", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-3454", "mrqa_triviaqa-validation-403", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-6406", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-4003", "mrqa_searchqa-validation-8019", "mrqa_searchqa-validation-13456"], "SR": 0.703125, "CSR": 0.7163461538461539, "EFR": 0.3157894736842105, "Overall": 0.6275208755060728}, {"timecode": 13, "before_eval_results": {"predictions": ["more than $45,000", "international metropolitan region", "easier credit to the lower and middle income earners", "1273", "the Dutch Republic", "Robert Gates", "Jean-Claude Juncker", "September 5, 1985", "Edict of Potsdam", "2.2 inches", "four", "artisans and farmers", "BBC Books", "San Diego", "the Prophet Mohammad", "a setup phase in each involved node before any packet is transferred to establish the parameters of communication", "Christopher Nolan", "Turkey", "John Flamsteed", "Costa Concordia", "fish sauce", "right", "T\u1ebdt Nguy\u00ean \u00d0\u00e1n", "Heath Ledger", "Dublin", "\u201cgluten-free\u201d", "Tesco", "Gower Peninsula", "Facebook", "Daily Bugle", "Helen Gurley Brown", "Dick Turpin", "tomato", "eukharisti\u0101", "Neighbours", "Las Vegas", "Car ferry the Herald of Free Enterprise", "Dumbo", "collective Noun for Alligators", "Pete Seeger", "Birmingham", "Jack Nicholson", "1955", "Virgil", "Manhattan", "Humber International Terminal", "Vietnam", "Duke of Norfolk", "parliament", "Oliver Cromwell's", "Denmark", "Ypres", "the Yukon Territory", "Nowhere Boy", "San Francisco 49ers, the Dallas Dallas, the Washington Redskins and the Baltimore", "gorman as Rose Lindsey", "Woolsthorpe-by-Belvoir", "1943", "Lonnie", "France's famous Louvre museum", "landmark", "the boss man", "750 million pesos", "government"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7091517857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9773", "mrqa_squad-validation-4293", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-3980", "mrqa_triviaqa-validation-2949", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-1448", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-849", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-5769", "mrqa_searchqa-validation-3230", "mrqa_newsqa-validation-3449"], "SR": 0.671875, "CSR": 0.7131696428571428, "EFR": 0.38095238095238093, "Overall": 0.6399181547619047}, {"timecode": 14, "before_eval_results": {"predictions": ["Dwight D. Eisenhower", "Ban Ki-moon", "Most Western countries", "around 300,000", "six", "their inherent difficulty", "3,792,621", "4 ft 11 in", "66\u201334 Mya", "Elton Rule", "Thermochemical", "Missy", "no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription, who has ever been charged by authorities.", "Ismailiyah, Egypt", "mantle", "harpoons", "one", "The Guns of Navarone", "chickenpox", "Israel", "Idaho", "rare postage stamps", "red fez hat and soft yellow babouche slippers", "from the first part of this century to 1985", "he stated that, \"Bob Dole can not beat Bill Clinton.", "", "Alanis Morissette", "Dennis Hastert", "1,000,000 milligrams", "Deadwood", "cat scratch fever", "an earthquake", "Daland", "pickle", "Moscow", "Budapest", "snowman", "fog", "The $25,000 Pyramid", "pry bars", "Stalin", "Ikea", "New York", "Columbia University College of Physicians and Surgeons", "Jennifer Lopez", "Theodore Roosevelt", "Marco Polo", "\"Welcome,\"", "Hydra", "taxonomy", "Gadsden Purchase", "Matterhorn", "Paraguay", "Robert Sherwood", "methods vary by the sources of information that are drawn on, how that information is sampled, and the types of instruments that are used in data collection", "is a federal republic composed of 50 states, a federal district, five major self - governing territories, and various possessions", "red", "Conrad Murray", "Gatwick Airport (IATA: LGW, ICAO: EGKK)", "Rio Airways", "more than 100", "blue-purple", "1885", "PeopleMover"], "metric_results": {"EM": 0.515625, "QA-F1": 0.57796875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4532", "mrqa_squad-validation-6383", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-4526", "mrqa_searchqa-validation-16293", "mrqa_searchqa-validation-3536", "mrqa_searchqa-validation-8183", "mrqa_searchqa-validation-1682", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-6432", "mrqa_searchqa-validation-16935", "mrqa_searchqa-validation-432", "mrqa_searchqa-validation-10715", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-3313", "mrqa_searchqa-validation-16054", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-13754", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-5972", "mrqa_searchqa-validation-6128", "mrqa_searchqa-validation-3124", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-9896", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-4582", "mrqa_newsqa-validation-609", "mrqa_hotpotqa-validation-3885"], "SR": 0.515625, "CSR": 0.7, "EFR": 0.45161290322580644, "Overall": 0.6514163306451612}, {"timecode": 15, "before_eval_results": {"predictions": ["the Netherlands", "counties or powiats", "Donald Davies", "C. J. Anderson", "the French", "until the empire fell", "the study of rocks", "The Book of Discipline of the United Methodist Church", "Captain America: Civil War", "1967", "826", "Torchwood: Miracle Day", "1524\u201325", "Soul on Ice", "a public parkfor the benefit", "bone fracture", "Nirvana", "Powhatan", "kFC Famous Bowl", "Ed Sullivan", "theft", "zer -g", "jupiter", "Tennessee", "William Randolph Hearst", "Universal", "Macbeth", "the metropolitan bishop or metropolitan", "John Alden", "pig", "Ghost Town", "penny", "mutual fund", "Tennessee", "Challenger/STS-8", "Mrs. Dalloway", "1960s", "le Roman de la Rose", "a popular radio show", "Oscar Wilde", "Jeopardy", "whole hog", "Chile", "John Cabot", "Zorba the Mastiff", "kimchi", "Wales", "taximeter", "Othello", "a retirement-type subsidy", "aub", "King of the Hill", "Cairo", "a pet", "Sheev Palpatine", "a routing table", "Michael Mizrachi", "The Watchtower and Awake", "Capellini", "Pandosia and Heraclea", "Ike", "William Scott Miller", "June 13, 1960", "Michael Phelps"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5590277777777777}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10247", "mrqa_searchqa-validation-15060", "mrqa_searchqa-validation-13858", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-8694", "mrqa_searchqa-validation-4321", "mrqa_searchqa-validation-6619", "mrqa_searchqa-validation-13119", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-6220", "mrqa_searchqa-validation-2873", "mrqa_searchqa-validation-2751", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-335", "mrqa_searchqa-validation-2765", "mrqa_searchqa-validation-7694", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-3870", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-4847", "mrqa_naturalquestions-validation-5986", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-3352", "mrqa_newsqa-validation-4098"], "SR": 0.515625, "CSR": 0.6884765625, "EFR": 0.4838709677419355, "Overall": 0.655563256048387}, {"timecode": 16, "before_eval_results": {"predictions": ["in a glass case", "John D. Rockefeller", "1989", "economic inequality", "attribution", "hospitals and other institutions", "copper mining trade", "80.4", "a roof extension", "January 1985", "chloroplast dimorphism", "1489", "Tim Storms", "wyverns", "Grolsch", "pentathlon", "dustbin lids", "Superman", "nests", "Trampoline", "red olives", "Venus", "things", "The Manchurian Candidate", "hirple", "The Devil's Advocate", "potato pie", "the Roman", "Port In A Storm", "the ossicle", "The Kinks", "deep-rooted", "David Webb", "Daniel Craig", "parez", "a fire-adapted community", "cranberry juice", "Valentina Tereshkova", "Harland Sanders", "govenor of California", "Coax", "gold", "Nancy Lopez", "Buddhism", "Macaulay Culkin", "a French composer", "Los Angeles Times", "Nicholas Shakespeare", "Madagascar", "decomposition reaction", "kappa", "the Federal Convention of 1787", "media", "Microsoft", "Glenn Close", "Giorgio Vasari and Federico Zuccari", "Rotherham United", "Manchester", "John Nicholas Galleher", "fifty", "Wally", "Eden Park", "cents", "William Adelin"], "metric_results": {"EM": 0.390625, "QA-F1": 0.44706959706959704}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.30769230769230765, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7425", "mrqa_squad-validation-6772", "mrqa_squad-validation-3692", "mrqa_squad-validation-2534", "mrqa_squad-validation-8926", "mrqa_searchqa-validation-5417", "mrqa_searchqa-validation-1526", "mrqa_searchqa-validation-12957", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1960", "mrqa_searchqa-validation-16056", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-14701", "mrqa_searchqa-validation-14817", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-6026", "mrqa_searchqa-validation-13570", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-14391", "mrqa_searchqa-validation-4989", "mrqa_searchqa-validation-13641", "mrqa_searchqa-validation-7089", "mrqa_searchqa-validation-8424", "mrqa_searchqa-validation-14998", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-14135", "mrqa_searchqa-validation-3745", "mrqa_searchqa-validation-5418", "mrqa_searchqa-validation-1193", "mrqa_naturalquestions-validation-2800", "mrqa_triviaqa-validation-6406", "mrqa_hotpotqa-validation-5088", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1225", "mrqa_triviaqa-validation-1746"], "SR": 0.390625, "CSR": 0.6709558823529411, "EFR": 0.5641025641025641, "Overall": 0.668105439291101}, {"timecode": 17, "before_eval_results": {"predictions": ["the principles of legal certainty and good faith", "1862", "35", "bad publicity", "$5 million", "Robert Boyle", "40%", "Camp Mercury nuclear test site", "Knights Templar", "ABC", "more than 70", "Dick Wolf", "Spmi", "Dalmatians", "Joseph Conrad", "fencing", "zebra", "Margaret Atwood", "Macarena", "U.S. Treasury", "levis", "Walter Ralegh", "Rocky Down Mexico Way", "gas masks", "San Francisco", "the Maltese Falcon", "Al-Qaeda", "Lyndon Johnson", "Taj Mahal", "jumper cables", "Montana", "wood", "actuaries", "Immanuel Kant", "Joe Lieberman", "Beethoven", "\"D\" Briefing Flashcards", "cities named delhi", "lm", "butch lesbians", "London", "cash machine", "northern pike", "Edith Bolling Wilson", "Cather", "Burmese Python", "marsupial", "the anchor", "Joe Franklin", "Karl Rove", "telescope", "a preacher's family", "Minus Heel shoes", "The Stations of the Cross originated in pilgrimages to Jerusalem", "David Kaye", "jaws", "Rapa Nui", "Conservative Party", "Rovaniemi", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "we might find us in five or 10 years saying we've made a big mistake.", "Africa and Asia", "Francisco Pizarro", "Parashara"], "metric_results": {"EM": 0.5, "QA-F1": 0.5638480392156863}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-13450", "mrqa_searchqa-validation-8911", "mrqa_searchqa-validation-1245", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-11789", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-16591", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-1796", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-8814", "mrqa_searchqa-validation-29", "mrqa_searchqa-validation-14747", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-10463", "mrqa_searchqa-validation-6790", "mrqa_searchqa-validation-1896", "mrqa_searchqa-validation-4709", "mrqa_searchqa-validation-16380", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-8637", "mrqa_triviaqa-validation-7755", "mrqa_hotpotqa-validation-2922", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2371", "mrqa_naturalquestions-validation-246"], "SR": 0.5, "CSR": 0.6614583333333333, "EFR": 0.34375, "Overall": 0.6221354166666666}, {"timecode": 18, "before_eval_results": {"predictions": ["William the Lion", "the Song dynasty", "inequality", "3", "PBS", "1,000", "establishing relationships with other necessary participants", "four", "Mexico\u2013United States border", "thousands", "dancers", "Horseshoe Crabs", "Pennsylvania", "Netherlands", "884", "Oxford", "Tibet", "All Quiet on the Western Front", "the hypothalamus", "coal", "a pre-crash system", "Samuel Johnson", "\"Livin' On A Prayer,\"", "fluent", "trenchcoat", "Cunard", "the United States Department of Transportation", "Adam Sandler", "Pocahontas", "Boston Marathon", "Wyoming", "Lucy in the Sky", "kayak", "Martin Luther King", "avocados", "a sewing machine", "(Sly & Robbie, Grace Jones, Peter Tosh, The", "the gallbladder", "on his toe", "Tony Awards", "Brazil", "stiletto", "Scotland", "Deep Blue", "the Chemical Element", "Matt Leinart", "egg Benedict", "a 3-letter insecticide", "Jonathan Demme", "Isabela Island", "Judas Iscariot", "Green Lantern", "Komodo dragon", "AD 95", "The Tiber", "Bob Barker", "ankle", "a Peach", "2006", "Robert Wagner", "the Illuminati", "coagulation", "Pluto", "Omerta"], "metric_results": {"EM": 0.625, "QA-F1": 0.7003720238095239}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5544", "mrqa_searchqa-validation-13705", "mrqa_searchqa-validation-1762", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-12590", "mrqa_searchqa-validation-14448", "mrqa_searchqa-validation-3830", "mrqa_searchqa-validation-1799", "mrqa_searchqa-validation-2048", "mrqa_searchqa-validation-2772", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-12483", "mrqa_searchqa-validation-1", "mrqa_searchqa-validation-16272", "mrqa_searchqa-validation-8842", "mrqa_naturalquestions-validation-9428", "mrqa_triviaqa-validation-3287", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-178", "mrqa_triviaqa-validation-404"], "SR": 0.625, "CSR": 0.6595394736842105, "EFR": 0.3333333333333333, "Overall": 0.6196683114035088}, {"timecode": 19, "before_eval_results": {"predictions": ["39", "Kevin Harlan", "seizures", "religion", "88", "tentacles", "Calendar of Saints", "triplet oxygen", "indulgences", "ranean", "1st", "confer", "Conformation", "\"Settlers in Canada\", this phrase about boat propulsion means to decide your own fate", "the Mules", "Norah Jones", "Zorba the Mastiff", "New Zealand", "Tim McGraw & Faith Hill", "Birch-tree", "Oshkosh", "the eagle", "Ecuador", "holy water", "Garfield", "The Terminator", "China", "Anastacia Lyn Newkirk", "a carrier", "the box", "Jet Blue", "onions", "Sam Waterston", "Nacho Libre", "vodka", "the House of Lords", "pizza", "Leather Tuscadero", "General Ulysses S. Grant", "essay", "Spider-Man", "foot fault", "a rock", "U.S. currency", "hama", "The Granite State", "\"P Dictionary\"", "Gambia River river", "New Brunswick", "copper", "Sweden", "Annabel", "alternating current", "one", "Yondu Udonta", "Rudolph", "Edward VII", "1999", "Dungeness", "Pakistani territory", "Osama bin Laden's sons", "Wine Spectator", "Liszt Strauss Wagner Dvorak", "Neil Connor"], "metric_results": {"EM": 0.5, "QA-F1": 0.581374007936508}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9522", "mrqa_squad-validation-2011", "mrqa_searchqa-validation-14391", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-3733", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-7694", "mrqa_searchqa-validation-4071", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-1149", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-94", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-8611", "mrqa_searchqa-validation-15564", "mrqa_searchqa-validation-10899", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-16597", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-11422", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-7464", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-1953", "mrqa_hotpotqa-validation-2535", "mrqa_newsqa-validation-648", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-7456"], "SR": 0.5, "CSR": 0.6515625, "EFR": 0.34375, "Overall": 0.62015625}, {"timecode": 20, "UKR": 0.826171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4275", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4898", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-5769", "mrqa_hotpotqa-validation-986", "mrqa_naturalquestions-validation-10630", "mrqa_naturalquestions-validation-1577", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-9428", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1293", "mrqa_newsqa-validation-1356", "mrqa_newsqa-validation-2124", "mrqa_newsqa-validation-2153", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-648", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-1", "mrqa_searchqa-validation-10106", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10850", "mrqa_searchqa-validation-10996", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-1109", "mrqa_searchqa-validation-11220", "mrqa_searchqa-validation-11422", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-11789", "mrqa_searchqa-validation-1193", "mrqa_searchqa-validation-11974", "mrqa_searchqa-validation-12187", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-12483", "mrqa_searchqa-validation-12590", "mrqa_searchqa-validation-12712", "mrqa_searchqa-validation-12767", "mrqa_searchqa-validation-12816", "mrqa_searchqa-validation-12920", "mrqa_searchqa-validation-13114", "mrqa_searchqa-validation-13175", "mrqa_searchqa-validation-13299", "mrqa_searchqa-validation-13318", "mrqa_searchqa-validation-13450", "mrqa_searchqa-validation-13491", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-13626", "mrqa_searchqa-validation-13641", "mrqa_searchqa-validation-13714", "mrqa_searchqa-validation-13858", "mrqa_searchqa-validation-13858", "mrqa_searchqa-validation-13950", "mrqa_searchqa-validation-14135", "mrqa_searchqa-validation-14244", "mrqa_searchqa-validation-14448", "mrqa_searchqa-validation-14747", "mrqa_searchqa-validation-14998", "mrqa_searchqa-validation-15089", "mrqa_searchqa-validation-1526", "mrqa_searchqa-validation-15377", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-15564", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-15673", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-16054", "mrqa_searchqa-validation-16092", "mrqa_searchqa-validation-16272", "mrqa_searchqa-validation-16287", "mrqa_searchqa-validation-16293", "mrqa_searchqa-validation-16363", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-1652", "mrqa_searchqa-validation-16597", "mrqa_searchqa-validation-1682", "mrqa_searchqa-validation-1796", "mrqa_searchqa-validation-1799", "mrqa_searchqa-validation-1896", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2483", "mrqa_searchqa-validation-2626", "mrqa_searchqa-validation-2746", "mrqa_searchqa-validation-2751", "mrqa_searchqa-validation-2873", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3124", "mrqa_searchqa-validation-3198", "mrqa_searchqa-validation-3230", "mrqa_searchqa-validation-335", "mrqa_searchqa-validation-3536", "mrqa_searchqa-validation-3745", "mrqa_searchqa-validation-3776", "mrqa_searchqa-validation-381", "mrqa_searchqa-validation-3830", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-4071", "mrqa_searchqa-validation-4107", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4526", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-4890", "mrqa_searchqa-validation-5132", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6026", "mrqa_searchqa-validation-6128", "mrqa_searchqa-validation-6432", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-6619", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-6790", "mrqa_searchqa-validation-7089", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-7431", "mrqa_searchqa-validation-7569", "mrqa_searchqa-validation-7954", "mrqa_searchqa-validation-8045", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-8694", "mrqa_searchqa-validation-9252", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-94", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-10084", "mrqa_squad-validation-10085", "mrqa_squad-validation-10160", "mrqa_squad-validation-10166", "mrqa_squad-validation-10203", "mrqa_squad-validation-10288", "mrqa_squad-validation-10291", "mrqa_squad-validation-10363", "mrqa_squad-validation-10376", "mrqa_squad-validation-10396", "mrqa_squad-validation-10448", "mrqa_squad-validation-10452", "mrqa_squad-validation-1046", "mrqa_squad-validation-108", "mrqa_squad-validation-1106", "mrqa_squad-validation-113", "mrqa_squad-validation-1207", "mrqa_squad-validation-1223", "mrqa_squad-validation-1256", "mrqa_squad-validation-1319", "mrqa_squad-validation-1348", "mrqa_squad-validation-1506", "mrqa_squad-validation-1535", "mrqa_squad-validation-1557", "mrqa_squad-validation-1566", "mrqa_squad-validation-1611", "mrqa_squad-validation-1642", "mrqa_squad-validation-1685", "mrqa_squad-validation-1730", "mrqa_squad-validation-1751", "mrqa_squad-validation-1764", "mrqa_squad-validation-1815", "mrqa_squad-validation-1897", "mrqa_squad-validation-1964", "mrqa_squad-validation-198", "mrqa_squad-validation-2011", "mrqa_squad-validation-2024", "mrqa_squad-validation-2076", "mrqa_squad-validation-2086", "mrqa_squad-validation-2107", "mrqa_squad-validation-2122", "mrqa_squad-validation-2192", "mrqa_squad-validation-2196", "mrqa_squad-validation-2250", "mrqa_squad-validation-226", "mrqa_squad-validation-2275", "mrqa_squad-validation-2277", "mrqa_squad-validation-2286", "mrqa_squad-validation-2301", "mrqa_squad-validation-2318", "mrqa_squad-validation-2347", "mrqa_squad-validation-2361", "mrqa_squad-validation-2541", "mrqa_squad-validation-2546", "mrqa_squad-validation-2613", "mrqa_squad-validation-2642", "mrqa_squad-validation-2666", "mrqa_squad-validation-2667", "mrqa_squad-validation-2673", "mrqa_squad-validation-2709", "mrqa_squad-validation-2742", "mrqa_squad-validation-2754", "mrqa_squad-validation-2925", "mrqa_squad-validation-2970", "mrqa_squad-validation-3020", "mrqa_squad-validation-3022", "mrqa_squad-validation-3094", "mrqa_squad-validation-3131", "mrqa_squad-validation-3187", "mrqa_squad-validation-323", "mrqa_squad-validation-3363", "mrqa_squad-validation-338", "mrqa_squad-validation-341", "mrqa_squad-validation-3461", "mrqa_squad-validation-3478", "mrqa_squad-validation-3510", "mrqa_squad-validation-3524", "mrqa_squad-validation-3563", "mrqa_squad-validation-3568", "mrqa_squad-validation-3573", "mrqa_squad-validation-3600", "mrqa_squad-validation-3642", "mrqa_squad-validation-370", "mrqa_squad-validation-3707", "mrqa_squad-validation-3714", "mrqa_squad-validation-3728", "mrqa_squad-validation-3731", "mrqa_squad-validation-3800", "mrqa_squad-validation-3861", "mrqa_squad-validation-3899", "mrqa_squad-validation-3933", "mrqa_squad-validation-3952", "mrqa_squad-validation-396", "mrqa_squad-validation-4023", "mrqa_squad-validation-404", "mrqa_squad-validation-4053", "mrqa_squad-validation-4099", "mrqa_squad-validation-4103", "mrqa_squad-validation-425", "mrqa_squad-validation-4518", "mrqa_squad-validation-4551", "mrqa_squad-validation-456", "mrqa_squad-validation-4673", "mrqa_squad-validation-4702", "mrqa_squad-validation-4711", "mrqa_squad-validation-4762", "mrqa_squad-validation-4797", "mrqa_squad-validation-4799", "mrqa_squad-validation-4826", "mrqa_squad-validation-4844", "mrqa_squad-validation-4847", "mrqa_squad-validation-4884", "mrqa_squad-validation-4915", "mrqa_squad-validation-4929", "mrqa_squad-validation-5008", "mrqa_squad-validation-5014", "mrqa_squad-validation-5035", "mrqa_squad-validation-5080", "mrqa_squad-validation-5168", "mrqa_squad-validation-5178", "mrqa_squad-validation-5185", "mrqa_squad-validation-5206", "mrqa_squad-validation-5207", "mrqa_squad-validation-523", "mrqa_squad-validation-5249", "mrqa_squad-validation-5256", "mrqa_squad-validation-5268", "mrqa_squad-validation-5272", "mrqa_squad-validation-5371", "mrqa_squad-validation-5420", "mrqa_squad-validation-5428", "mrqa_squad-validation-5547", "mrqa_squad-validation-557", "mrqa_squad-validation-5584", "mrqa_squad-validation-5589", "mrqa_squad-validation-564", "mrqa_squad-validation-5680", "mrqa_squad-validation-5715", "mrqa_squad-validation-5785", "mrqa_squad-validation-5800", "mrqa_squad-validation-5827", "mrqa_squad-validation-5837", "mrqa_squad-validation-5890", "mrqa_squad-validation-5902", "mrqa_squad-validation-5944", "mrqa_squad-validation-596", "mrqa_squad-validation-5986", "mrqa_squad-validation-6041", "mrqa_squad-validation-6067", "mrqa_squad-validation-607", "mrqa_squad-validation-6127", "mrqa_squad-validation-6139", "mrqa_squad-validation-614", "mrqa_squad-validation-6197", "mrqa_squad-validation-636", "mrqa_squad-validation-6366", "mrqa_squad-validation-6383", "mrqa_squad-validation-6413", "mrqa_squad-validation-6423", "mrqa_squad-validation-6428", "mrqa_squad-validation-645", "mrqa_squad-validation-6527", "mrqa_squad-validation-6554", "mrqa_squad-validation-659", "mrqa_squad-validation-6608", "mrqa_squad-validation-6625", "mrqa_squad-validation-6701", "mrqa_squad-validation-6717", "mrqa_squad-validation-673", "mrqa_squad-validation-6746", "mrqa_squad-validation-6779", "mrqa_squad-validation-6848", "mrqa_squad-validation-6891", "mrqa_squad-validation-6898", "mrqa_squad-validation-6905", "mrqa_squad-validation-6918", "mrqa_squad-validation-6968", "mrqa_squad-validation-7055", "mrqa_squad-validation-7076", "mrqa_squad-validation-7107", "mrqa_squad-validation-7117", "mrqa_squad-validation-7141", "mrqa_squad-validation-7186", "mrqa_squad-validation-7312", "mrqa_squad-validation-7314", "mrqa_squad-validation-7379", "mrqa_squad-validation-7386", "mrqa_squad-validation-7434", "mrqa_squad-validation-7491", "mrqa_squad-validation-7660", "mrqa_squad-validation-7786", "mrqa_squad-validation-7834", "mrqa_squad-validation-7914", "mrqa_squad-validation-7941", "mrqa_squad-validation-7950", "mrqa_squad-validation-7984", "mrqa_squad-validation-8012", "mrqa_squad-validation-8077", "mrqa_squad-validation-8140", "mrqa_squad-validation-8151", "mrqa_squad-validation-8232", "mrqa_squad-validation-8261", "mrqa_squad-validation-8288", "mrqa_squad-validation-8370", "mrqa_squad-validation-8444", "mrqa_squad-validation-8516", "mrqa_squad-validation-8576", "mrqa_squad-validation-8625", "mrqa_squad-validation-865", "mrqa_squad-validation-8719", "mrqa_squad-validation-8726", "mrqa_squad-validation-8789", "mrqa_squad-validation-8980", "mrqa_squad-validation-9040", "mrqa_squad-validation-9174", "mrqa_squad-validation-921", "mrqa_squad-validation-9306", "mrqa_squad-validation-9317", "mrqa_squad-validation-9336", "mrqa_squad-validation-9347", "mrqa_squad-validation-937", "mrqa_squad-validation-9421", "mrqa_squad-validation-9434", "mrqa_squad-validation-9522", "mrqa_squad-validation-9550", "mrqa_squad-validation-9614", "mrqa_squad-validation-9629", "mrqa_squad-validation-9649", "mrqa_squad-validation-9650", "mrqa_squad-validation-9754", "mrqa_squad-validation-9803", "mrqa_squad-validation-9873", "mrqa_squad-validation-9889", "mrqa_squad-validation-9970", "mrqa_triviaqa-validation-0", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1448", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1487", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-1594", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-172", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-183", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1919", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2143", "mrqa_triviaqa-validation-2175", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2779", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-3213", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3776", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4449", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-4592", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5191", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-5247", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-5420", "mrqa_triviaqa-validation-5450", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5876", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-606", "mrqa_triviaqa-validation-6062", "mrqa_triviaqa-validation-6090", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6349", "mrqa_triviaqa-validation-6357", "mrqa_triviaqa-validation-6395", "mrqa_triviaqa-validation-6417", "mrqa_triviaqa-validation-6512", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-6667", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-6713", "mrqa_triviaqa-validation-6752", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-6892", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-707", "mrqa_triviaqa-validation-7077", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7317", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7456", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7528", "mrqa_triviaqa-validation-7544", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7756", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-928"], "OKR": 0.775390625, "KG": 0.49921875, "before_eval_results": {"predictions": ["1968", "Baden-W\u00fcrttemberg", "NFL owners", "Von Miller", "Ford's Victorian plants\u2014in Broadmeadows and Geelong\u2014will close in October 2016)", "artisans and craftsmen", "The Christmas Invasion", "widespread agitation for constitutional reform", "Georgetown University", "One Flew Over the Cuckoo's Nest", "the United Kingdom", "Matt Dillon", "Hamlet", "dogs must have qualified throughout the...", "Samuel Goldwyn", "John Bull", "Pheonix", "Netherlands", "plankton", "Israel", "jellyfish", "Patti LuPone", "Village People", "a good natured Ice Cream Salesman", "the gods themselves", "chickens", "rodeo", "Sydney", "Nightingale", "the Grand Duchy of Luxembourg", "Reptilia", "Gossage", "orangutans", "Howie Mandel", "suffrage", "Joe", "The Wapshot family", "the Moody Blues", "Amtrak", "Kurt Russell", "tailored", "The Bridges of Madison County", "her daughter, Francesca Hilton", "Rabindranath Tagore", "Galileo Descartes", "Saint Patrick", "the tuba", "(Drepanidinae)", "the Museums", "Tahiti", "Lincoln Tunnel", "New Haven", "Predator 2", "Kanawha River", "1970", "Beaujolais", "Achille Lauro", "Klasky Csupo", "Point of Entry", "football", "19, standing 6'2\"", "Mahatma Gandhi", "Howard", "1877"], "metric_results": {"EM": 0.5, "QA-F1": 0.5674479166666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2980", "mrqa_searchqa-validation-10577", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-12742", "mrqa_searchqa-validation-4887", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-8709", "mrqa_searchqa-validation-13715", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-6839", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-2348", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-914", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-9905", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-14292", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-15790", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-14044", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-4733", "mrqa_naturalquestions-validation-1023", "mrqa_newsqa-validation-3345", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-7387"], "SR": 0.5, "CSR": 0.6443452380952381, "EFR": 0.34375, "Overall": 0.6177752976190476}, {"timecode": 21, "before_eval_results": {"predictions": ["weak force", "over 18 million", "Chicago Pile-1", "English", "Welsh", "the Rhine Valley", "Castle Keep", "\"Harmony\"", "a \"Magical Girl\"", "Mount Rushmore", "University of Florida", "New York", "Richard M. Nixon", "George Soule", "Dublin", "Pretoria", "Dutchman", "Kareem Abdul-Jabbar", "Chicago Cubs", "Copenhagen", "Elvis Presley", "a case", "rod", "herbivore", "Fear", "the Cultural Revolution", "John Updike", "Kilimanjaro", "Mars", "Mongolia", "Gloria Allred", "presbyter", "the Baltimore Symphony Orchestra", "Qike", "Mexico City", "the karting", "caricaturist", "Yale", "Alexander Solzhenitsyn", "Neil Armstrong", "a ton", "1", "Bridget Jones", "Portugal", "Orleans", "a powerboat", "867-5309", "no pun in ten did.", "Henry James", "Seth Rogen", "The Moon Bore Twins", "Mr. Smith Goes to Washington", "James Crofts", "four", "sleeping with the Past", "Robin Hood", "August 24, 1572", "Nassau County", "German", "deployment of unmanned drones", "Kenyan and Somali governments issued a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "Muqtada al-Sadr", "state's attorney", "Turkey"], "metric_results": {"EM": 0.53125, "QA-F1": 0.564889705882353}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9127", "mrqa_squad-validation-5135", "mrqa_searchqa-validation-10307", "mrqa_searchqa-validation-12657", "mrqa_searchqa-validation-2371", "mrqa_searchqa-validation-3217", "mrqa_searchqa-validation-13619", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-15759", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-11882", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-12284", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-3506", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-15403", "mrqa_searchqa-validation-12407", "mrqa_naturalquestions-validation-8728", "mrqa_triviaqa-validation-4019", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2230"], "SR": 0.53125, "CSR": 0.6392045454545454, "EFR": 0.5666666666666667, "Overall": 0.6613304924242425}, {"timecode": 22, "before_eval_results": {"predictions": ["Asia", "Classic", "electromagnetic force", "N Kenyabi", "over 60 percent", "knowledge or skills", "retina", "calligraphy", "Songs of Innocence", "Repent ye", "Van", "Mariah Carey", "Spanish Inquisition", "Princess Pocahontas", "Top chef", "winter", "Milwaukee", "Dylan Thomas", "Aruba", "the Hubble Space Telescope", "Kyushu", "King George I of Great Britain", "Ireland", "SAT or ACT", "Matt Damon", "Laura Geller", "Netflix", "Varietal", "HIV", "Golden Fleece", "Macduff", "The Adventures of Sherlock Holmes", "Varney Airlines", "siamangutan", "right", "a brown rat", "William Shakespeare", "fritter", "Portugal", "Henry Cavendish", "El Salvador", "Jeremy Bentham", "Amerigo Vespucci", "the head swan", "walnuts", "Jerusalem", "Iceland", "all men are created", "Sudan", "Warsaw", "Massachusetts", "Edward G. Robinson", "September 19 - 22, 2017", "Revelation was the last book accepted into the Christian biblical canon", "drawing letters in the air", "Mount Kilimanjaro", "Estonia", "pink", "Bayern Munich", "Friday", "Orange County", "three", "$249", "KBR"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6224228896103896}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8458", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-5718", "mrqa_searchqa-validation-2609", "mrqa_searchqa-validation-12460", "mrqa_searchqa-validation-1918", "mrqa_searchqa-validation-13794", "mrqa_searchqa-validation-7933", "mrqa_searchqa-validation-228", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-13688", "mrqa_searchqa-validation-7123", "mrqa_searchqa-validation-14674", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-13250", "mrqa_searchqa-validation-4943", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-11195", "mrqa_searchqa-validation-10730", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3323", "mrqa_triviaqa-validation-9", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-5518", "mrqa_newsqa-validation-1165"], "SR": 0.53125, "CSR": 0.6345108695652174, "EFR": 0.5, "Overall": 0.6470584239130435}, {"timecode": 23, "before_eval_results": {"predictions": ["Jean- Marc Bosman", "the web", "Genghis Khan", "late", "seafloor spreading", "Pannerdens Kanaal", "John Delaney", "Rita Mae Brown", "Himalayan", "a holdfast", "Sun Wu", "St. Andrew's Cross", "Cadillac", "William Henry Harrison", "Charlatan", "\"Red, Yellow, Blue\"", "Band of Brothers", "Von Bischoff", "Tigrinya", "Department of Justice", "Alice Walker", "Doc Holiday", "Toyota", "Nathan Kelly", "1976", "Jimmy Carter", "Amsterdam", "lunar", "Louis XV", "taking a quick power nap", "prime time", "Perrier", "Hinduism", "Steinbeck", "Rome", "Redcliffe", "Pineapple", "a \"selected track\" scroll", "God and Man at Yale", "Connecticut", "guitar", "oxygen", "July 4, 1826", "The Hoboken Five", "Dan Quayle", "an anymphet", "MiG-31", "OnStar", "the Bodleian Library", "Terrence Malick", "Dionysus", "Cotton Bowl", "ecological", "letter series", "3.99 degrees", "triathlon", "barrels, wooden casks, kegs, or tubs", "Algiers", "Liverpool Bay", "DEVGRU", "Lee Byung-hun", "The Rosie Show", "San Antonio's Brooke Army Medical Center and University Hospital", "a monthly allowance"], "metric_results": {"EM": 0.375, "QA-F1": 0.453125}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4692", "mrqa_squad-validation-6150", "mrqa_searchqa-validation-132", "mrqa_searchqa-validation-5192", "mrqa_searchqa-validation-4170", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-4015", "mrqa_searchqa-validation-1835", "mrqa_searchqa-validation-4788", "mrqa_searchqa-validation-4542", "mrqa_searchqa-validation-9678", "mrqa_searchqa-validation-14277", "mrqa_searchqa-validation-15206", "mrqa_searchqa-validation-14699", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-7372", "mrqa_searchqa-validation-14386", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-9522", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-13112", "mrqa_searchqa-validation-13697", "mrqa_searchqa-validation-3191", "mrqa_searchqa-validation-15218", "mrqa_searchqa-validation-13693", "mrqa_searchqa-validation-193", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-11849", "mrqa_searchqa-validation-16329", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-10326", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9942", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-7348", "mrqa_hotpotqa-validation-3686", "mrqa_hotpotqa-validation-2947"], "SR": 0.375, "CSR": 0.6236979166666667, "EFR": 0.45, "Overall": 0.6348958333333333}, {"timecode": 24, "before_eval_results": {"predictions": ["knight winning a bout.", "lobes", "pedagogy", "smaller, weaker swimmers", "seven", "Constellations", "The Thing", "Paraguay", "Cambridge", "David Hockney", "Aaron Burr", "Michael Caine", "Niagara Falls", "Hunter S. Thompson", "zucchetto", "Oklahoma", "Bridges of Madison County", "Eve", "Gnarls", "photons", "a subregion", "art", "Rafael Nadal", "combustion", "Lance Armstrong", "Ra-Atum", "Prague", "Six Flags Over Texas", "kwanzaa", "artificial", "Jack London", "Like a Rock", "Jim Jarmusch", "Ice Age", "a volcano", "Amerigo Vespucci", "gossip", "India", "Nike", "The Travel Book: A Journey Through Every Country in the World", "Jeff Probst", "Lennox Lewis", "Clarence Birdseye", "Gibraltar", "to protect babies", "Cal Ripken Jr.", "Cole Porter", "Ray Kroc", "vitamin A", "Andorra", "Adrianus", "Monaco", "Thunder Road", "Copernicus", "infidelity", "\"I am trying get the hang of this new fangled writing machine, but am not making a shining success of it.", "Dieppe", "ravens", "the Mohawk, Onondaga, Oneida, Cayuga, Seneca, and Tuscarora peoples", "Ballon d'Or", "Edward III", "served in the military", "cut off his ear and a finger", "Meredith Kercher"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7488839285714286}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1, 0.0, 1.0, 1.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4488", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-16667", "mrqa_searchqa-validation-3469", "mrqa_searchqa-validation-5897", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-16129", "mrqa_searchqa-validation-16506", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-13191", "mrqa_searchqa-validation-5315", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-1762", "mrqa_searchqa-validation-13872", "mrqa_naturalquestions-validation-4429", "mrqa_triviaqa-validation-7287", "mrqa_hotpotqa-validation-3103", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-2022"], "SR": 0.703125, "CSR": 0.6268750000000001, "EFR": 0.2631578947368421, "Overall": 0.5981628289473684}, {"timecode": 25, "before_eval_results": {"predictions": ["1950s through the 1970s", "Los Angeles", "10.0%", "ITT", "Chinese", "an evil person", "\"All Greek To You,\"", "Wet Seal", "Kings", "Daniel Craig", "a bad peace", "winter", "Nevermore", "the basking shark", "Golda Meir", "rats", "heresy", "Budweiser", "Havana", "Jeopardy", "John Fogerty", "Designer", "an nymphet", "Event cards", "St. Mark", "Nellie Bly", "beef", "Morocco", "Home selling", "Fritos", "John Foster Dulles", "freestyle", "Tootsie", "Guatemala", "a bishop", "John Hersey", "the Bachelor", "Golden Pond", "Dachshund", "Paul Simon", "a person practising quackery", "Frank Lloyd Wright", "Jonathan Reinstein", "bony frill", "Jurassic", "The Godfather", "Milwaukee Deep", "Dame Melba", "the Battle of Waterloo", "Michelangelo Buonarroti", "Warsaw", "the Romanovs", "Montreal", "eleven", "4.25 inches", "New Zealand", "the Eagle", "Fancy Dress Shop", "1968", "Eric Bolling", "Africa", "falling space debris", "hatchlings", "Anil Kapoor"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6340277777777779}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12073", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-16542", "mrqa_searchqa-validation-8089", "mrqa_searchqa-validation-905", "mrqa_searchqa-validation-11849", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-4183", "mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-10962", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-1835", "mrqa_searchqa-validation-8264", "mrqa_searchqa-validation-12676", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13749", "mrqa_searchqa-validation-3973", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7147", "mrqa_triviaqa-validation-7433", "mrqa_hotpotqa-validation-5067", "mrqa_hotpotqa-validation-3509", "mrqa_newsqa-validation-4024"], "SR": 0.546875, "CSR": 0.6237980769230769, "EFR": 0.41379310344827586, "Overall": 0.6276744860742705}, {"timecode": 26, "before_eval_results": {"predictions": ["detailed treatment with statistical mechanics", "2011", "the Doctor's third on-screen regeneration", "one MSP", "Spain", "Nine to 5", "a bat", "pharaoh", "Sir Isaac Newton", "San Francisco", "Spider-Man", "Ethiopia", "Pluto", "George Balanchine", "Alice Pearce", "fled", "remoulade", "grottoesque", "radical", "the Great Famine", "Dragster", "Sam Shepard", "the New York Rens", "\"We've Only Just Begun\"", "a random number generator", "Lance Armstrong", "ozone", "liquor", "an asylum", "Fall Out Boy", "John Boehner", "AC/DC", "a science fiction novel", "Anne Shirley", "Fondue", "Laolla Opera House", "a cave with a small cave, perhaps with religious connotations. word on \"G\"", "silver", "JFK", "penicillin", "Ophiuchi", "Shahjahanabad", "giving mouth", "a pumpkins", "Mercury", "a newcomer called Ellis Wyatt", "onboard", "moss", "Malcolm X", "oxygen", "$3,000", "Yellowknife", "the S - stage of interphase", "1924", "Pakhangba", "Chad", "flea", "Dionysus", "Canada's first train robbery", "Tamara Rutland", "Adelaide", "snow", "Stratfor", "The Bronx County District Attorneys Office"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6024305555555556}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-7594", "mrqa_searchqa-validation-12810", "mrqa_searchqa-validation-13682", "mrqa_searchqa-validation-14208", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-2027", "mrqa_searchqa-validation-7421", "mrqa_searchqa-validation-3183", "mrqa_searchqa-validation-15867", "mrqa_searchqa-validation-16374", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11460", "mrqa_searchqa-validation-2385", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-1710", "mrqa_searchqa-validation-2939", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-2789", "mrqa_searchqa-validation-2807", "mrqa_naturalquestions-validation-9670", "mrqa_triviaqa-validation-2655", "mrqa_hotpotqa-validation-4672"], "SR": 0.5625, "CSR": 0.6215277777777778, "EFR": 0.42857142857142855, "Overall": 0.6301760912698413}, {"timecode": 27, "before_eval_results": {"predictions": ["1259", "highways", "marginal", "NFL", "a power", "Aerosmith", "grasshopper", "water power", "balloon", "Regal", "Germany", "-helix", "wasted", "To the Best of My Ability", "Latter-day Saints", "The Tempest", "Dave Durden", "the bull ring", "Pop Goes The World", "Sitka", "Pirates", "Jordan", "Mary Leakey", "The West Wing", "a professor working with Egon Spengler", "Vermouth", "Elton Hercules John", "Buddhism", "Abraham Lincoln", "Whittaker Chambers", "a bat", "the ear", "The Capital Century", "changelings", "Dream Baby", "Mulan", "J. D. Salinger", "Ranch", "ammonia", "USS LST", "globulins", "Manhattan island", "Ivanhoe", "Toyota", "Cerberus", "A woman", "Patrick Henry", "Spanish", "penguin", "Sanford and Son", "The Mole", "Panther chameleon", "the end zone ( 10 yards ) and 7 yards to where the holder places the ball", "Escherichia coli", "Patriots have appeared in the Super Bowl ten times in franchise history", "Georgia", "Normandy", "\"H\"", "Southbank", "country", "four sections", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil", "Cambodian territory", "repaired"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5983207614942528}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.27586206896551724, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11118", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-323", "mrqa_searchqa-validation-5074", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-10086", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-14651", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-15450", "mrqa_searchqa-validation-7543", "mrqa_searchqa-validation-11161", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-1975", "mrqa_searchqa-validation-11400", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-4883", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-14796", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-12368", "mrqa_searchqa-validation-15562", "mrqa_searchqa-validation-8902", "mrqa_naturalquestions-validation-10270", "mrqa_naturalquestions-validation-225", "mrqa_triviaqa-validation-4088", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-453"], "SR": 0.53125, "CSR": 0.6183035714285714, "EFR": 0.5, "Overall": 0.6438169642857142}, {"timecode": 28, "before_eval_results": {"predictions": ["since 2001", "15\u20131", "Solim\u00f5es Basin", "diamond", "A Tale of Two Cities", "The Rime of the Ancient Mariner", "fibre optics", "Coward", "the Liberty Bell", "South Bend", "a keelman", "a hard yellow cheese", "Voting Rights Act", "Graf Zeppelin", "Jerry Mathers", "Shirley Jackson", "Memphis", "life, liberty, and the pursuit of Happiness", "Rope", "a crossword clue", "the Russian Mir", "touchpad", "Reno", "Mary Baker Eddy", "ozone", "Volkswagen", "Best Buy", "armadillos", "Punxsutawney Phil", "diamond", "Jerusalem", "Bangladesh", "The Big Surprise & Who Pays", "A Midsummer Night's Dream", "ginseng", "Treasure Island", "Prospero", "Rwanda", "the opossum rat", "mole", "James Monroe", "I Remember It Well", "abundant", "Mickey Rooney", "The Beggar's Opera", "1", "Book Knowledge Flashcards", "Monopoly", "the Ark of the Covenant", "Ancestral Chart Form", "The Crucible", "Yente", "1936", "unknown origin", "The Sentencing Reform Act, part of the Comprehensive Crime Control Act of 1984", "1969", "Munich", "Canada", "Marilyn Martin", "one", "First Family of Competitive eating", "he checked himself into a Los Angeles mental institution", "U.N. drug chief", "Bhutto narrowly escaped injury"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6527723861283643}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-8384", "mrqa_searchqa-validation-7180", "mrqa_searchqa-validation-12869", "mrqa_searchqa-validation-329", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-1436", "mrqa_searchqa-validation-10857", "mrqa_searchqa-validation-489", "mrqa_searchqa-validation-5758", "mrqa_searchqa-validation-5703", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-4109", "mrqa_searchqa-validation-10071", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-9791", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-2336", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-3037", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-846"], "SR": 0.59375, "CSR": 0.6174568965517242, "EFR": 0.38461538461538464, "Overall": 0.6205707062334217}, {"timecode": 29, "before_eval_results": {"predictions": ["reducing poverty", "\"world classic of epoch-making oratory.\"", "declare martial law", "Chad", "albatross", "Little Red Riding Hood", "Grover Cleveland", "Cosmopolitan", "Yellow Fever", "kinetic", "viruses", "Cessna 172", "Ka-bala", "the Civil War", "Libya", "Giuseppe Garibaldi", "Gang of Four", "piracy", "Edith Wharton", "Anchorage", "cobra", "trout", "\"E\" BAY: A snail in Strasbourg", "Nashville", "Kiev", "Red Bull", "It's Howdy Doody Time", "Martin Luther King", "a nice clean cage", "Steve Austin", "an Asylum", "John Donne", "a jack", "George Sand", "bees have just invaded your property or adjacent area and have now settled in a large clump on a tree, a fence or on some other object", "Vitus Bering", "a swimming pool", "Solomon", "Texas", "cosmic rays", "marigolds", "John \"Jack\" McCall", "Katie Holmes", "an oracle", "Joe DiMaggio", "Charles IV", "the genus Neotoma", "Poetry: A Magazine of Verse", "California", "a bat", "Quiz", "Apple", "William Jennings Bryan", "Zara Larsson and MNEK", "1943", "George III", "Canada", "Herbert Lom Dies", "June 22, 1953", "Synerg Group Corp.", "Saint Motel", "1831", "July", "the 3rd District of Utah"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7174107142857143}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-2151", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-12429", "mrqa_searchqa-validation-14595", "mrqa_searchqa-validation-867", "mrqa_searchqa-validation-10730", "mrqa_searchqa-validation-8420", "mrqa_searchqa-validation-10365", "mrqa_searchqa-validation-4880", "mrqa_searchqa-validation-6883", "mrqa_searchqa-validation-13534", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-5027", "mrqa_searchqa-validation-15920", "mrqa_searchqa-validation-6140", "mrqa_searchqa-validation-12237", "mrqa_triviaqa-validation-63", "mrqa_triviaqa-validation-7313", "mrqa_hotpotqa-validation-4093", "mrqa_newsqa-validation-3175"], "SR": 0.640625, "CSR": 0.6182291666666666, "EFR": 0.5652173913043478, "Overall": 0.6568455615942028}, {"timecode": 30, "UKR": 0.8125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1309", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1830", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2858", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-3829", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-4275", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-5067", "mrqa_hotpotqa-validation-5088", "mrqa_hotpotqa-validation-589", "mrqa_hotpotqa-validation-986", "mrqa_naturalquestions-validation-10630", "mrqa_naturalquestions-validation-1577", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6544", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-989", "mrqa_newsqa-validation-1293", "mrqa_newsqa-validation-1356", "mrqa_newsqa-validation-2124", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3213", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3449", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3538", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-1", "mrqa_searchqa-validation-10025", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10071", "mrqa_searchqa-validation-10086", "mrqa_searchqa-validation-10119", "mrqa_searchqa-validation-10256", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-10283", "mrqa_searchqa-validation-10365", "mrqa_searchqa-validation-10577", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10693", "mrqa_searchqa-validation-10730", "mrqa_searchqa-validation-10899", "mrqa_searchqa-validation-10996", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-1114", "mrqa_searchqa-validation-11220", "mrqa_searchqa-validation-1149", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-1153", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11789", "mrqa_searchqa-validation-11832", "mrqa_searchqa-validation-11849", "mrqa_searchqa-validation-11882", "mrqa_searchqa-validation-11931", "mrqa_searchqa-validation-11974", "mrqa_searchqa-validation-12088", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-12237", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12256", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-12483", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-12634", "mrqa_searchqa-validation-12712", "mrqa_searchqa-validation-12793", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-12816", "mrqa_searchqa-validation-12920", "mrqa_searchqa-validation-13114", "mrqa_searchqa-validation-132", "mrqa_searchqa-validation-13290", "mrqa_searchqa-validation-13374", "mrqa_searchqa-validation-13477", "mrqa_searchqa-validation-13534", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13587", "mrqa_searchqa-validation-13626", "mrqa_searchqa-validation-13714", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13749", "mrqa_searchqa-validation-13754", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-13794", "mrqa_searchqa-validation-13872", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14135", "mrqa_searchqa-validation-14208", "mrqa_searchqa-validation-14272", "mrqa_searchqa-validation-14463", "mrqa_searchqa-validation-14532", "mrqa_searchqa-validation-14674", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14796", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-1499", "mrqa_searchqa-validation-15218", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-15506", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-15673", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-15949", "mrqa_searchqa-validation-16129", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16272", "mrqa_searchqa-validation-16363", "mrqa_searchqa-validation-16372", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16506", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-16597", "mrqa_searchqa-validation-16724", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-1682", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-1710", "mrqa_searchqa-validation-1796", "mrqa_searchqa-validation-1997", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2177", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-2371", "mrqa_searchqa-validation-2483", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2772", "mrqa_searchqa-validation-2869", "mrqa_searchqa-validation-29", "mrqa_searchqa-validation-2925", "mrqa_searchqa-validation-2939", "mrqa_searchqa-validation-2986", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-323", "mrqa_searchqa-validation-3230", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-335", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-3506", "mrqa_searchqa-validation-3733", "mrqa_searchqa-validation-3745", "mrqa_searchqa-validation-3780", "mrqa_searchqa-validation-381", "mrqa_searchqa-validation-3839", "mrqa_searchqa-validation-4071", "mrqa_searchqa-validation-4107", "mrqa_searchqa-validation-4183", "mrqa_searchqa-validation-4387", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-4411", "mrqa_searchqa-validation-4458", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-4788", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-4870", "mrqa_searchqa-validation-4880", "mrqa_searchqa-validation-4887", "mrqa_searchqa-validation-4890", "mrqa_searchqa-validation-4989", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5132", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5595", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-5718", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-5777", "mrqa_searchqa-validation-5896", "mrqa_searchqa-validation-5897", "mrqa_searchqa-validation-6128", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-6790", "mrqa_searchqa-validation-6895", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-6972", "mrqa_searchqa-validation-7123", "mrqa_searchqa-validation-7421", "mrqa_searchqa-validation-7442", "mrqa_searchqa-validation-7464", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-7584", "mrqa_searchqa-validation-7647", "mrqa_searchqa-validation-7694", "mrqa_searchqa-validation-7796", "mrqa_searchqa-validation-7954", "mrqa_searchqa-validation-8045", "mrqa_searchqa-validation-8089", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-8264", "mrqa_searchqa-validation-8292", "mrqa_searchqa-validation-8358", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8611", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8709", "mrqa_searchqa-validation-8814", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-8911", "mrqa_searchqa-validation-8918", "mrqa_searchqa-validation-9138", "mrqa_searchqa-validation-914", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-9252", "mrqa_searchqa-validation-9328", "mrqa_searchqa-validation-9393", "mrqa_searchqa-validation-9452", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-9791", "mrqa_searchqa-validation-9905", "mrqa_squad-validation-10085", "mrqa_squad-validation-10099", "mrqa_squad-validation-10160", "mrqa_squad-validation-10166", "mrqa_squad-validation-10203", "mrqa_squad-validation-10448", "mrqa_squad-validation-10452", "mrqa_squad-validation-10459", "mrqa_squad-validation-108", "mrqa_squad-validation-1106", "mrqa_squad-validation-113", "mrqa_squad-validation-1207", "mrqa_squad-validation-1319", "mrqa_squad-validation-1348", "mrqa_squad-validation-1506", "mrqa_squad-validation-1566", "mrqa_squad-validation-1611", "mrqa_squad-validation-1642", "mrqa_squad-validation-1730", "mrqa_squad-validation-1751", "mrqa_squad-validation-1897", "mrqa_squad-validation-198", "mrqa_squad-validation-2024", "mrqa_squad-validation-2076", "mrqa_squad-validation-226", "mrqa_squad-validation-2277", "mrqa_squad-validation-2286", "mrqa_squad-validation-2318", "mrqa_squad-validation-2541", "mrqa_squad-validation-2613", "mrqa_squad-validation-2642", "mrqa_squad-validation-2653", "mrqa_squad-validation-2709", "mrqa_squad-validation-2908", "mrqa_squad-validation-296", "mrqa_squad-validation-3020", "mrqa_squad-validation-3022", "mrqa_squad-validation-3094", "mrqa_squad-validation-338", "mrqa_squad-validation-341", "mrqa_squad-validation-3461", "mrqa_squad-validation-3478", "mrqa_squad-validation-3524", "mrqa_squad-validation-3563", "mrqa_squad-validation-3568", "mrqa_squad-validation-3600", "mrqa_squad-validation-3632", "mrqa_squad-validation-367", "mrqa_squad-validation-370", "mrqa_squad-validation-3707", "mrqa_squad-validation-3800", "mrqa_squad-validation-3861", "mrqa_squad-validation-3899", "mrqa_squad-validation-3933", "mrqa_squad-validation-404", "mrqa_squad-validation-4053", "mrqa_squad-validation-4099", "mrqa_squad-validation-4103", "mrqa_squad-validation-43", "mrqa_squad-validation-4488", "mrqa_squad-validation-4551", "mrqa_squad-validation-456", "mrqa_squad-validation-4661", "mrqa_squad-validation-4673", "mrqa_squad-validation-468", "mrqa_squad-validation-4711", "mrqa_squad-validation-4797", "mrqa_squad-validation-4915", "mrqa_squad-validation-5008", "mrqa_squad-validation-5080", "mrqa_squad-validation-5135", "mrqa_squad-validation-5168", "mrqa_squad-validation-5185", "mrqa_squad-validation-523", "mrqa_squad-validation-5249", "mrqa_squad-validation-5256", "mrqa_squad-validation-5268", "mrqa_squad-validation-5272", "mrqa_squad-validation-5371", "mrqa_squad-validation-557", "mrqa_squad-validation-5581", "mrqa_squad-validation-5584", "mrqa_squad-validation-564", "mrqa_squad-validation-5715", "mrqa_squad-validation-5785", "mrqa_squad-validation-5800", "mrqa_squad-validation-5837", "mrqa_squad-validation-5902", "mrqa_squad-validation-596", "mrqa_squad-validation-6039", "mrqa_squad-validation-6067", "mrqa_squad-validation-607", "mrqa_squad-validation-6127", "mrqa_squad-validation-6139", "mrqa_squad-validation-614", "mrqa_squad-validation-6186", "mrqa_squad-validation-6197", "mrqa_squad-validation-636", "mrqa_squad-validation-6366", "mrqa_squad-validation-6413", "mrqa_squad-validation-6423", "mrqa_squad-validation-6428", "mrqa_squad-validation-6505", "mrqa_squad-validation-6527", "mrqa_squad-validation-6554", "mrqa_squad-validation-659", "mrqa_squad-validation-6625", "mrqa_squad-validation-6701", "mrqa_squad-validation-6717", "mrqa_squad-validation-6746", "mrqa_squad-validation-6804", "mrqa_squad-validation-6848", "mrqa_squad-validation-6891", "mrqa_squad-validation-6968", "mrqa_squad-validation-7076", "mrqa_squad-validation-7107", "mrqa_squad-validation-7117", "mrqa_squad-validation-7186", "mrqa_squad-validation-7193", "mrqa_squad-validation-7274", "mrqa_squad-validation-7314", "mrqa_squad-validation-7379", "mrqa_squad-validation-7386", "mrqa_squad-validation-7434", "mrqa_squad-validation-7461", "mrqa_squad-validation-7491", "mrqa_squad-validation-7914", "mrqa_squad-validation-7945", "mrqa_squad-validation-7984", "mrqa_squad-validation-8012", "mrqa_squad-validation-8045", "mrqa_squad-validation-8140", "mrqa_squad-validation-8232", "mrqa_squad-validation-8288", "mrqa_squad-validation-8370", "mrqa_squad-validation-8426", "mrqa_squad-validation-8458", "mrqa_squad-validation-8516", "mrqa_squad-validation-8576", "mrqa_squad-validation-8625", "mrqa_squad-validation-865", "mrqa_squad-validation-8707", "mrqa_squad-validation-8710", "mrqa_squad-validation-8719", "mrqa_squad-validation-8726", "mrqa_squad-validation-8789", "mrqa_squad-validation-895", "mrqa_squad-validation-9174", "mrqa_squad-validation-9306", "mrqa_squad-validation-9317", "mrqa_squad-validation-9336", "mrqa_squad-validation-9421", "mrqa_squad-validation-9522", "mrqa_squad-validation-9550", "mrqa_squad-validation-9614", "mrqa_squad-validation-9650", "mrqa_squad-validation-9754", "mrqa_squad-validation-9771", "mrqa_squad-validation-9773", "mrqa_squad-validation-9803", "mrqa_triviaqa-validation-0", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1448", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1594", "mrqa_triviaqa-validation-1673", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2007", "mrqa_triviaqa-validation-21", "mrqa_triviaqa-validation-2276", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-2753", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3213", "mrqa_triviaqa-validation-3260", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-4449", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-4592", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5247", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-5420", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6038", "mrqa_triviaqa-validation-6062", "mrqa_triviaqa-validation-6090", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6349", "mrqa_triviaqa-validation-6512", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6667", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-707", "mrqa_triviaqa-validation-7077", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-7317", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7456", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-7528", "mrqa_triviaqa-validation-7544", "mrqa_triviaqa-validation-7577", "mrqa_triviaqa-validation-7626", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7728", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7756", "mrqa_triviaqa-validation-82", "mrqa_triviaqa-validation-9", "mrqa_triviaqa-validation-928"], "OKR": 0.73828125, "KG": 0.51484375, "before_eval_results": {"predictions": ["42,000", "prayer for grace", "teleforce", "The Boy Who Cried Wolf", "the chow chow", "Jeopardy", "the Northern Mockingbird", "a physician or surgeon", "hibernate", "the Caspian Sea", "Chris Matthews", "New Zealand", "Heracles", "President Nixon", "a fluke", "an angle like the one seen here is referred to as this type of angle", "Mussolini", "Transformers", "A Time to Hope", "Cambodia", "Arthur Balfour", "Harry Reid", "apricot liqueur", "Bundestag", "Ghost", "Tulip poplar", "Christmas", "the Heart of Mary", "a pram", "New Zealand", "Dante Alighieri", "The Princess Diaries", "Edward II", "bananas", "AIDS", "Hope Diamond", "parsley", "Origami", "India", "John Miller", "Dionysus", "spleen", "sand", "Indiana County", "Bob Dylan", "The Monkees", "the Rue Morgue", "a tip-off", "Stephen Crane", "Quiz Show", "Roger Maris", "Dolly Parton", "Jonathan Breck", "Cuernavaca, Durango, and Tepoztl\u00e1n and at the Churubusco Studios", "classical neurology", "John Lennon", "Mull", "Periodic Table", "Buzzcocks", "Erinsborough", "Ministry of European Integration", "200", "English", "observers and other page participants"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6125}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1957", "mrqa_searchqa-validation-3939", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-4363", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-4923", "mrqa_searchqa-validation-15324", "mrqa_searchqa-validation-5603", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-5777", "mrqa_searchqa-validation-7543", "mrqa_searchqa-validation-154", "mrqa_searchqa-validation-3175", "mrqa_searchqa-validation-14396", "mrqa_searchqa-validation-4237", "mrqa_searchqa-validation-1691", "mrqa_searchqa-validation-9333", "mrqa_searchqa-validation-2030", "mrqa_searchqa-validation-1372", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-7849", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-6320", "mrqa_searchqa-validation-3516", "mrqa_naturalquestions-validation-9962", "mrqa_hotpotqa-validation-1451"], "SR": 0.53125, "CSR": 0.6154233870967742, "EFR": 0.5, "Overall": 0.6362096774193547}, {"timecode": 31, "before_eval_results": {"predictions": ["architect or engineer", "a distant territory", "Planet of the Apes", "a sheep's milk cheese", "Barnard College", "pickling", "John Cheever", "Warsaw", "carrion", "a discus", "Civil War", "Elvis Presley", "polygon", "El Salvador", "Quincy", "TWAIN TRACTS", "Robert Frost", "Great Expectations", "lice", "a dormouse", "Spain", "Irving G. Thalberg Memorial Award", "hypothermia", "deep-rooted", "uranium", "harpoons", "a chainaw", "Macarena", "horses", "Mayan", "The Hustler", "Toms de Torquemada", "Samuel Colt", "Mount Rushmore", "China", "a trapdoor", "Trinity College", "Grey's Anatomy", "SeaWorld", "El Salvador", "Angola", "vinegar", "Mississippi", "a easement", "$800, 12. Dance style of brothers & Kennedy", "Al Gore", "FBI", "Monaco", "Jacques Cartier", "codemonkey13981", "Times Square", "beef", "September 29, 2017", "pre-Columbian times, the American Bison, is difficult to domesticate and was never domesticated by Native Americans", "Wichita, Kansas ( Host : Wichita State University)", "Crystal Palace", "African antelope", "Prince Harry", "Wake Island", "The Thomas Crown Affair", "8,648", "alcohol", "President Obama and Britain's Prince Charles", "Columbia, Illinois"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5911458333333334}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9802", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-14633", "mrqa_searchqa-validation-12966", "mrqa_searchqa-validation-16933", "mrqa_searchqa-validation-15896", "mrqa_searchqa-validation-6026", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-5323", "mrqa_searchqa-validation-9685", "mrqa_searchqa-validation-5902", "mrqa_searchqa-validation-5819", "mrqa_searchqa-validation-2101", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-16042", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-1559", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-11658", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-3058", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-6415", "mrqa_hotpotqa-validation-4612", "mrqa_newsqa-validation-1754", "mrqa_newsqa-validation-619"], "SR": 0.515625, "CSR": 0.6123046875, "EFR": 0.4838709677419355, "Overall": 0.6323601310483872}, {"timecode": 32, "before_eval_results": {"predictions": ["Fifth, Sixth and Seventh", "the \"blurring of theological and confessional differences in the interests of unity.\"", "the Sting", "Jamaica", "silver", "trench", "Jabez Stone", "Afghanistan", "the Civil War", "reptiles", "ulcer", "Cubism", "Investopedia", "k.d. lang", "Florence Nightingale", "Wyoming", "republic", "a pinhead", "gaijin", "1984", "Alexander Calder", "Fertile Crescent", "Proud Mary", "kallos", "Coneheads", "Manhattan Project", "Oh Baby", "punk", "Louis XIV", "Toshiba", "Sinclair Lewis", "an eclipse", "Sir Isaac Newton", "Islam", "Michael Clayton", "Michigan", "Delphi", "dormouse", "Kiss And Kill Killers", "itanic", "Athens", "Julius Caesar", "Abner", "tribbles", "angular", "Curb Your Enthusiasm", "quartz", "Vietnamese", "dulcimer", "the green girl", "Indiana", "Generation X", "a Nosferatu - like Dracula who lives on the bottom floor of the flat in a stone coffin and generally keeps to himself", "Department of Health and Human Services, Office of Inspector General, as of 2000 there were more than 6,000 entities issuing birth certificates", "1971 -- 1979", "hydrogen", "Barack Obama", "1768", "\"Trzy kolory\", French: \" Trois couleurs\"", "45th", "5", "Stansted and Gatwick", "Cameroon", "Vernon Forrest"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5778769841269842}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.42857142857142855, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7858", "mrqa_searchqa-validation-15690", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-6650", "mrqa_searchqa-validation-8222", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-1537", "mrqa_searchqa-validation-3080", "mrqa_searchqa-validation-16444", "mrqa_searchqa-validation-7133", "mrqa_searchqa-validation-12221", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-16026", "mrqa_searchqa-validation-14908", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-12958", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-15962", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-5165", "mrqa_naturalquestions-validation-6575", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-6252", "mrqa_triviaqa-validation-216", "mrqa_triviaqa-validation-4589", "mrqa_hotpotqa-validation-4647", "mrqa_newsqa-validation-1477", "mrqa_newsqa-validation-1670"], "SR": 0.546875, "CSR": 0.6103219696969697, "EFR": 0.4827586206896552, "Overall": 0.631741118077325}, {"timecode": 33, "before_eval_results": {"predictions": ["driving Israel out of the Gaza Strip", "frontier market or occasionally an emerging market", "Moreau", "the Hatfield- McCoy feud", "Admiral Nelson", "Rio Grande", "the Old Guard", "Earhart", "Halloween", "Jane Austen", "New Zealand", "Vietnam War", "China", "eland", "William Jennings Bryan", "American Sign Language", "a Rose Garden", "McMillan", "bamboo", "humpback whale", "1000", "Carrie Bradshaw", "Oliver Stone", "a depression", "All the King's Men", "a millimeter", "Peking Man", "what you did", "eggplant", "Germany", "sea", "a grade point average", "tea", "The Scene", "Detroit", "Versailles", "a scotch", "Daytona", "World Trade Center", "Yellowstone", "Napoleon", "Martin Luther", "a whittling", "Prince", "Otis Elevator", "AllMusic", "Heroes", "Angel", "Hawaii", "Formula One", "Rock Band", "Mickey Rooney", "her twin sister Phoebe ( MacKenzie Mauzy )", "Valmiki", "18th century", "Valentine Dyall", "Dnieper", "Ytterby", "Rain Man", "Rochester", "Tom Kartsotis", "George Washington", "\"Up\"", "Alan Graham"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6491071428571429}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3416", "mrqa_searchqa-validation-4255", "mrqa_searchqa-validation-11871", "mrqa_searchqa-validation-15092", "mrqa_searchqa-validation-14003", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-7435", "mrqa_searchqa-validation-4804", "mrqa_searchqa-validation-1362", "mrqa_searchqa-validation-1537", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-3877", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-11992", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-8883", "mrqa_searchqa-validation-5014", "mrqa_searchqa-validation-15121", "mrqa_searchqa-validation-8510", "mrqa_searchqa-validation-16603", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-4569", "mrqa_triviaqa-validation-2480", "mrqa_hotpotqa-validation-5442", "mrqa_newsqa-validation-4104"], "SR": 0.5625, "CSR": 0.6089154411764706, "EFR": 0.2857142857142857, "Overall": 0.5920509453781513}, {"timecode": 34, "before_eval_results": {"predictions": ["The Trial of a Time Lord", "UHF", "The Simpsons Movie", "neurosis", "Falstaff", "Madonna", "dogs", "Nathaniel Hawthorne", "a bad peace", "a banzai", "Quiz Show", "Montenegro", "Mountain Dew", "Bizarro", "Aislinn Derbez", "a bat", "Muddy Waters", "Minnesota", "the Declaration of Independence", "aunts", "mongoose", "Desperate Housewives", "Rockefeller Center", "a Garlic Yogurt Dip", "the First Lady", "horse latitudes", "50 million", "Tom Cruise", "Birtwick", "Napoleon Bonaparte", "Johann Strauss II", "James Madison", "the Amistad", "Marie Antoinette", "Colorado", "the adrenal gland, postganglionic neurons of the sympathetic nervous system, and part of the brain called the locus coerules", "the Harry Potter novel series", "Alabama", "Antarctica", "Good Humour", "Ned Kelly", "Sin City", "a fish", "George F. Babbitt", "a pig", "hearsay", "the Apollo 11 mission", "Herakles", "Nintendo", "the Turkish Delight", "Rafael Nadal", "Dachshund", "Henry Purcell", "the adult Nala was voiced by American actress Moira Kelly", "Teddy Randazzo, Bobby Weinstein, and Lou Stallman", "Mel Blanc", "Australia", "Nietzsche", "Second World War", "Scandinavian design", "1951", "a funeral", "1995", "her family"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6465825534759357}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.8, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-13873", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-13717", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-16250", "mrqa_searchqa-validation-10235", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-800", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-2781", "mrqa_searchqa-validation-2365", "mrqa_searchqa-validation-6620", "mrqa_searchqa-validation-12919", "mrqa_searchqa-validation-12235", "mrqa_searchqa-validation-13820", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5824", "mrqa_searchqa-validation-6570", "mrqa_searchqa-validation-3461", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-10213", "mrqa_hotpotqa-validation-5320", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1276"], "SR": 0.5625, "CSR": 0.6075892857142857, "EFR": 0.35714285714285715, "Overall": 0.6060714285714285}, {"timecode": 35, "before_eval_results": {"predictions": ["University of Paris", "16,000", "Agent Orange", "Walter Cronkite", "a tea", "Michelle Pfeiffer", "a psychic midget", "Caledonia", "Steve Austin", "a \"red eft\"", "Bolshoi Ballet", "Tulip", "Calvin Coolidge", "dynamite", "the Redstone", "Bangladesh", "Slovakia", "the opossum rat", "St. Francis of Assisi", "AARP", "white", "serenity", "Abraham Lincoln", "dolomite", "George S. Kaufman", "a rolling stone", "the Elysian Fields", "Linus Pauling", "one", "the occupation", "Albert Einstein", "right", "Peter Pan", "Johnny", "Casino Royale", "Ponies", "a rubaiyat", "$20", "Hazmat Diver", "Taoism", "the Indus", "Iowa", "septum", "Elmore Leonard", "The Tolkien", "John Glenn", "the Dead Sea", "dark energy", "Cannonball Adderley", "Paris", "Dead Ringers", "khaki", "2018", "God of War III ( 2010)", "the probability of rejecting the null hypothesis given that it is true", "Jamaica", "tsarevitch", "the Prussian", "Rawhide", "Loughborough Technical Institute", "anabolic-androgenic", "Florida's Everglades", "vegan", "since 1983"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6182291666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.5, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-129", "mrqa_searchqa-validation-15031", "mrqa_searchqa-validation-9468", "mrqa_searchqa-validation-13393", "mrqa_searchqa-validation-816", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-8170", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-16397", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-11364", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-8725", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-1919", "mrqa_searchqa-validation-4390", "mrqa_searchqa-validation-4497", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-4206", "mrqa_triviaqa-validation-4453", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-3117", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3125"], "SR": 0.578125, "CSR": 0.6067708333333333, "EFR": 0.4074074074074074, "Overall": 0.6159606481481481}, {"timecode": 36, "before_eval_results": {"predictions": ["wood or coal", "white", "adaptive cruise control", "Green Eggs and Ham", "Amsterdam", "Andrew Jackson", "A Fable", "Pisa", "Trinity College", "President Clinton", "Hammurabi", "Elizabeth Garrett Anderson", "The id", "Riga", "a trumpet", "Celine Dion", "Guernsey", "Singapore", "roller coaster", "India", "Luxembourg", "a lock", "Aglauros", "the French", "dip", "charlotte russe", "Smith & Wesson", "\"A,\" and \"B\"", "kwanzaa", "a double curve", "Holstein", "Martin Luther King Day", "The Apprenticeship", "paprika", "DonCrucys", "John Waters", "the Germans", "Neruda", "the Sweater Girl", "Slavic", "Sicily", "the Caspian Sea", "Joseph Conrad", "the Lone Ranger", "the Santa Maria delle Grazie", "stability control", "whales", "the Pillsbury Bake-Off", "Alexandria", "cable cars", "Diane Ladd", "the European economy had collapsed", "1937", "August 9, 1945", "Brunei", "Benny Hill", "the Old Vic", "23 November 1946", "the \"eternal outsider, the sardonic drifter,\" someone who rebels against the social structure.", "\u00c6thelstan", "Jeff Klein", "1,500", "he is committed to equality, citing the repeal of the military's \"don't ask, don't tell\" policy", "David Graham"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5236979166666667}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.1, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3491", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-134", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-10137", "mrqa_searchqa-validation-9883", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-16932", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-10622", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-8173", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-10477", "mrqa_searchqa-validation-5565", "mrqa_searchqa-validation-1141", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-11174", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-12801", "mrqa_searchqa-validation-1989", "mrqa_searchqa-validation-13939", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-1664", "mrqa_triviaqa-validation-626", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-2300", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-258", "mrqa_triviaqa-validation-3013"], "SR": 0.4375, "CSR": 0.6021959459459459, "EFR": 0.2777777777777778, "Overall": 0.5891197447447448}, {"timecode": 37, "before_eval_results": {"predictions": ["near their current locations", "George Prescott Bush (born 1976), son of Jeb Bush, elected commissioner of the... Marvin Pierce Bush", "a money order", "Ismene", "Hawaii", "the Erie Canal", "(Catarinas, Mariquitas, Coccinelle, Lady birds)", "Apennines", "Ugly Betty", "Gulliver", "California", "the bullseye", "zynys", "engine", "lobotomy", "mariachi", "Cameron Crowe", "Quiz", "the pancreas", "Bednye liudi", "a Beretta", "a fledgling robin", "midnight blue", "Beethoven", "Andrew Johnson", "Tokyo", "1st August", "porcelain", "Eddie Wolfe", "25 years", "The Guns of Navarone", "Afghanistan", "God", "Igneous", "morphine", "root", "Bugs Bugs", "the Tigris", "Bailiwick of Guernsey", "Dracula", "Herbert", "Paul Simon", "The Great Serpent Mound", "beef", "Black Death", "Nancy Pelosi", "Annapolis", "the Philistines", "libretti", "Joe Biden", "Nebuchadnezzar", "state capitalism", "non-ferrous", "Cornett family", "1962", "Oxalic", "the Most Rev and Rt Hon George Carey", "hiphop", "American", "2,099", "Karen Floyd", "early detection and helping other women cope with the disease", "personally", "1903"], "metric_results": {"EM": 0.625, "QA-F1": 0.6600694444444444}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-1535", "mrqa_searchqa-validation-1847", "mrqa_searchqa-validation-7542", "mrqa_searchqa-validation-13983", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-15712", "mrqa_searchqa-validation-9155", "mrqa_searchqa-validation-16357", "mrqa_searchqa-validation-15981", "mrqa_searchqa-validation-4500", "mrqa_searchqa-validation-16727", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-202", "mrqa_searchqa-validation-3363", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-10946", "mrqa_naturalquestions-validation-953", "mrqa_triviaqa-validation-1177", "mrqa_triviaqa-validation-5047", "mrqa_hotpotqa-validation-550", "mrqa_newsqa-validation-1301"], "SR": 0.625, "CSR": 0.602796052631579, "EFR": 0.2916666666666667, "Overall": 0.5920175438596491}, {"timecode": 38, "before_eval_results": {"predictions": ["Tethys", "Hawaii", "Hudson Bay Company", "Olympia Dukakis", "the campesinos", "a peter pan collar, stitched front band and front button closing", "John Hopkins", "dogs", "Carl Jung", "crickets", "Tiger Woods", "Sherlock Holmes", "Mark Cuban", "Picasso", "basketball", "pearl", "the Hubble Space Telescope", "a parrot", "Lake Maracaibo", "Montenegro", "Oprah Winfrey", "terrorists", "cytokinesis", "an improvement of 160 SAT points or 4 ACT points on your score", "Beethoven", "Davy Crockett", "the Golden Hind", "limestone", "a globe", "Europe", "Panama", "anemia", "Ted", "the Pacific Ocean", "Moscow", "francs", "B", "creative", "Aswan", "Guernica", "Israel", "Velveeta", "mushrooms", "the English Channel", "Magellan", "Ray Kroc", "President George W. Bush", "The Monkees", "Blair Brown", "Kilimanjaro", "Hugh Laurie", "Gupta", "Justin Timberlake", "plate tectonics", "London", "Mrs Gibbons", "World War II", "Carl Michael Edwards", "Captain B.J. Hunnicutt", "AMC", "Vandenberg Air Force Base", "The Golden Girls", "9", "resources of fish"], "metric_results": {"EM": 0.5625, "QA-F1": 0.609375}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13911", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-701", "mrqa_searchqa-validation-16929", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-8006", "mrqa_searchqa-validation-16700", "mrqa_searchqa-validation-14905", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-16830", "mrqa_searchqa-validation-9400", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-16121", "mrqa_searchqa-validation-16340", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-12556", "mrqa_searchqa-validation-10147", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-1865", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-803", "mrqa_newsqa-validation-4193", "mrqa_newsqa-validation-4170"], "SR": 0.5625, "CSR": 0.6017628205128205, "EFR": 0.4642857142857143, "Overall": 0.6263347069597069}, {"timecode": 39, "before_eval_results": {"predictions": ["the Sunnah and Ahadith", "Martin Luther King Day", "Athena", "Iceland", "SOVIET TISMS", "a salamander", "Anzio", "Donald Trump", "the Big Berthas", "Conjunction Junction", "a barrel", "In God We Trust", "James Ross Clemens", "a visor", "contemporary", "Neverbeen Kissed", "1201", "a wheel", "amu", "Eurythmics", "marriage", "the jedoublen/jeopardy", "Samuel Goldwyn", "Judi Dench", "New York", "a pyramid", "Luffa", "Plutarch", "a suffix", "Poseidon", "Deepak Chopra", "Pretoria", "a clique", "Charlotte Corday", "Iwo Jima", "alexandrite", "Target", "Tennessee Valley Authority (TVA)", "Boston Marathon", "Budweiser", "delete one letter/space", "Beyond the Sea", "Cyprus", "Nancy Drew", "Yale", "The Twilight Zone", "New Zealand", "Rodgers & Hammerstein", "Levi's", "Rene Lacoste", "the twelfth paired cranial nerve", "political ideology", "There are 961 cities in the sections below", "The New Day became the longest reigning WWE Tag Team Champions in the title's history, breaking the old record of 331 days previously set by Paul London and Brian Kendrick", "oil", "New", "barrels, wooden casks, kegs, or tubs", "Tatton Park", "Ubba", "MediaCityUK", "the bench", "$5.5 billion", "the FBI", "George Melly"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5760416666666667}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true], "QA-F1": [0.0, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2119", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4376", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-15229", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-8354", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-7966", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-16695", "mrqa_searchqa-validation-15313", "mrqa_searchqa-validation-15470", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-7348", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2579", "mrqa_newsqa-validation-1263"], "SR": 0.515625, "CSR": 0.599609375, "EFR": 0.45161290322580644, "Overall": 0.6233694556451613}, {"timecode": 40, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1480", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2858", "mrqa_hotpotqa-validation-2883", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-3682", "mrqa_hotpotqa-validation-419", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4898", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-698", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-10630", "mrqa_naturalquestions-validation-1577", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-3637", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6049", "mrqa_naturalquestions-validation-6544", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9896", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1503", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-2153", "mrqa_newsqa-validation-237", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3524", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-4193", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-10119", "mrqa_searchqa-validation-10235", "mrqa_searchqa-validation-10256", "mrqa_searchqa-validation-10326", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10388", "mrqa_searchqa-validation-10477", "mrqa_searchqa-validation-10550", "mrqa_searchqa-validation-1057", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10850", "mrqa_searchqa-validation-10946", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11161", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11252", "mrqa_searchqa-validation-11330", "mrqa_searchqa-validation-11485", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-1153", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-11667", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-11727", "mrqa_searchqa-validation-11754", "mrqa_searchqa-validation-11998", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12073", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-12139", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-12235", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-12384", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12638", "mrqa_searchqa-validation-12735", "mrqa_searchqa-validation-12793", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-13020", "mrqa_searchqa-validation-132", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-13250", "mrqa_searchqa-validation-1327", "mrqa_searchqa-validation-13318", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-13557", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-1367", "mrqa_searchqa-validation-13714", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13749", "mrqa_searchqa-validation-13754", "mrqa_searchqa-validation-13794", "mrqa_searchqa-validation-13807", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-13983", "mrqa_searchqa-validation-14044", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-14391", "mrqa_searchqa-validation-14463", "mrqa_searchqa-validation-14505", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-14749", "mrqa_searchqa-validation-1478", "mrqa_searchqa-validation-14817", "mrqa_searchqa-validation-14905", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-14982", "mrqa_searchqa-validation-1499", "mrqa_searchqa-validation-15092", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-15229", "mrqa_searchqa-validation-15395", "mrqa_searchqa-validation-15441", "mrqa_searchqa-validation-15562", "mrqa_searchqa-validation-15644", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-15911", "mrqa_searchqa-validation-15951", "mrqa_searchqa-validation-1598", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16250", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16328", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-16409", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-16597", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16604", "mrqa_searchqa-validation-16724", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-16803", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16938", "mrqa_searchqa-validation-16943", "mrqa_searchqa-validation-1710", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1762", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-1847", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1997", "mrqa_searchqa-validation-2163", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2371", "mrqa_searchqa-validation-2483", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2641", "mrqa_searchqa-validation-2789", "mrqa_searchqa-validation-2925", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-3363", "mrqa_searchqa-validation-3492", "mrqa_searchqa-validation-3536", "mrqa_searchqa-validation-3583", "mrqa_searchqa-validation-3643", "mrqa_searchqa-validation-3654", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-3745", "mrqa_searchqa-validation-381", "mrqa_searchqa-validation-3877", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3973", "mrqa_searchqa-validation-4069", "mrqa_searchqa-validation-4071", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-4149", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-432", "mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-461", "mrqa_searchqa-validation-4648", "mrqa_searchqa-validation-4788", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4838", "mrqa_searchqa-validation-4880", "mrqa_searchqa-validation-4923", "mrqa_searchqa-validation-4989", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5074", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5150", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5212", "mrqa_searchqa-validation-5417", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5548", "mrqa_searchqa-validation-5584", "mrqa_searchqa-validation-5603", "mrqa_searchqa-validation-5616", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5902", "mrqa_searchqa-validation-6138", "mrqa_searchqa-validation-6250", "mrqa_searchqa-validation-6315", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-6472", "mrqa_searchqa-validation-6481", "mrqa_searchqa-validation-6620", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-6791", "mrqa_searchqa-validation-6976", "mrqa_searchqa-validation-6993", "mrqa_searchqa-validation-7133", "mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-7220", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7266", "mrqa_searchqa-validation-7388", "mrqa_searchqa-validation-7594", "mrqa_searchqa-validation-7723", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7850", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7933", "mrqa_searchqa-validation-7962", "mrqa_searchqa-validation-8003", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-816", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8510", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8709", "mrqa_searchqa-validation-8814", "mrqa_searchqa-validation-8824", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-8977", "mrqa_searchqa-validation-9034", "mrqa_searchqa-validation-9098", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-9452", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9731", "mrqa_searchqa-validation-9817", "mrqa_searchqa-validation-9905", "mrqa_searchqa-validation-9996", "mrqa_squad-validation-10175", "mrqa_squad-validation-1029", "mrqa_squad-validation-10448", "mrqa_squad-validation-10459", "mrqa_squad-validation-1046", "mrqa_squad-validation-10492", "mrqa_squad-validation-1106", "mrqa_squad-validation-1605", "mrqa_squad-validation-1621", "mrqa_squad-validation-1764", "mrqa_squad-validation-1815", "mrqa_squad-validation-1888", "mrqa_squad-validation-2076", "mrqa_squad-validation-2105", "mrqa_squad-validation-2107", "mrqa_squad-validation-2122", "mrqa_squad-validation-2275", "mrqa_squad-validation-2332", "mrqa_squad-validation-2347", "mrqa_squad-validation-2450", "mrqa_squad-validation-2485", "mrqa_squad-validation-2534", "mrqa_squad-validation-2546", "mrqa_squad-validation-2622", "mrqa_squad-validation-2653", "mrqa_squad-validation-2667", "mrqa_squad-validation-2830", "mrqa_squad-validation-2980", "mrqa_squad-validation-2991", "mrqa_squad-validation-3022", "mrqa_squad-validation-3187", "mrqa_squad-validation-3294", "mrqa_squad-validation-3335", "mrqa_squad-validation-3357", "mrqa_squad-validation-3366", "mrqa_squad-validation-341", "mrqa_squad-validation-3524", "mrqa_squad-validation-3642", "mrqa_squad-validation-3643", "mrqa_squad-validation-3701", "mrqa_squad-validation-3728", "mrqa_squad-validation-3800", "mrqa_squad-validation-3932", "mrqa_squad-validation-4016", "mrqa_squad-validation-4103", "mrqa_squad-validation-4194", "mrqa_squad-validation-4256", "mrqa_squad-validation-4293", "mrqa_squad-validation-4462", "mrqa_squad-validation-4518", "mrqa_squad-validation-4646", "mrqa_squad-validation-4731", "mrqa_squad-validation-4844", "mrqa_squad-validation-4847", "mrqa_squad-validation-4994", "mrqa_squad-validation-5098", "mrqa_squad-validation-5136", "mrqa_squad-validation-5171", "mrqa_squad-validation-523", "mrqa_squad-validation-5404", "mrqa_squad-validation-5428", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5584", "mrqa_squad-validation-5589", "mrqa_squad-validation-5602", "mrqa_squad-validation-5680", "mrqa_squad-validation-5780", "mrqa_squad-validation-5837", "mrqa_squad-validation-5890", "mrqa_squad-validation-5902", "mrqa_squad-validation-5992", "mrqa_squad-validation-6017", "mrqa_squad-validation-6063", "mrqa_squad-validation-6165", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6360", "mrqa_squad-validation-6366", "mrqa_squad-validation-6423", "mrqa_squad-validation-6505", "mrqa_squad-validation-6608", "mrqa_squad-validation-6779", "mrqa_squad-validation-7076", "mrqa_squad-validation-7107", "mrqa_squad-validation-7117", "mrqa_squad-validation-7141", "mrqa_squad-validation-7166", "mrqa_squad-validation-7193", "mrqa_squad-validation-7707", "mrqa_squad-validation-7725", "mrqa_squad-validation-7745", "mrqa_squad-validation-7786", "mrqa_squad-validation-7849", "mrqa_squad-validation-7914", "mrqa_squad-validation-7959", "mrqa_squad-validation-7976", "mrqa_squad-validation-8012", "mrqa_squad-validation-8129", "mrqa_squad-validation-8130", "mrqa_squad-validation-8151", "mrqa_squad-validation-8204", "mrqa_squad-validation-8250", "mrqa_squad-validation-8296", "mrqa_squad-validation-831", "mrqa_squad-validation-8343", "mrqa_squad-validation-8636", "mrqa_squad-validation-8707", "mrqa_squad-validation-8982", "mrqa_squad-validation-9074", "mrqa_squad-validation-9154", "mrqa_squad-validation-9174", "mrqa_squad-validation-9205", "mrqa_squad-validation-921", "mrqa_squad-validation-9306", "mrqa_squad-validation-9324", "mrqa_squad-validation-9519", "mrqa_squad-validation-9545", "mrqa_squad-validation-960", "mrqa_squad-validation-9614", "mrqa_squad-validation-9803", "mrqa_squad-validation-9970", "mrqa_triviaqa-validation-0", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-1149", "mrqa_triviaqa-validation-1177", "mrqa_triviaqa-validation-119", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1902", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-208", "mrqa_triviaqa-validation-216", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-4020", "mrqa_triviaqa-validation-403", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4058", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-43", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4368", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4592", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5062", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5114", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-5297", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-6038", "mrqa_triviaqa-validation-623", "mrqa_triviaqa-validation-624", "mrqa_triviaqa-validation-6281", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6417", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6713", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7137", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-7577", "mrqa_triviaqa-validation-7622", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7761", "mrqa_triviaqa-validation-82", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-928"], "OKR": 0.748046875, "KG": 0.52265625, "before_eval_results": {"predictions": ["antibonding", "the low dungeon", "1789-99", "London", "Athena", "Deep Purple", "a flip", "sniper", "Athens", "Court Jester or Fool", "Aristophanes", "Costa Rica", "copper", "Lord Nelson", "Youth International Party", "Manx", "Defoe", "draft-ietf-mmusic-rfc2326bis-04", "USDA", "landfills", "the American flag", "Peter Jackson", "Austin Powers", "Michael Caine", "Lake Victoria", "Atlantic bluefin tuna", "Tejano", "Puget Sound", "Belarus", "Fauve", "ARTHUR MURAMA", "The Big Easy", "Michael Jackson", "a tee box", "Mendel", "Newman", "Vaslav Nijinsky", "Pinocchio", "Niels Bohr", "Charles Mathews", "the troposphere", "Barbara Bush", "a ruby", "\"Suspicious Minds\"", "Torvill and Dean", "Q- tip", "bovine encephalopathy", "Coca-Cola", "Fall Out Boy", "suffrage", "Willie Pearl Russell", "Hugo Weaving", "Ant & Dec", "Isma'il Pasha", "ABBA", "heart failure", "Machine", "1978", "National Socialists", "Vernon Kay", "Secretary Janet Napolitano", "Chester Arthur Stiles", "Cardinal Glennon Children's Medical Center", "United States"], "metric_results": {"EM": 0.5, "QA-F1": 0.5919642857142857}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.7142857142857143, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9464", "mrqa_searchqa-validation-9000", "mrqa_searchqa-validation-15999", "mrqa_searchqa-validation-11871", "mrqa_searchqa-validation-8391", "mrqa_searchqa-validation-16146", "mrqa_searchqa-validation-6743", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-6454", "mrqa_searchqa-validation-8240", "mrqa_searchqa-validation-11789", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-4987", "mrqa_searchqa-validation-670", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-13407", "mrqa_searchqa-validation-8723", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-2188", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-5303", "mrqa_naturalquestions-validation-7022", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-5611", "mrqa_hotpotqa-validation-4034", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-352", "mrqa_hotpotqa-validation-5079"], "SR": 0.5, "CSR": 0.5971798780487805, "EFR": 0.46875, "Overall": 0.6263109756097561}, {"timecode": 41, "before_eval_results": {"predictions": ["October 2007", "NAFTA", "INXS", "Punky Night", "gold", "a ship", "All for Our Country", "artichoke", "eyes", "Algeria", "cassowary", "Dwight D. Eisenhower", "kryptonite", "masses", "Thomas Jefferson", "Jaclyn Smith", "Elizabeth I", "Virginia Woolf", "Carousel", "Wes Craven", "sextuplets", "ovaries", "a fever", "Rosa Parks", "candy cane", "The Big Sleep", "a woman", "a transatlantic ocean liner", "Menudo", "bankrupt", "Beyonce", "Jacob Marley", "Kaiser Wilhelm II", "F-5", "Pisa", "cards", "Graf Zeppelin", "Helen Carlisle", "Charles Lindbergh", "The Raven", "Jack Johnson", "Thomas Edison", "New Jersey", "Volkswagen", "Latter-day", "a college degree", "mint", "Yorkshire", "Bob Hope", "Marathon", "Sidney Poitier", "Mickey Mantle", "an unknown recipient", "a set of related data", "The Lone Gunmen", "Wolf Hall", "M65", "Les Temps modernes", "North Avenue and First Avenue", "Great Lakes and Midwestern", "President George Bush", "$1.4 million", "that she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "Albrecht D\u00fcrer"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6850581709956709}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.42857142857142855, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-536", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-7131", "mrqa_searchqa-validation-9241", "mrqa_searchqa-validation-12925", "mrqa_searchqa-validation-7848", "mrqa_searchqa-validation-13074", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-11250", "mrqa_searchqa-validation-13386", "mrqa_searchqa-validation-11411", "mrqa_searchqa-validation-10636", "mrqa_searchqa-validation-3214", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-223", "mrqa_searchqa-validation-4873", "mrqa_naturalquestions-validation-2956", "mrqa_newsqa-validation-2677", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-3809", "mrqa_triviaqa-validation-837"], "SR": 0.609375, "CSR": 0.5974702380952381, "EFR": 0.4, "Overall": 0.6126190476190476}, {"timecode": 42, "before_eval_results": {"predictions": ["30 November 1963", "Winslow Homer", "Supernanny", "Princess Anne", "Kashmir", "tornado", "Dead Ringers", "Estonia", "Women in Love", "Devil's Advocate", "Selig", "Princeton University", "Texas", "Herman Melville", "John Tyler", "Paris", "Pacific Time Zone", "Swift", "a trident", "San Antonio", "Alexander Solzhenitsyn", "dynamite", "artillery", "a mermaid", "the Olsen Twins", "Ford", "consonants", "Alaska", "banc", "blushes", "Sikhism", "Treaty of this", "William Pitt the Younger", "Toyota", "sea shore", "nectar", "Samsonite", "Census Bureau", "President Nixon", "beans", "a ditch", "St. Louis", "Aaron Burr", "baht", "Hearst", "Custer", "February", "Cubism", "Jan Hus", "sortir", "Halloween", "Rory Mc Ilroy", "Al Pacino", "Chinese", "the keel", "Ceylon", "Puff the Magic Dragon", "Brian Stokes Mitchell", "ZZ Top, Lynyrd Skynyrd", "Regional League North", "Fargo", "The Book", "militants", "Quentin Coldwater"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5817708333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-8797", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-11269", "mrqa_searchqa-validation-44", "mrqa_searchqa-validation-13346", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-14871", "mrqa_searchqa-validation-11673", "mrqa_searchqa-validation-3989", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-5076", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-5296", "mrqa_searchqa-validation-2438", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-7468", "mrqa_searchqa-validation-10672", "mrqa_naturalquestions-validation-5235", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-8660", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-3809", "mrqa_hotpotqa-validation-4767", "mrqa_newsqa-validation-3836", "mrqa_newsqa-validation-2883", "mrqa_hotpotqa-validation-1032"], "SR": 0.484375, "CSR": 0.5948401162790697, "EFR": 0.42424242424242425, "Overall": 0.6169415081042988}, {"timecode": 43, "before_eval_results": {"predictions": ["Sundays", "Calvin Coolidge", "Independence Day", "Lincoln", "Sunni", "RBI", "Leather Tuscadero", "a snowy owl", "a body, body part, or personal object", "Joshua", "guttural", "lapis lazuli", "Dorothy", "sentences", "toga", "9", "the Battle of the Little Bighorn", "napalm", "nougat", "Hieronymus Bosch", "Islamabad", "the lower house", "Superhero", "(Disney)", "Ezra Pound", "Venus", "Moonlight", "the Buk missile system", "Pyrrhus", "Dan Quayle", "hate", "a flop with chicks", "Men in Black", "the Medal of Honor", "Guitar Hero", "Sphinx", "Like a Rock", "Duke", "Cain", "The Goonies", "Preamble", "Eldridge Cleaver", "Paris", "Kim Possible", "the pernay region of Champagne", "Little Miss Muffet", "Shampoo", "gumbo", "a Bunsen burner", "Cain", "Jeff Gordon", "napkin", "sleeping with the Past", "Noah Schnapp", "Thebes", "insects", "New Zealand", "La Familia Michoacana", "Elliot Fletcher", "Nia Kay", "Australian Environment Minister Peter Garrett", "Robert Wagner", "Argentina", "Aung San Suu Kyi"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6260416666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3611", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-4274", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-14656", "mrqa_searchqa-validation-1646", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-4287", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-3058", "mrqa_searchqa-validation-3472", "mrqa_searchqa-validation-4301", "mrqa_searchqa-validation-8613", "mrqa_naturalquestions-validation-2023", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-1276", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1224"], "SR": 0.609375, "CSR": 0.5951704545454546, "EFR": 0.52, "Overall": 0.6361590909090908}, {"timecode": 44, "before_eval_results": {"predictions": ["31\u201324", "the G-20 spouses", "228", "Monday", "UCLA", "Russian residents and worldwide viewers, in English or in Russian, what they think about Russia's role in the international community.", "Diversity", "six", "Aravane Rezai", "Sunday", "a dad", "$150 billion over 10 years", "the Taliban", "hot and humid", "2.5 million", "Sky Club", "Ireland", "glass shards", "six", "Clifford Harris", "litter reduction and recycling", "Kim Il Sung died", "Marco Polo", "Friday", "a curmudgeonly senior citizen, Carl, tries to cope with the enthusiasm of Russell", "Wicked", "more than 200", "she always does when she comes in here", "summer", "Mumbai", "murdered four Lakewood, Washington, police officers Sunday", "air support", "Ken Choi", "sovereignty over them", "he fears a desperate country with a potential power vacuum that could lash out", "Jet Republic", "heavy brush", "North Korea", "Pakistan is in the midst of an intense military offensive against Taliban militants.", "Barack Obama", "Facebook and Google", "opium has accounted for more than half of Afghanistan's gross domestic product in 2007", "delays", "some dental work done", "a Golden Globe Awards", "to share personal information", "The Charlie Daniels Band", "on the family's blog", "Capitol Records", "former defense minister", "Southern California", "1526", "bloodstream or surrounding tissue", "Bryan Cranston", "Zeus", "Eva Marie", "Dublin", "Finland's northernmost province, Lapland", "Scrappy Moc", "Tool", "Kentucky", "Portland", "jihad", "Franklin"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5301081730769232}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.7999999999999999, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.07692307692307691, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3776", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2441", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-1250", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-4106", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-4181", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2172", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2551", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1278", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4885", "mrqa_triviaqa-validation-1396", "mrqa_triviaqa-validation-3245", "mrqa_hotpotqa-validation-3065", "mrqa_triviaqa-validation-7568"], "SR": 0.453125, "CSR": 0.5920138888888888, "EFR": 0.34285714285714286, "Overall": 0.6000992063492063}, {"timecode": 45, "before_eval_results": {"predictions": ["45\u00b0", "Scabby the Rat", "Henrik Stenson", "gun charges", "snow", "Peshawar police official said.", "forgery and flying without a valid license", "14 years", "independence from China", "the immorality of these deviant young men", "246", "kick out the invaders", "Yemen", "more than 30", "has become dependent on humanitarian aid", "Iran of trying to build nuclear bombs", "an \"unnamed international terror group\"", "The Rosie Show", "writing and starring in 'The Prisoner' about a former spy locked away in an isolated village who tries to escape each episode.", "Silicon Valley", "look-alike", "1950s", "Mandi Hamlin", "DBG", "the FBI", "the Movement for Democratic Change", "suspicious of the way their business books were being handled.", "\"the tech world is a meritocracy, and one becomes successful because he or she has a \"big brain.\"", "Iran's development of a nuclear weapon", "death", "$8.8 million", "Falklands", "money or other discreet aid", "comfort those in mourning", "2,700-acre", "always hot and humid and it rains almost every day of the year", "1969", "co-chair of the Genocide Prevention Task Force", "$1.45 billion", "the death of a pregnant soldier", "Garth Brooks", "32", "\"A good vegan cupcake has the power to transform everything for the better,\" Singer said.", "Aryan Airlines Flight 1625", "killing, which took place in the poor neighborhood of Rione Sanita, where Camorra, the name for organized crime in Naples", "Russia", "lightning strikes", "Chinese tourists", "that the alleged victim of the sexual assault admitted that the encounter that took place early Sunday morning wasensual", "release of the four men", "wings", "non-ferrous", "Selena Bieber", "The United States Secretary of State is the foreign minister of the United States and is the primary conductor of state - to - state diplomacy", "Liverpool", "Loch Lomond", "Mitford sisters", "anabolic\u2013androgenic steroids", "1858", "The Saturdays", "the high-powered job", "Chen Lu", "on the altar or near it", "Harriet Hayes"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6435216391398456}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1739130434782609, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.24999999999999997, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-1605", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-863", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-2729", "mrqa_newsqa-validation-3810", "mrqa_naturalquestions-validation-2740", "mrqa_naturalquestions-validation-6224", "mrqa_triviaqa-validation-4127", "mrqa_hotpotqa-validation-215", "mrqa_searchqa-validation-15782", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-14084", "mrqa_hotpotqa-validation-3030"], "SR": 0.5625, "CSR": 0.5913722826086957, "EFR": 0.35714285714285715, "Overall": 0.6028280279503105}, {"timecode": 46, "before_eval_results": {"predictions": ["821,784", "over charged", "Anne Frank", "the number of new cases is falling", "he eventually gave up 70 percent of his father-in-law's farm, which he then owned.", "The Stooges comedy farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \" follow the Sun,\"", "Zuma", "CBS, CNN, Fox and The Associated Press.", "64", "Samuel Herr, 26, and Juri Kibuishi, 23, of Irvine", "Saturday", "Scabby", "her niece, Amanda Knox's aunt", "federal officers' bodies", "Democratic", "five of us for the United States and two against us because they were stranded in Japan.", "33-year-old", "German authorities", "will \" apologize for his behavior\" Friday when he makes a statement at PGA headquarters in Ponte Vedra Beach, Florida, his agent said.", "nearly three weeks after the body of a pregnant soldier was found in a hotel near Fort Bragg.", "Yemen", "Dr. Conrad Murray", "2011", "backbreaking", "Rev. Alberto Cutie", "64", "Pakistan", "Burhanuddin Rabbani", "a pair of white boxer shorts", "airport security", "William Randolph Hearst", "Jacob Zuma", "climate change", "Brett Driverins", "his son, Isaac, and daughter, Rebecca", "10", "comfort those in mourning", "the rule of law in the United States of America.", "money goes directly to the inmates that designed certain pieces.", "Obama", "Wake Forest", "to reach car owners who haven't complied fully with recall", "Derek Mears", "6-2 6-1", "bipartisan", "Caster Semenya", "Pat Quinn", "Thaksin", "Gary Brooker", "Hurricane Gustav", "the Dalai Lama", "Toronto", "the final episode airing in the UK on 25 December 2015 on ITV", "2018", "The Hague", "Alexei Kosygin", "a car door", "sheepskin", "Guthred", "Over forty", "painter", "Amnesty International", "Easter", "calories"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5932422392762466}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.2666666666666667, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.11764705882352941, 1.0, 1.0, 0.32, 0.15384615384615383, 1.0, 0.0, 1.0, 0.125, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.18181818181818185, 0.4, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-3757", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-2806", "mrqa_newsqa-validation-1112", "mrqa_newsqa-validation-2257", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-478", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-2503", "mrqa_triviaqa-validation-117", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-8578"], "SR": 0.515625, "CSR": 0.5897606382978724, "EFR": 0.3225806451612903, "Overall": 0.5955932566918325}, {"timecode": 47, "before_eval_results": {"predictions": ["Hailee Steinfeld as Mattie Ross and Jeff Bridges", "the coffee shop Monk's", "Fertile Crescent ( Mesopotamia and Ancient Egypt )", "Greek \u1f61\u03c3\u03b1\u03bd\u03bd\u03ac, h\u014dsann\u00e1 ) is from Hebrew \u05d4\u05d5\u05e9\u05d9\u05e2 \u05e0\u05d0 h\u00f4\u0161\u00ee\u02bf\u00e2 - n\u0101", "electors", "interphase", "more than 600", "Exodus 20 : 7", "Robert Hooke", "regulatory site", "Howrah", "Taron Egerton", "Muhammad Yunus", "Joaquin Phoenix ( as Johnny Cash ), four songs by Reese Witherspoon ( as June Carter Cash )", "The program's philosophy is intended to help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship.", "Malloy as Pierre, Phillipa Soo as Anatole, Amber Gray as H\u00e9l\u00e8ne, Shaina Taub as Mary", "Tommy James and the Shondells", "1985 -- 1993", "1,228 km / h ( 763 mph)", "Daniel A. Dailey", "1835", "blue", "December 1941", "Germanic elements", "Brooks & Dunn", "The heaviest fully enclosed armoured fighting vehicle ever built. Five were ordered, but only two hulls and one turret were completed before the testing grounds were captured by the advancing Soviet forces.", "Biotic -- Biotic resources are obtained from the biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "Copernicus", "the AL East team", "eight", "patristic authors", "Richie Cunningham", "this mod enables access to the mini-game", "1985", "Nicole DuPort", "2001", "the United States economy first went into an economic recession.", "endocytosis", "May 3, 2005", "Dr. Hartwell Carver", "In 1927, the provisional Indian Olympic Committee formally became the Indian Olympic Association ( IOA )", "Weston - super-Mare", "60", "Jeff East", "California", "Stephen A. Douglas", "Stephen A. Davis", "drawn on the bank's own funds and signed by a cashier", "Cetshwayo", "Howard Caine", "Inti", "Some Like It Hot", "Pacific Ocean", "lacrimal fluid", "Dziga Vertov", "American", "Mikoyan design bureau", "a park spokesman", "Derek Mears", "Operation Pipeline Express.", "bees, honey, and the Black Madonna", "Sylvester Stallone", "Rights of Man", "Michael Partain"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6598112559417706}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [0.48484848484848486, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.07999999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.33333333333333337, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-170", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3459", "mrqa_naturalquestions-validation-167", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-5605", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6765", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-239", "mrqa_searchqa-validation-1177"], "SR": 0.609375, "CSR": 0.5901692708333333, "EFR": 0.24, "Overall": 0.5791588541666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Julia Ormond", "President pro tempore of the Senate", "drivers who meet more exclusive criteria", "1980s", "a popular and influential campaign song of the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "the eighth series of the UK version of The X Factor", "the U.S. Supreme Court's decision in Chisholm v. Georgia", "Buffalo Bill", "the United States", "W. Edwards Deming", "Omar Khayyam", "Gettysburg College", "Eva Green", "Superman", "Charles Path\u00e9", "Harold Godwinson", "before 1986, people often did not obtain a Social Security number until the age of about 14,", "Stephanie Judith Tanner", "six 50 minute ( one - hour with advertisements ) episodes", "Emma Watson and Dan Stevens", "1972", "her sister Ophelia and the feminine counterpart of Thing, Lady Fingers", "1979", "Godfrey", "three levels", "Orangeville, Ontario, Canada", "South Africa", "The player will encounter jungles, forts, ruins, and small villages", "Pyeongchang County, South Korea", "Paris", "31 March 2018", "since 3, 1, and 4 are the first three significant digits of \u03c0.", "( the Soviet Union and its satellite states ) and powers in the Western Bloc ( the United States, its NATO allies and others )", "relators", "Eddie Murphy", "in hospital", "Achal Kumar Jyoti", "1992", "2026", "Hirschman", "The American Revolution was a colonial revolt that took place between 1765 and 1783", "May 19, 2008", "UNESCO / ILO", "the Plaza Hotel", "Salman Khan", "Ernest Hemingway", "Homer Banks, Carl Hampton and Raymond Jackson", "13,000 astronomical units ( 0.21 ly)", "two years", "Antigonon leptopus", "the Great Crash", "four", "Stevie Wonder", "Donny Osmond and Hines Ward", "Detroit, Michigan", "\"houskov\u00e9 knedl\u00edky\"", "the controversial and explicit nature of many of their songs", "Troy and Tara", "the U.S.", "Peshawar", "honey Bear", "Voltaire", "\"Barbie Miss Astronaut,\"", "1986"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49497424861585615}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.20689655172413793, 0.0, 0.823529411764706, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2702702702702703, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-82", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-9487", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-8035", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-2425", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-8837", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-549", "mrqa_hotpotqa-validation-1769", "mrqa_hotpotqa-validation-1349", "mrqa_newsqa-validation-707", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-1603", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-5839", "mrqa_searchqa-validation-4335"], "SR": 0.40625, "CSR": 0.5864158163265306, "EFR": 0.2894736842105263, "Overall": 0.5883029001074114}, {"timecode": 49, "before_eval_results": {"predictions": ["In the final episode of the series", "Thomas Edison", "interstate communications by radio, television, wire, satellite, and cable", "Arousal regulation", "McKim Marriott", "one person", "Michael Jackson and Lionel Richie", "103", "December 25", "1966", "28 July 1914 to 11 November 1918", "Chinese New Year, also known as the Spring Festival in modern China, is an important Chinese festival celebrated at the turn of the traditional lunisolar Chinese calendar", "winter", "Barbara Harper", "Ed Sheeran", "an Ohio newspaper", "Rococo - era France", "Odoacer", "Revenge of the Wars ( 2005 )", "marks", "quantitative data or both", "15 August 1947", "Jason Marsden", "Kelli Goss", "Kenny Rogers", "more than 3,800 years", "2010", "Nicole Gale Anderson", "Tecala", "U.S. Supreme Court's decision in Chisholm v. Georgia", "Moctezuma II", "16 December 1689", "Ali Daei", "a U.S. federal statute intended to increase consistency in United States federal sentencing", "a loanword of the Visigothic word guma", "1967", "the naos", "an apprentice", "1959", "Roger Federer", "a certified question or proposition of law", "T.S. Eliot", "the macula", "William Shakespeare", "1960", "Lake Wales", "Austin, Texas", "most northerly of the five major circles of latitude", "instantaneous", "Bob Dylan", "Jay Gorney", "once every two weeks", "Alexandria", "Taiwan", "the United States Army", "Japan", "corn", "South Africa", "quality of teaching and learning in American schools", "six", "chili pepper", "Lizzie Borden", "Forgery", "Windsor Castle"], "metric_results": {"EM": 0.4375, "QA-F1": 0.546844607598284}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.22222222222222224, 0.0, 0.4, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.823529411764706, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.7272727272727273, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-3583", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6892", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-7232", "mrqa_naturalquestions-validation-6666", "mrqa_naturalquestions-validation-82", "mrqa_naturalquestions-validation-4747", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-4883", "mrqa_naturalquestions-validation-1949", "mrqa_triviaqa-validation-7373", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-5473", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-1993", "mrqa_searchqa-validation-13908"], "SR": 0.4375, "CSR": 0.5834375, "EFR": 0.3888888888888889, "Overall": 0.6075902777777777}, {"timecode": 50, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1480", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2858", "mrqa_hotpotqa-validation-2883", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4898", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-698", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-10630", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-167", "mrqa_naturalquestions-validation-170", "mrqa_naturalquestions-validation-1819", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6061", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-6544", "mrqa_naturalquestions-validation-6666", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7232", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-8837", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9578", "mrqa_newsqa-validation-1045", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1817", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1883", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2153", "mrqa_newsqa-validation-2172", "mrqa_newsqa-validation-2306", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2441", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3524", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4106", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-707", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-778", "mrqa_searchqa-validation-10025", "mrqa_searchqa-validation-10119", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10326", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10388", "mrqa_searchqa-validation-10477", "mrqa_searchqa-validation-10550", "mrqa_searchqa-validation-10631", "mrqa_searchqa-validation-10995", "mrqa_searchqa-validation-11161", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-11485", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-1153", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-11673", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-11727", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-11998", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12073", "mrqa_searchqa-validation-12076", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-12139", "mrqa_searchqa-validation-12235", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-12384", "mrqa_searchqa-validation-12511", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12634", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12793", "mrqa_searchqa-validation-12845", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-13020", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13159", "mrqa_searchqa-validation-132", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-13217", "mrqa_searchqa-validation-13250", "mrqa_searchqa-validation-13318", "mrqa_searchqa-validation-13386", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-13557", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-1367", "mrqa_searchqa-validation-13714", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13749", "mrqa_searchqa-validation-13754", "mrqa_searchqa-validation-13807", "mrqa_searchqa-validation-13911", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-14044", "mrqa_searchqa-validation-14055", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14356", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-14391", "mrqa_searchqa-validation-1440", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14698", "mrqa_searchqa-validation-14817", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-14982", "mrqa_searchqa-validation-1499", "mrqa_searchqa-validation-15015", "mrqa_searchqa-validation-15039", "mrqa_searchqa-validation-15060", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-15332", "mrqa_searchqa-validation-15441", "mrqa_searchqa-validation-15562", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-15782", "mrqa_searchqa-validation-15951", "mrqa_searchqa-validation-1598", "mrqa_searchqa-validation-15981", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16121", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16328", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16357", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-16597", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16688", "mrqa_searchqa-validation-16724", "mrqa_searchqa-validation-1675", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-16938", "mrqa_searchqa-validation-16943", "mrqa_searchqa-validation-1710", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-1847", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2363", "mrqa_searchqa-validation-2371", "mrqa_searchqa-validation-2483", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2597", "mrqa_searchqa-validation-2641", "mrqa_searchqa-validation-2746", "mrqa_searchqa-validation-2789", "mrqa_searchqa-validation-303", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-3536", "mrqa_searchqa-validation-3583", "mrqa_searchqa-validation-3643", "mrqa_searchqa-validation-3654", "mrqa_searchqa-validation-3661", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-381", "mrqa_searchqa-validation-3877", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-4069", "mrqa_searchqa-validation-4071", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-4149", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-4287", "mrqa_searchqa-validation-432", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-461", "mrqa_searchqa-validation-4648", "mrqa_searchqa-validation-4788", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4838", "mrqa_searchqa-validation-4880", "mrqa_searchqa-validation-4900", "mrqa_searchqa-validation-4923", "mrqa_searchqa-validation-4989", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5074", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5287", "mrqa_searchqa-validation-5417", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5548", "mrqa_searchqa-validation-5584", "mrqa_searchqa-validation-5603", "mrqa_searchqa-validation-5616", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5902", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6138", "mrqa_searchqa-validation-6315", "mrqa_searchqa-validation-6620", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6743", "mrqa_searchqa-validation-6773", "mrqa_searchqa-validation-6790", "mrqa_searchqa-validation-6791", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-6993", "mrqa_searchqa-validation-7133", "mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-7220", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7266", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-7388", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7723", "mrqa_searchqa-validation-7850", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7933", "mrqa_searchqa-validation-7962", "mrqa_searchqa-validation-8003", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-816", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8510", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8688", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-8725", "mrqa_searchqa-validation-8814", "mrqa_searchqa-validation-8824", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-9034", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9452", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9572", "mrqa_searchqa-validation-9620", "mrqa_searchqa-validation-9731", "mrqa_searchqa-validation-9808", "mrqa_searchqa-validation-9817", "mrqa_searchqa-validation-9905", "mrqa_squad-validation-10175", "mrqa_squad-validation-10448", "mrqa_squad-validation-10459", "mrqa_squad-validation-1046", "mrqa_squad-validation-1106", "mrqa_squad-validation-1605", "mrqa_squad-validation-1621", "mrqa_squad-validation-1764", "mrqa_squad-validation-2076", "mrqa_squad-validation-2107", "mrqa_squad-validation-2122", "mrqa_squad-validation-2332", "mrqa_squad-validation-2347", "mrqa_squad-validation-2450", "mrqa_squad-validation-2485", "mrqa_squad-validation-2546", "mrqa_squad-validation-2653", "mrqa_squad-validation-2667", "mrqa_squad-validation-2830", "mrqa_squad-validation-2991", "mrqa_squad-validation-3022", "mrqa_squad-validation-3187", "mrqa_squad-validation-3294", "mrqa_squad-validation-3335", "mrqa_squad-validation-3366", "mrqa_squad-validation-341", "mrqa_squad-validation-3524", "mrqa_squad-validation-3643", "mrqa_squad-validation-3701", "mrqa_squad-validation-3728", "mrqa_squad-validation-3800", "mrqa_squad-validation-3932", "mrqa_squad-validation-4016", "mrqa_squad-validation-4103", "mrqa_squad-validation-4256", "mrqa_squad-validation-4293", "mrqa_squad-validation-4462", "mrqa_squad-validation-4518", "mrqa_squad-validation-4731", "mrqa_squad-validation-4844", "mrqa_squad-validation-4994", "mrqa_squad-validation-5098", "mrqa_squad-validation-5136", "mrqa_squad-validation-5171", "mrqa_squad-validation-523", "mrqa_squad-validation-5404", "mrqa_squad-validation-5428", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5602", "mrqa_squad-validation-5680", "mrqa_squad-validation-5780", "mrqa_squad-validation-5890", "mrqa_squad-validation-5902", "mrqa_squad-validation-5992", "mrqa_squad-validation-6017", "mrqa_squad-validation-6063", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6360", "mrqa_squad-validation-6423", "mrqa_squad-validation-6505", "mrqa_squad-validation-6779", "mrqa_squad-validation-7107", "mrqa_squad-validation-7117", "mrqa_squad-validation-7141", "mrqa_squad-validation-7166", "mrqa_squad-validation-7193", "mrqa_squad-validation-7707", "mrqa_squad-validation-7725", "mrqa_squad-validation-7742", "mrqa_squad-validation-7914", "mrqa_squad-validation-7959", "mrqa_squad-validation-7976", "mrqa_squad-validation-8012", "mrqa_squad-validation-8129", "mrqa_squad-validation-8130", "mrqa_squad-validation-8151", "mrqa_squad-validation-8204", "mrqa_squad-validation-8250", "mrqa_squad-validation-8296", "mrqa_squad-validation-831", "mrqa_squad-validation-8636", "mrqa_squad-validation-8707", "mrqa_squad-validation-9074", "mrqa_squad-validation-9205", "mrqa_squad-validation-921", "mrqa_squad-validation-9306", "mrqa_squad-validation-9519", "mrqa_squad-validation-960", "mrqa_squad-validation-9614", "mrqa_squad-validation-9803", "mrqa_squad-validation-9970", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-1149", "mrqa_triviaqa-validation-1177", "mrqa_triviaqa-validation-119", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1333", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1902", "mrqa_triviaqa-validation-1905", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-403", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4064", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4368", "mrqa_triviaqa-validation-4592", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5062", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5114", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-5297", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-623", "mrqa_triviaqa-validation-624", "mrqa_triviaqa-validation-6281", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6417", "mrqa_triviaqa-validation-6713", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-7125", "mrqa_triviaqa-validation-7137", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7577", "mrqa_triviaqa-validation-7622", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7761", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-928"], "OKR": 0.705078125, "KG": 0.52421875, "before_eval_results": {"predictions": ["New England Patriots", "All These Things That I've Done ''", "Help! is the fifth studio album by English rock band the Beatles", "1985", "1992 to 2013", "acts as the bottom of the larynx", "Australia", "Hirschman", "southern New Mexico in the Southwestern United States", "dermis", "Majandra Delfino", "Christopher Jones", "honey, tree and vine fruits, flowers, berries, and most root vegetables", "1.1", "the President", "Meredith Brody ( Zoe McLellan ), a transfer from the NCIS Great Lakes field office, who has worked as a Special Agent Afloat and is keen to leave her past behind as she moves to New Orleans", "Michael Christopher McDowell", "a source of lumber and as recreational areas", "a children's song that is often sung in schools, at camps and at birthday parties", "Dorothy Gale", "1952", "John B. Watson", "1966", "Goneril", "an abbreviation used in the publications of the Myers -- Briggs Type Indicator ( MBTI ) to refer to one of sixteen personality types", "Norman Whitfield", "Donny Osmond", "Warren Hastings", "1901", "2018", "1926", "1770 and 1848", "the 2nd largest ethnic Maya group in Guatemala ( after the K'iche')", "writ of certiorari", "Marley & Me", "Odoacer", "Kate Walsh", "Kristy Swanson", "Isabella Palmieri", "Orographic lift", "Cam Clarke", "Bob Dylan", "Lake Buena Vista, Florida", "a state or other organizational body that controls the factors of production", "people of France to the people of the United States", "the Inuit", "Afonso IV", "1939 to 1945", "Thawne", "Amanda Fuller", "Graham McTavish", "Joshua Tree National Park", "aircraft carrier", "Brian Clough", "D\u00e2mbovi\u021ba River", "Winchester, Nevada", "Nicolas Vanier", "30,000", "Brazilian supreme court judge", "family", "\"Contour stripping\"", "Golden", "Julie Griffith", "Sweden"], "metric_results": {"EM": 0.5, "QA-F1": 0.597049317614575}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.7692307692307692, 0.18181818181818182, 1.0, 0.5, 0.12500000000000003, 1.0, 1.0, 0.09523809523809523, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.6, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9656", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-4801", "mrqa_naturalquestions-validation-4238", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4185", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-10201", "mrqa_naturalquestions-validation-1676", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-9011", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-3504", "mrqa_naturalquestions-validation-5522", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-10288", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-2996", "mrqa_hotpotqa-validation-2773", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-16656", "mrqa_searchqa-validation-3014"], "SR": 0.5, "CSR": 0.5818014705882353, "EFR": 0.34375, "Overall": 0.5891727941176471}, {"timecode": 51, "before_eval_results": {"predictions": ["Vicente Fox", "Blake Lively", "John Bull", "in late 1968", "the Central and South regions", "the Earth itself moves ( and also carries the continents with it ) as it expands from a central axis", "Burj Khalifa", "the chief priests", "Danny Veltri", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles ( 2010 -- 14 )", "sperm's midpiece ( at the base of the sperm head )", "Manhattan, the Bronx, Queens, Brooklyn, and Staten Island", "the British East India company to sell tea from China in American colonies without paying any taxes", "The hitchhiking scene with Elvis and Gary Lockwood was filmed near Camarillo, California", "Luke", "Garwin Sanford", "motions and resolutions such as a motion of no confidence, motion of adjournment, motion of censure and calling attention notice as per the rules", "Horace Lawson Hunley", "Beecher Prep, a mainstream private school", "The season was ordered in May 2017", "2002", "Pangaea", "east side of the Ligurian and Tyrrhenian Seas", "Domhnall Gleeson", "Baltimore", "Lulu", "north to the Southern Ocean ( or, depending on definition, to Antarctica ) in the south", "Hermann Ebbinghaus", "Bachendri Pal", "approximately 26,000 years", "J.P. Zenger High", "a bicameral body composed of a 31 - member Senate and a 150 - member House of Representatives", "Ramones", "Norman Pritchard", "Richard Burbage", "the type of bird as well as the diet of the bird", "Palm Sunday celebrations", "1975", "1978", "1876", "after their original live broadcasts end", "Kansas and Oklahoma", "the date of the widely publicized Scopes Trial in the United States", "catfish aquaculture farms produce the majority of farm - raised catfish consumed in the United States", "Ethel Robinson", "Baaghi", "over 3,800 years", "her sister Ophelia and the feminine counterpart of Thing, Lady Fingers", "Cornett family", "Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson", "December 24, 1836", "tennis", "Derek Batey", "Debbie Abrahams", "elderships", "June 26, 1970", "Donald Sutherland", "American", "six", "Defense lawyers trying to save their client from the death penalty argued Tuesday that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq", "A Prairie Home Companion", "Magellan", "John Paul Jones", "cancer"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6247630691941759}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.18181818181818182, 0.8, 0.6666666666666666, 0.1, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.4444444444444445, 0.3157894736842105, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.25, 0.10526315789473685, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.5, 0.7142857142857143, 0.375, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.25806451612903225, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8204", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-65", "mrqa_naturalquestions-validation-1046", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3072", "mrqa_naturalquestions-validation-5181", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-10344", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-2270", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-4042", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-9031", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-3429", "mrqa_triviaqa-validation-4987", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-2465", "mrqa_newsqa-validation-2157"], "SR": 0.484375, "CSR": 0.5799278846153846, "EFR": 0.24242424242424243, "Overall": 0.5685329254079254}, {"timecode": 52, "before_eval_results": {"predictions": ["Duck", "type I interferons are produced when the body recognizes a virus has invaded it", "1992", "1890s", "art", "Sarah Silverman", "AMX - 50", "radioisotope thermoelectric generator", "Gertrude Niesen", "Bruce Willis as John McClane", "Chicago", "the president", "Sir Ronald Ross", "Jackie Robinson", "Walter Brennan", "New Brunswick", "Gorakhpur Junction", "electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions", "1973", "remittance is a transfer of money by a foreign worker to an individual in his or her home country", "Sams brother, MaturMaker, are also looking to be drafted in the NBA, with Simons being a postgraduate andMaker's decision relating to him being held back a year.", "stability, security, and predictability of British law and government enabled Hong Kong to flourish as a centre for international trade", "an Aldabra giant tortoise", "Jerry Leiber and Mike Stoller", "the rise of literacy, technological advances in printing, and improved economics of distribution", "Bill Patriots", "Balaam is riding, which tries to avoid the angel", "Vaca", "above ground", "state ownership of the means of production", "November 3, 2007", "at least 28", "northern Louisiana, west to Colorado, and east to Massachusetts", "more than 160 high - rises", "47 cents", "Jenny Draper", "332", "James Rodr\u00edguez", "Madhya Pradesh", "The geopolitical divisions in Europe that created a concept of East and West originated in the Roman Empire", "Sanjana Ajay Pethewala", "The White House Executive chef", "1994 until the end of the series in 2010", "Neil Armstrong and pilot Buzz Aldrin", "slowing the vehicle", "1950s", "Paul the Apostle", "commissioned officers are given a one - time stipend when commissioned to purchase their required uniform items", "Louis Mountbatten", "Baltimore, Maryland", "the temporal lobes of the brain and the pituitary gland", "re-building", "speedway", "red", "1868", "October 21, 2016", "Sri Lanka Freedom Party", "650", "Alexandre Caizergues", "Robert Park", "Hobbylygirl15", "the Chautauqua movement", "London", "CNN"], "metric_results": {"EM": 0.34375, "QA-F1": 0.4690940604223852}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false], "QA-F1": [0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.11764705882352941, 0.0, 0.34782608695652173, 0.5, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.5, 1.0, 0.26666666666666666, 0.6923076923076924, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 0.25, 0.5, 1.0, 0.0, 0.0, 0.23529411764705882, 0.4444444444444445, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-5997", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-9697", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-8623", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-9295", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-4643", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-4500", "mrqa_naturalquestions-validation-4067", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2226", "mrqa_triviaqa-validation-653", "mrqa_hotpotqa-validation-5401", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-16271", "mrqa_newsqa-validation-2587"], "SR": 0.34375, "CSR": 0.5754716981132075, "EFR": 0.35714285714285715, "Overall": 0.5905854110512129}, {"timecode": 53, "before_eval_results": {"predictions": ["geodynamico", "\u201cMiss Potter\u2019s", "congregation", "The Simpsons", "Louis Le Vau", "Tesco", "holography", "Jordan", "Tacitus", "Pickwick", "Colombia", "Godfigu", "Sanskrit", "London Terminal", "Robinson", "Georgia", "your Excellency", "a muezzin", "Separate Tables", "Alan Sugar", "Wordsworth", "Avocados", "Benedict meaning", "Air", "the Witham, Welland, Nene and Great Ouse", "piano", "Guinea", "Melpomene", "the Kiel Canal", "Nairobi", "Treaty of Amiens", "Manchester City F.C.", "The Full Monty", "Diana", "Muriel Bing", "Damian Green", "Lord Chesterfield", "bad luck", "the Kit-Howr Club", "lungs", "Charlie Schwarzenegger", "Gibraltar", "Jeffrey Archer", "Angola", "UV", "McMurphy", "a dove", "Scooby-Doo", "Jackson Pollock", "North America", "orchid", "a market data and statistics portal, while combined private labels sold more, in 2014 Blue Bell was the best - selling ice cream brand in the United States", "amino acids glycine and arginine", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Hong Kong", "Manchester\u2013Boston Regional Airport", "Iynx", "New York", "Lillo Brancato Jr.", "304,000", "Man of Steel", "Taylor Swift", "Apple", "1-0"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5598462301587301}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.33333333333333337, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1940", "mrqa_triviaqa-validation-1974", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-4256", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5811", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-7092", "mrqa_triviaqa-validation-5150", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-5802", "mrqa_triviaqa-validation-3968", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-1192", "mrqa_triviaqa-validation-496", "mrqa_triviaqa-validation-1463", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-1864", "mrqa_newsqa-validation-2526", "mrqa_searchqa-validation-3006", "mrqa_searchqa-validation-11064", "mrqa_newsqa-validation-3130"], "SR": 0.515625, "CSR": 0.5743634259259259, "EFR": 0.0967741935483871, "Overall": 0.5382900238948626}, {"timecode": 54, "before_eval_results": {"predictions": ["Follicle-stimulating", "Jacob", "Klaus dolls", "Mungo Park", "poetry", "Blackburn", "Clijsters", "Myanmar", "Chemnitz", "bone fracture", "Bassoon", "New South Wales", "The Penguin", "Sandie Shaw", "Manchester", "Lowestoft", "the sinus node", "Jason Bourne", "Vickers-Armstrong's", "bismarck herring", "Cary Grant", "Andean", "silica sand, soda ash, dolomite and limestone", "Novak Djokovic", "Joan Rivers", "the Cholderton estate", "South Africa", "myxomatosis", "Menelaus", "Petra Kvitova", "the Arizona Diamondbacks", "lips", "Puck", "a rescue lifeboat", "Robert Maxwell", "Richie McCaw", "Alaska", "kabaddi", "nothing", "Tim Farron", "tarator", "Cunard Line", "whipped cream", "the Most Rev and Rt Hon George Carey", "Kiel Canal", "Loch Lomond", "stenographer", "York", "photography", "Michael runway.", "Prince William", "$100", "Christ", "Since 28 April 2001", "Nindal Ski Jump", "An All-Colored Vaudeville Show", "not interested in promoting a democratic system of government.", "Ku Klux Klan", "Donald Duck", "President Bush", "aircraft", "transcript", "Washington", "Battle of Prome"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5769965277777778}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.4444444444444445, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-168", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2146", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-6958", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-172", "mrqa_triviaqa-validation-5080", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-6831", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6135", "mrqa_triviaqa-validation-7518", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-5047", "mrqa_triviaqa-validation-6242", "mrqa_triviaqa-validation-1579", "mrqa_naturalquestions-validation-6521", "mrqa_naturalquestions-validation-6492", "mrqa_hotpotqa-validation-4243", "mrqa_hotpotqa-validation-3084", "mrqa_searchqa-validation-9224", "mrqa_searchqa-validation-15802"], "SR": 0.515625, "CSR": 0.5732954545454545, "EFR": 0.16129032258064516, "Overall": 0.55097965542522}, {"timecode": 55, "before_eval_results": {"predictions": ["Lorelei", "Taekwondo", "Rick Wakeman", "I-Spy", "Paul Keating", "the Surrealist movement", "\u00e1stron", "Gertrude", "daisy", "Conrad Murray", "Xenophon", "bccoli", "The Young Men's Christian Association", "cauliflower", "tune", "egg", "Wembley", "Martin Clunes", "kushtaka", "\"Wild Thing\"", "Drop the Dead Donkey", "Mariette", "marriage", "Flaw Girl", "&quot;poison&quot", "goad", "paddington bear", "Alison Krauss", "St Asaph", "John F. Kennedy", "Turkey", "the Earth", "the Grail", "Tim Farron", "a shot-loading barbell", "Hampton Court Palace", "Nile River", "the first bond", "Leo Tolstoy", "Numb3rs", "Manchester", "Agatha Christie", "Sousa", "latham Sholes", "Edmund Cartwright", "Thailand", "Prince Eddy", "Leeds", "Rouen", "Pompey", "Robert Kennedy", "January 2004", "Turing", "C\u00e9line Dion version of the hit Eric Carmen song", "Division of Fawkner", "nuclear weapons", "Muslim", "fled Zimbabwe and found his qualifications mean little as a refugee.", "Congress", "nine", "Baton Rouge", "Los Angeles", "Florida", "Robert Gates"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6453125}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3098", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-2646", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-141", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-2066", "mrqa_triviaqa-validation-1897", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3171", "mrqa_triviaqa-validation-607", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-3527", "mrqa_triviaqa-validation-2559", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-4382", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-5469", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-2653", "mrqa_searchqa-validation-612", "mrqa_newsqa-validation-3186"], "SR": 0.59375, "CSR": 0.5736607142857143, "EFR": 0.4230769230769231, "Overall": 0.6034100274725274}, {"timecode": 56, "before_eval_results": {"predictions": ["Mercia", "violin", "metatarsal guards", "hyenas", "transsexual", "eremony", "Possumhaw Viburnum", "Mary Stuart, Lord Darnley", "Chelsea", "Confucius", "AOL", "pascal", "The Nightcomers", "the moon", "finger-crossing", "Muriel Spark", "Neil Dudgeon", "the Model T", "hydrogen and helium", "fisherman", "Finland", "\"Born To Be Wild\"", "Telstar", "Periodic Table", "phylloquinone", "Video", "scapulae", "James A. Garfield", "Pentecost", "her mother, the Princess Royal", "fox", "Pink Floyd\u2019s Animals", "music (to be performed) in a fiery manner", "Brighton", "Verona", "Grimsby", "Mozart", "Peter Stuyvesant", "Dangerous Minds", "Menorca", "Italy", "Tony Blackburn", "Peru", "Stevie Wonder", "Dandy", "Triumph and Disaster", "kurkum", "Switzerland", "turkeys", "The Last King of Scotland", "britishtvsitcoms", "plate tectonics", "1966", "The Vampire Diaries", "October 13, 1980", "9", "phyllocnistis liquidambarisella", "jobs", "will be available under the inverted glass pyramid of the Louvre.", "Anthony Chambers", "polyglot", "Pilgrims", "3800 - 4500 Angstrom", "Heisman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.594937193627451}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.125, 0.11764705882352941, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-2220", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-1935", "mrqa_triviaqa-validation-1163", "mrqa_triviaqa-validation-5404", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-1139", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-1694", "mrqa_triviaqa-validation-2670", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-1564", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-2561", "mrqa_triviaqa-validation-1565", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-7527", "mrqa_hotpotqa-validation-450", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-1200", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-3742"], "SR": 0.546875, "CSR": 0.5731907894736843, "EFR": 0.41379310344827586, "Overall": 0.601459278584392}, {"timecode": 57, "before_eval_results": {"predictions": ["Copenhagen", "Damascus", "the beach", "Robben Island", "Uranus", "Apollo", "Amalthea", "Jeffrey Archer", "pommel horse", "South Africa", "Tanzania", "Rowan Atkinson", "yule goat", "John Douglas", "Venice", "ViennaVienna", "Mental Floss", "the Tamagotchi", "criseyde", "an analgesic is something designed to relieve pain, usually in some drug used to combat swelling or aches", "George IV", "April", "lala", "a pen", "Chiropractic", "Croatia", "a circle", "yellow", "jellyfish", "fish", "Sahara desert", "Jack Johnson", "Whisky Galore", "Belgium", "New Year\u2019s Eve", "The Hustle", "Indonesia", "red Admiral", "shorthand", "the Daily Mirror", "horse", "Adrian Edmondson", "the Indianapolis 500", "a burthen of about 60 tons", "chicken kiev and scotch eggs", "ry", "Leo zodiac", "Sergeant-Major Bullimore", "Argentina", "Pamplona", "Prince Edward Island", "San Francisco, California", "Jim Justice", "California", "micronutrient-rich", "Jewel", "Macomb County", "Abu Sayyaf", "then- Sen. Obama", "glamour and hedonism", "Claudius", "Crash", "San Simeon", "Only the 2002 FIFA World Cup had more than one host, being split between Japan and South Korea"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5372126032282283}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.375, 1.0, 1.0, 0.0, 0.16216216216216214]}}, "before_error_ids": ["mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-2640", "mrqa_triviaqa-validation-7535", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-2292", "mrqa_triviaqa-validation-3344", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-2259", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-3063", "mrqa_triviaqa-validation-7215", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-4207", "mrqa_naturalquestions-validation-3837", "mrqa_hotpotqa-validation-2155", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-1685", "mrqa_searchqa-validation-11097", "mrqa_naturalquestions-validation-2775"], "SR": 0.484375, "CSR": 0.5716594827586207, "EFR": 0.42424242424242425, "Overall": 0.603242881400209}, {"timecode": 58, "before_eval_results": {"predictions": ["Ural Mountain", "Jinnah International Airport", "tarn", "Althorp", "Isaac Newton", "Dalai Lama", "Nick Berry", "five", "fish", "copper", "Neighbours", "La Toya", "the northern prawn", "Petra Kvitova", "lecroix", "Red", "Eurythmics", "Microsoft", "Twiggy", "four", "sport specific fit", "Kevin Spacey", "South Africa", "Bologna", "Peter Paul Rubens", "Golf", "eels", "Massachusetts", "eye", "Venus", "The Duchess", "$100", "Jimmy Carter", "the Boston Pops Orchestra", "the U.S. Marshals", "gods are one-eyed", "Brussels", "Strangeways", "meninges", "Nevada", "(Judi Dench)", "iron", "Cartoons", "'Erroneous' Number One", "silks", "Siddhartha Gautama", "one quarter", "plimsoll", "seattlepi.com", "the Big Bopper", "England", "private individuals or their employers", "Gaget, Gauthier & Co. workshop", "the piloerection", "Australian", "2002", "New Zealand national team", "Matthew Fisher", "its intention to set up headquarters in Dublin", "Republicans", "Gandhi", "Alabama", "the Republic of the Marshall Islands", "Ravi River"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6052905701754385}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true], "QA-F1": [0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4106", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-3060", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-1241", "mrqa_triviaqa-validation-2445", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-742", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-6453", "mrqa_triviaqa-validation-6626", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-3915", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-2549", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1872", "mrqa_naturalquestions-validation-8609", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3080", "mrqa_newsqa-validation-2154", "mrqa_searchqa-validation-15463"], "SR": 0.546875, "CSR": 0.571239406779661, "EFR": 0.13793103448275862, "Overall": 0.545896588252484}, {"timecode": 59, "before_eval_results": {"predictions": ["the Mongol Empire", "Love Is All Around", "Isle of Anglesey", "willow", "Staffordshire", "marbles", "Zager & Evans", "Andes Mountains of Chile and Argentina", "Julie Andrews Edwards", "jackson", "Malm\u00f6", "Coral Sea", "The Wrestling Classic", "Christmas", "the chapel", "abdomen", "leprosy", "the United States", "John Maynard Keynes", "Salix", "Wisconsin", "the Mediterranean Sea", "Orwell", "The Pirates of Penzance", "Khartoum", "cyclopes", "Amnesty International", "east", "Caernarfon", "Persian landing", "Uganda", "e. Nesbit", "Taiwan", "anesthetic agents", "Cork", "dye", "Lexus CT 200h", "Pakistan", "Jules Verne", "Jack Johnson", "mackey Mouse", "prince Igor", "the Velvet Underground", "shark", "Valentine Dyall", "a burthen of about 60 tons", "China", "Donald Trump", "chromium", "Venezuela", "Victor Hugo", "lula ( Lesley Ann Warren )", "Senate Majority and Minority Leaders are two United States Senators and members of the party leadership of the United States Senate", "Pittsburgh", "supply chain management", "travel diary", "Antrim", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "up three of the last four months", "governor. Arnold Schwarzenegger", "New Zealand", "bird-of-paradise", "Margaret Mead", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5667836363148863}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.21621621621621623, 0.25, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.22222222222222218]}}, "before_error_ids": ["mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-7208", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-1148", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-2672", "mrqa_triviaqa-validation-2576", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-2067", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-2237", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-4541", "mrqa_triviaqa-validation-2111", "mrqa_triviaqa-validation-2270", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-4696", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-3373", "mrqa_triviaqa-validation-1232", "mrqa_triviaqa-validation-2259", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-3170", "mrqa_hotpotqa-validation-1001", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-1433", "mrqa_searchqa-validation-5663", "mrqa_naturalquestions-validation-3342"], "SR": 0.484375, "CSR": 0.5697916666666667, "EFR": 0.36363636363636365, "Overall": 0.5907481060606061}, {"timecode": 60, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1480", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-274", "mrqa_hotpotqa-validation-2858", "mrqa_hotpotqa-validation-2883", "mrqa_hotpotqa-validation-2947", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4898", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-5886", "mrqa_hotpotqa-validation-698", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-10288", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10630", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1500", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1819", "mrqa_naturalquestions-validation-1845", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2270", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2623", "mrqa_naturalquestions-validation-2800", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-3072", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4117", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-442", "mrqa_naturalquestions-validation-4437", "mrqa_naturalquestions-validation-4488", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-6544", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-6824", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7232", "mrqa_naturalquestions-validation-730", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-8204", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9175", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-974", "mrqa_newsqa-validation-1045", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1817", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1883", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2172", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2441", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-2551", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-326", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3303", "mrqa_newsqa-validation-3363", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3622", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3708", "mrqa_newsqa-validation-3743", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-709", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-940", "mrqa_searchqa-validation-10025", "mrqa_searchqa-validation-10326", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10388", "mrqa_searchqa-validation-10477", "mrqa_searchqa-validation-10631", "mrqa_searchqa-validation-10995", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-11485", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-1153", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-11673", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-11727", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-11998", "mrqa_searchqa-validation-12002", "mrqa_searchqa-validation-12073", "mrqa_searchqa-validation-12076", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-12139", "mrqa_searchqa-validation-12259", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-12384", "mrqa_searchqa-validation-12511", "mrqa_searchqa-validation-12513", "mrqa_searchqa-validation-12634", "mrqa_searchqa-validation-12793", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-13020", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13159", "mrqa_searchqa-validation-132", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-13217", "mrqa_searchqa-validation-13250", "mrqa_searchqa-validation-13318", "mrqa_searchqa-validation-13386", "mrqa_searchqa-validation-13474", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-1367", "mrqa_searchqa-validation-13714", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13749", "mrqa_searchqa-validation-13754", "mrqa_searchqa-validation-13807", "mrqa_searchqa-validation-13911", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-1397", "mrqa_searchqa-validation-14055", "mrqa_searchqa-validation-14356", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-14391", "mrqa_searchqa-validation-1452", "mrqa_searchqa-validation-14660", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-14982", "mrqa_searchqa-validation-1499", "mrqa_searchqa-validation-15015", "mrqa_searchqa-validation-15039", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-15332", "mrqa_searchqa-validation-15441", "mrqa_searchqa-validation-15562", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-15782", "mrqa_searchqa-validation-15951", "mrqa_searchqa-validation-1598", "mrqa_searchqa-validation-15981", "mrqa_searchqa-validation-16121", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16328", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16357", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-16597", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16688", "mrqa_searchqa-validation-16724", "mrqa_searchqa-validation-1675", "mrqa_searchqa-validation-16938", "mrqa_searchqa-validation-16943", "mrqa_searchqa-validation-1710", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-1847", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2363", "mrqa_searchqa-validation-2371", "mrqa_searchqa-validation-2483", "mrqa_searchqa-validation-2506", "mrqa_searchqa-validation-2597", "mrqa_searchqa-validation-2641", "mrqa_searchqa-validation-2746", "mrqa_searchqa-validation-2789", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3125", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-3354", "mrqa_searchqa-validation-3536", "mrqa_searchqa-validation-3583", "mrqa_searchqa-validation-3643", "mrqa_searchqa-validation-3654", "mrqa_searchqa-validation-3661", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-381", "mrqa_searchqa-validation-3877", "mrqa_searchqa-validation-390", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-4069", "mrqa_searchqa-validation-4071", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-4149", "mrqa_searchqa-validation-4287", "mrqa_searchqa-validation-432", "mrqa_searchqa-validation-4443", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-459", "mrqa_searchqa-validation-461", "mrqa_searchqa-validation-4788", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4838", "mrqa_searchqa-validation-4880", "mrqa_searchqa-validation-4900", "mrqa_searchqa-validation-4989", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5074", "mrqa_searchqa-validation-5090", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-5287", "mrqa_searchqa-validation-5417", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5548", "mrqa_searchqa-validation-5584", "mrqa_searchqa-validation-5603", "mrqa_searchqa-validation-5616", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5902", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6315", "mrqa_searchqa-validation-6620", "mrqa_searchqa-validation-6663", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6743", "mrqa_searchqa-validation-6773", "mrqa_searchqa-validation-6791", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-6993", "mrqa_searchqa-validation-7133", "mrqa_searchqa-validation-7220", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7266", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-7388", "mrqa_searchqa-validation-7485", "mrqa_searchqa-validation-7723", "mrqa_searchqa-validation-7850", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7933", "mrqa_searchqa-validation-7962", "mrqa_searchqa-validation-8003", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-816", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8510", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8688", "mrqa_searchqa-validation-8695", "mrqa_searchqa-validation-8814", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-9034", "mrqa_searchqa-validation-9080", "mrqa_searchqa-validation-9262", "mrqa_searchqa-validation-9334", "mrqa_searchqa-validation-9452", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9620", "mrqa_searchqa-validation-9731", "mrqa_searchqa-validation-9808", "mrqa_searchqa-validation-9817", "mrqa_searchqa-validation-9905", "mrqa_squad-validation-10175", "mrqa_squad-validation-10448", "mrqa_squad-validation-10459", "mrqa_squad-validation-1046", "mrqa_squad-validation-1106", "mrqa_squad-validation-1605", "mrqa_squad-validation-1764", "mrqa_squad-validation-2076", "mrqa_squad-validation-2107", "mrqa_squad-validation-2332", "mrqa_squad-validation-2450", "mrqa_squad-validation-2485", "mrqa_squad-validation-2546", "mrqa_squad-validation-2653", "mrqa_squad-validation-2667", "mrqa_squad-validation-2991", "mrqa_squad-validation-3022", "mrqa_squad-validation-3187", "mrqa_squad-validation-3294", "mrqa_squad-validation-3335", "mrqa_squad-validation-3366", "mrqa_squad-validation-341", "mrqa_squad-validation-3524", "mrqa_squad-validation-3643", "mrqa_squad-validation-3701", "mrqa_squad-validation-3800", "mrqa_squad-validation-3932", "mrqa_squad-validation-4016", "mrqa_squad-validation-4103", "mrqa_squad-validation-4256", "mrqa_squad-validation-4462", "mrqa_squad-validation-4518", "mrqa_squad-validation-4731", "mrqa_squad-validation-4844", "mrqa_squad-validation-5098", "mrqa_squad-validation-5136", "mrqa_squad-validation-5171", "mrqa_squad-validation-5404", "mrqa_squad-validation-5428", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5602", "mrqa_squad-validation-5680", "mrqa_squad-validation-5780", "mrqa_squad-validation-5890", "mrqa_squad-validation-5902", "mrqa_squad-validation-5992", "mrqa_squad-validation-6017", "mrqa_squad-validation-6063", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6360", "mrqa_squad-validation-6423", "mrqa_squad-validation-6505", "mrqa_squad-validation-6779", "mrqa_squad-validation-7107", "mrqa_squad-validation-7117", "mrqa_squad-validation-7193", "mrqa_squad-validation-7707", "mrqa_squad-validation-7725", "mrqa_squad-validation-7742", "mrqa_squad-validation-7914", "mrqa_squad-validation-7976", "mrqa_squad-validation-8129", "mrqa_squad-validation-8130", "mrqa_squad-validation-8151", "mrqa_squad-validation-8204", "mrqa_squad-validation-8250", "mrqa_squad-validation-8296", "mrqa_squad-validation-8636", "mrqa_squad-validation-8707", "mrqa_squad-validation-9074", "mrqa_squad-validation-9205", "mrqa_squad-validation-921", "mrqa_squad-validation-9306", "mrqa_squad-validation-9519", "mrqa_squad-validation-960", "mrqa_squad-validation-9614", "mrqa_squad-validation-9803", "mrqa_squad-validation-9970", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-1149", "mrqa_triviaqa-validation-1177", "mrqa_triviaqa-validation-1195", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1254", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1333", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1478", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1579", "mrqa_triviaqa-validation-1674", "mrqa_triviaqa-validation-1784", "mrqa_triviaqa-validation-1902", "mrqa_triviaqa-validation-1905", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-2172", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2498", "mrqa_triviaqa-validation-2597", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2701", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3063", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3229", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-374", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-3968", "mrqa_triviaqa-validation-403", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4064", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4114", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4592", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-5062", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-5297", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5767", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-623", "mrqa_triviaqa-validation-6281", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6713", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6958", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-7137", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-7267", "mrqa_triviaqa-validation-7322", "mrqa_triviaqa-validation-7348", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7396", "mrqa_triviaqa-validation-7577", "mrqa_triviaqa-validation-7622", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-7761", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-928"], "OKR": 0.6640625, "KG": 0.5125, "before_eval_results": {"predictions": ["Taliban captive", "a North Korean Foreign Ministry spokesman described U.S. Vice President Taro Aso as a \"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "This Is It", "free fixes for the consumer", "Roman Catholic", "the Taliban and their al Qaeda associates", "\"Empire of the Sun,\"", "Aung San Suu Kyi", "that he is protecting him and is with him", "nearly $2 billion", "to regain the trust of those customers who are driving our vehicles", "338", "Latvia", "a one-shot victory in the Bob Hope Classic on the final hole", "14", "along the equator between South America and Africa", "Mark Fields", "Los Angeles", "Basilan", "2009", "bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "dismissed all charges Wednesday night and ordered the release of the four men", "state senators", "Ken Choi", "40", "\"We exercise all around the globe and have joint exercises with countries all over the world. So do many other nations.\"", "16", "walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening", "school", "she always does when she comes in here", "Obama", "Wednesday", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation", "Turkey", "Sen. Barack Obama", "30-minute", "Dennis Davern", "for a project which does nothing more than perpetuate misconceptions about the state and its citizens.", "democracy", "the U.S. Holocaust Memorial Museum", "three", "Sunday", "Newcastle", "Port-au- Prince, Haiti", "11th year in a row", "birdie four at the last hole", "in a library bookcase", "a female soldier", "Doral", "Doral", "burhanuddin Rabbani", "Joe Walsh", "Cuernavaca, Durango, and Tepoztl\u00e1n and at the Churubusco Studios", "1939", "evolution", "France", "Sandi Tok reptig", "Dublin Institute of Technology", "Schutzstaffel", "Iowa State", "jedoublen", "Staten Island man wheels", "tom Sawyer", "Afghanistan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6175852972133543}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 0.23076923076923078, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.5333333333333333, 1.0, 0.8, 0.5, 0.8125000000000001, 1.0, 0.7692307692307693, 1.0, 0.888888888888889, 0.0, 1.0, 0.5217391304347826, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3188", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3656", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3030", "mrqa_newsqa-validation-3804", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3481", "mrqa_newsqa-validation-4181", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-399", "mrqa_newsqa-validation-3902", "mrqa_newsqa-validation-3897", "mrqa_naturalquestions-validation-5976", "mrqa_triviaqa-validation-5809", "mrqa_hotpotqa-validation-4568", "mrqa_searchqa-validation-8137", "mrqa_searchqa-validation-14733", "mrqa_searchqa-validation-16587"], "SR": 0.5, "CSR": 0.5686475409836065, "EFR": 0.21875, "Overall": 0.5494326331967214}, {"timecode": 61, "before_eval_results": {"predictions": ["60 euros", "Eleven", "bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "that they don't feel", "people switched from the very bad category to the pretty bad category", "murder", "school", "he won two Emmys for work on the 'Columbo' series starring Peter Falk.", "people around the world commented, pondered, and paid tribute", "Nigeria, Africa's largest producer.", "Kurt Cobain", "a dad.", "Prime Minister Nouri al-Maliki", "1983", "robo-female voice to reply: \"It sure looks like rain today,\"", "Jiverly Wong, who is believed to be in his early 40s.", "Tuesday", "Revolutionary Armed Forces of Colombia", "ICE chief Julie Myers", "Arizona", "$24.1 million", "executive director of the Americas Division of Human Rights Watch", "Iggy Pop", "over charged", "outside the courthouse", "CNN", "The Valley swim Club", "al Qaeda", "Ronald Reagan UCLA Medical Center", "blood", "Ventures", "prison inmates", "Manmohan Singh's Congress party", "shuttled between Baghdad and Damascus, carrying messages between the two capitals in an effort to defuse tensions.", "children ages 3 to 17", "individual pieces", "paintings, the 1962 \"Tete de Cheval\" (\"Horse's Head\") and the 1944 \"Verre et Pichet\" (\"Glass and Pitcher\") by Picasso", "ice", "\"new chapter\" of improved governance in Afghanistan", "It has been accepted, and as I depart, I want to offer once again my sincere apologies to any person who has been abused by any priest", "Port-au-Prince, Haiti", "Joan Rivers", "militant group", "three", "the outlet mall", "two-state solution", "iPhone 4S news", "Idriss Deby", "Egypt", "Hyundai", "Afghanistan", "Samara Cook", "December 19, 2016", "between the Eastern Ghats and the Bay of Bengal", "Wyatt Earp", "Arizona Diamondbacks", "Hibernian", "Dulwich", "seventh generation", "North Greenwich Arena", "seal seal", "Van Halen", "Nine to Five", "zebra"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5224749018607784}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.18181818181818182, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 0.5, 0.08, 0.7499999999999999, 0.0, 0.10526315789473684, 0.4, 1.0, 0.25806451612903225, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3030", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-3669", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-1540", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-3462", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-1207", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-918", "mrqa_newsqa-validation-1216", "mrqa_naturalquestions-validation-8493", "mrqa_naturalquestions-validation-3795", "mrqa_searchqa-validation-11266", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-11282"], "SR": 0.40625, "CSR": 0.5660282258064516, "EFR": 0.34210526315789475, "Overall": 0.5735798227928693}, {"timecode": 62, "before_eval_results": {"predictions": ["Aravane Rezai", "dead man's naked body", "Steve Jobs", "she was overwhelmed and amazed by the beauty of Prague and every time I arrive in Prague, even now, I'm still amazed.", "through the weekend", "three", "more than 30", "wacko", "cancer", "Russia", "Asashoryu", "between June 20 and July 20", "American Civil Liberties Union", "suspended a student who admitted to hanging a noose in a campus library,", "\" Maria Maria\"", "Russia", "Silvan Shalom", "Michael Arrington", "German Chancellor", "cancer awareness", "two", "Mark Fields", "complicated and deeply flawed", "15", "those traveling near the Somali coast", "crawling into the rubble", "84-year", "TSA", "\"underwear bomber\" Umar Farouk AbdulMutallab", "$250,000", "the producer's", "Las Vegas", "North Korea", "apartment building in Cologne, Germany", "the moon's surface", "three", "Sabina Guzzanti", "the foyer of the BBC building in Glasgow, Scotland", "Fort Hood, Texas", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "one", "Columbia, Illinois", "\"A Lion Among Men,\"", "Roman Catholic", "his campaign message of change to U.S. dealings with Iran.", "Serie A", "Inmates design and produce the Haeftling range.", "Oxbow", "$55.7 million", "Venus Williams", "Saturday", "real - time chat", "Crepuscular animals", "the Germanic elements", "brambles", "the U.S. Marshals", "fedora", "Doniphan County", "Katarina Witt", "Sir George Cayley, 6th Baronet", "mincemeat", "large intestine", "Dred Scott", "Columbia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5982092126623377}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.75, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.12500000000000003, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-842", "mrqa_newsqa-validation-2769", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2136", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-42", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-3288", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-5041", "mrqa_triviaqa-validation-7487", "mrqa_hotpotqa-validation-740", "mrqa_hotpotqa-validation-256", "mrqa_searchqa-validation-13182"], "SR": 0.515625, "CSR": 0.5652281746031746, "EFR": 0.2903225806451613, "Overall": 0.5630632760496672}, {"timecode": 63, "before_eval_results": {"predictions": ["social reincorporation", "1959", "1960", "12 hours", "a new queen in town.", "a Royal Air Force helicopter", "Phillip A. Myers", "reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees.", "forgery and flying without a valid license", "cross-country skiers", "Jacob", "\"It has never been the policy of this president or this administration to torture.\"", "Everglades", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Piers Morgan Tonight", "Ghana", "President-elect Barack Obama", "troops always have the right to defend themselves and are given leeway to use their best judgment on the battlefield.", "the Taliban and their al Qaeda associates", "abuse", "deportation", "Austin Wuennenberg", "October 19", "the death of a pregnant soldier", "5,600", "1,500 Marines", "free services", "a number of leaders I've talked with over the last several days.", "his father", "Arnold Drummond", "Mokotedi Mpshe", "27", "we seek a new way forward, based on mutual interest and mutual respect.", "Michael Schumacher", "1994", "jailed", "Manmohan Singh", "it said no one got on or off the aircraft, which spent 20 seconds on the ground.", "named his company Polo", "American Civil Liberties Union", "sixteen", "reverse the Taliban's momentum and stabilize the country's government.", "terrorism", "The patient", "criminals who had fired on an army patrol shot and killed the students", "people expressed jubilation at their chance to vote, which will be tasked with drafting a new constitution after three decades of Mubarak's rule.", "Salt Lake City, Utah", "five", "outlaws", "a real estate holding", "northern Australia", "Michael Schumacher", "Coriolis force", "Extroverted Thinking ( Te )", "West Virginia", "\"IRL\"", "the Grail", "Attack the Block", "the Nazi Party", "Frank Ocean", "Pushing Daisies", "Halo 3", "delectation", "Daltoni"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6528382423371648}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.7777777777777778, 1.0, 1.0, 0.7777777777777778, 0.4827586206896552, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-1852", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-2864", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-721", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-20", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2410", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2069", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-7242", "mrqa_triviaqa-validation-164", "mrqa_hotpotqa-validation-1706", "mrqa_searchqa-validation-7179", "mrqa_hotpotqa-validation-2684"], "SR": 0.59375, "CSR": 0.565673828125, "EFR": 0.4230769230769231, "Overall": 0.5897032752403847}, {"timecode": 64, "before_eval_results": {"predictions": ["Saturday", "Garth Brooks", "Caylee Anthony", "glass shards", "a long-range missile on its launch pad", "the execution.", "Keating Holland", "up to $50,000", "$52.4 million", "United", "U.S.", "Ike", "June 6, 1944", "behavioral health-care", "tie salesman", "64", "Johannesburg", "WGC-CA Championship", "clashes broke out Tuesday after Copts took to the streets to protest last week's burning of a church.", "Victoria Harbor", "Saturday", "will give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.", "August 4, 2000", "1913", "when times get tough,", "visitors used to spend only an hour or so at his house, then leave still thinking of George Washington as that grim, old man on the dollar bill.", "Blagojevich", "more than 170", "J. Crew", "105-year", "North Korea's reclusive leader Kim Jong- Il", "Kim Clijsters", "Phillip A. Myers", "CEO of an engineering and construction company with a vast personal fortune.", "alert patients of possible tendon ruptures and tendonitis.", "Christopher Savoie", "Pew Research Center", "Russian air force", "Graeme Smith", "Tim Clark, Matt Kuchar and Bubba Watson", "an explosion, reported about 11:30 a.m. Tuesday,", "Republicans", "Nazi Germany", "Seoul", "tuatara", "November 1", "Sabina Guzzanti", "Linda", "its intention to set up headquarters in Dublin.", "\"G gossip Girl\"", "one representative", "1966", "2018", "London", "Sigmund Freud", "Kazakhstan", "the Walther P.38 pistol", "Elijah Wood", "1994", "Charles", "The house next door", "Alexandria", "bridge"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5562280739590522}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5925925925925926, 0.08695652173913043, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1009", "mrqa_newsqa-validation-3902", "mrqa_newsqa-validation-2430", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-2764", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-3098", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-7203", "mrqa_hotpotqa-validation-4576", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-7240"], "SR": 0.46875, "CSR": 0.5641826923076922, "EFR": 0.2647058823529412, "Overall": 0.5577308399321266}, {"timecode": 65, "before_eval_results": {"predictions": ["a deceased organ donor", "\"momentous discovery\"", "Kenner, Louisiana", "a share in the royalties", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband", "Latvia", "capital murder and three counts of attempted murder", "hackers claiming to be hackers have distorted this truth in order to further their hidden agenda, and some Anons have taken the bait.", "Michelle Rounds", "Kurdish militant group", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list, had been released.", "Desmond Tutu", "grocery store", "anyone who will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "that NATO fighters followed the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.", "London's 20,000-capacity O2 Arena.", "Sharon Bialek", "six", "Millvina Dean, is auctioning off her remaining mementos of the doomed ship to pay nursing home bills.", "Abbey Road", "an empty water bottle down the touchline following a disallowed goal for Arsenal.", "space shuttle Discovery", "A passenger plane", "former detainees of Immigration and Customs Enforcement", "Anil Kapoor.", "Muslim", "Cash for Clunkers", "off the north coast of Puerto Rico.", "380,000", "genocide, crimes against humanity, and war crimes.", "crawling into the rubble", "Hakeemullah Mehsud", "Chesley \"Sully\" Sullenberger", "rabbit hole", "scored a hat-trick", "Transport Workers Union leaders", "Molotov cocktails, rocks and glass.", "Red Lines", "U.S. Vice President Dick Cheney", "Da Vinci Code", "the results by a chaplain about 1:45 p.m., per jail policy.", "tusks", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "Hurricane Gustav", "Dubai", "the moon's surface", "nine", "summer", "Benazir Bhutto", "positive signal", "use of torture and indefinite detention", "Southport, North Carolina", "stock market crash of October 29, 1929", "emperors", "Nevada", "Ireland", "George Bernard Shaw", "Denmark", "26 December 1982", "ARY Films", "honey", "mail", "Amadeus", "squash"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6067663582380152}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5454545454545455, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1395348837209302, 0.0, 1.0, 1.0, 0.07692307692307693, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-2769", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-2303", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-850", "mrqa_naturalquestions-validation-1799", "mrqa_triviaqa-validation-4463", "mrqa_triviaqa-validation-1328", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-5359", "mrqa_searchqa-validation-5042"], "SR": 0.546875, "CSR": 0.5639204545454546, "EFR": 0.20689655172413793, "Overall": 0.5461165262539185}, {"timecode": 66, "before_eval_results": {"predictions": ["e", "Deimos", "(2008-2011)", "Australian", "Clothes", "proportion", "\"The Outsiders\", then 2 years later, \"Wall Street,\" and \"Major League.\"", "Pierre-Auguste Renoir", "Taipei", "Jamaica", "Fame", "rebels", "George Clooney Jr.", "Hee Haw", "everyone who has been given much, much will be asked", "Tel Megiddo", "Livingstone", "koi", "William", "toothache", "Charlemagne", "Afghanistan", "PachelbeJ", "first base", "air sucked out from the space between its walls, creating a vacuum.", "Mark Twain", "Bonnie Prince", "Wales", "film historian", "Colombia", "Jacques Marquette", "Mistletoe", "Dodger Stadium", "Simon Cowell", "# Quiz #", "Onomastic Sobriquets", "Hanna Glawari", "New Mexico", "Library of Congress", "bulls", "Tokyo", "Exodus", "AAA", "ice fishing", "John Lennon", "\"Everyday People,\" \"Dance to the Music\" and the entirety of his funk,", "The Skyscraper Museum", "Vietcong soldiers", "ca+ion", "Beau Bridges", "gemstone quality", "depression", "Jyotirindra Basu", "artist   \u00c9douard Manet ( 1832 -- 1883 )", "Austrian", "Hobart", "John Quincy Adams", "Jackie DeShannon", "1970s", "Prince of Cambodia Norodom Sihanouk", "3 full-length computer-generated animated film", "the federal government is asleep at the switch", "a full garden and pool, a tennis court, or several heli-pads.", "19, standing 6'2\", with his auburn hair pulled back in a queue."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5205492424242424}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.8, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669]}}, "before_error_ids": ["mrqa_searchqa-validation-11639", "mrqa_searchqa-validation-2896", "mrqa_searchqa-validation-12977", "mrqa_searchqa-validation-16462", "mrqa_searchqa-validation-8008", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-1881", "mrqa_searchqa-validation-10153", "mrqa_searchqa-validation-2191", "mrqa_searchqa-validation-2135", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-14788", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-6221", "mrqa_searchqa-validation-4855", "mrqa_searchqa-validation-7639", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-309", "mrqa_searchqa-validation-2753", "mrqa_searchqa-validation-16201", "mrqa_searchqa-validation-3751", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-5052", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-4945", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-3345"], "SR": 0.453125, "CSR": 0.5622667910447761, "EFR": 0.37142857142857144, "Overall": 0.5786921974946695}, {"timecode": 67, "before_eval_results": {"predictions": ["animals", "George H.W. Bush", "Rudyard Kipling", "Spartans", "Charles Baudelaire", "a surrogate", "caution", "Melchior Lengyel", "1,065", "Lake Baikal", "Columbus", "James Madison", "the Library of Congress", "movie house", "Peter Rabbit", "Ellen Holly", "Nebraska", "the French flag", "whiskey", "saxophones", "Hipparchus", "Me Talk pretty One Day", "the cards", "Chicago", "Martha's Vineyard", "the Apprentice", "carpe diem", "Secret", "Auguste Rodin", "Catherine of Aragon", "Rugby", "the NHL", "William of Orange", "Transylvania", "the CP", "the exoskeleton", "velocity", "sugar", "Nanjing", "a circle", "the Moor of Venice", "apricots", "the 15 Biggest Mergers of All Time", "tennis scoring system", "Tour de France", "Grambling State University", "pharaoh", "spontaneous", "a Stetson.", "Vietnam", "mask", "1799", "Andreas Vesalius", "Marty J. Walsh", "weekly", "Le Grand Casino de Monte-Carlo", "SW19", "1968", "2,664", "Armando Iannucci", "Joe Lieberman, I-Connecticut,", "38 feet", "the vaccine, approved in 2006, is recommended for girls around 11 or 12.", "XVideos"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6328125}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13968", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-16629", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-15603", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-4646", "mrqa_searchqa-validation-13809", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-13386", "mrqa_searchqa-validation-10437", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-15620", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-14913", "mrqa_searchqa-validation-844", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-648", "mrqa_searchqa-validation-9988", "mrqa_searchqa-validation-3833", "mrqa_triviaqa-validation-2666", "mrqa_hotpotqa-validation-2328", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-1372"], "SR": 0.59375, "CSR": 0.5627297794117647, "EFR": 0.2692307692307692, "Overall": 0.5583452347285067}, {"timecode": 68, "before_eval_results": {"predictions": ["Santa Fe", "propeller", "\"insinuate\"", "a person who has charge of the entrance of a building and is often the owner's representative", "hexadecimal", "the Mason-Dixon Line", "Edward, the Black Prince", "a blackbird", "\"No hostage will be released until all our demands are met,\"", "Dashiell Hammett", "St. Louie", "souvlaki", "Gertrude Stein", "sports", "30 Days of Night", "daytime running lights", "One Day My Dog Stopped Loving Me", "viola", "a torpedo", "nickel", "Students for a Democratic Society", "Inland Sea", "Primates", "17th", "Vladimir Nabokov", "Los Angeles", "Braintree", "Atlanta", "Earth", "Absalom", "Rhode Island", "The Godfather", "French toast", "the Firmament", "Decree", "small", "Malawian Airlines", "Bratislava", "Pe peanut Chocolate Candies", "Henry Wadsworth", "a million", "Sweden", "John F. Kennedy", "Five Easy pieces", "Stephen Douglas", "the UNESCO's Asia-Pacific Heritage 2000 Award for Conservation", "Snips & Snails & Puppy dog tails", "Cars", "Tigers", "Thomas More", "caricatura", "After Shawn's kidnapping", "2013", "Marty J. Walsh", "Zagreb", "Norway", "Rocky Marciano", "Eternal Flame", "Boyd Gaming", "Debbie Isitt", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "the simple puzzle video game,", "30,000", "Bryant Purvis"], "metric_results": {"EM": 0.5625, "QA-F1": 0.640625}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.16666666666666669, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14114", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7323", "mrqa_searchqa-validation-10736", "mrqa_searchqa-validation-5390", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-16315", "mrqa_searchqa-validation-14903", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-13905", "mrqa_searchqa-validation-6092", "mrqa_searchqa-validation-5316", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-4549", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-9369", "mrqa_searchqa-validation-16664", "mrqa_searchqa-validation-13976", "mrqa_searchqa-validation-13744", "mrqa_searchqa-validation-10020", "mrqa_searchqa-validation-4048", "mrqa_searchqa-validation-312", "mrqa_searchqa-validation-16749", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-4931", "mrqa_hotpotqa-validation-512", "mrqa_newsqa-validation-320"], "SR": 0.5625, "CSR": 0.5627264492753623, "EFR": 0.32142857142857145, "Overall": 0.5687841291407867}, {"timecode": 69, "before_eval_results": {"predictions": ["salsa", "A Wrinkle in Time", "France", "Madagascar", "Thelma & Louise", "Nike", "Universal Studios Hollywood", "A thousand splendid suns", "plump", "Jane Eyre", "Kansas City", "Belarus", "bring your own", "Funky Town", "Bob Crane", "Kashmir", "Giovanni Bertati", "1.7", "Victor Hugo", "Helsinki", "octopuses", "Vipers", "The Importance of Being earningsest", "Austin", "Sir Edmund Hillary", "Hanna Glawari", "hot air balloons", "All the King's Men", "Martin Luther", "Canada", "carbon monoxide", "his involvement in the 2007 \"D.C. Madam\" scandal", "\"It's My Party\"", "the General Electric Company", "Sarah Hughes", "the beaver", "Ganges River", "Daylight Saving Time", "Mississippi River", "Vassar College", "switch your leaders", "Macedonia", "\"In Cold Blood\"", "apocrypha", "Antarctica", "Foil", "Judas", "the Shirley Temple Story", "Bacall", "tornado", "Bob Fosse", "3,000 metres ( 9,800 ft )", "the common name of a typical child's friend", "a contemporary drama in a rural setting", "squash", "Hans Lippershey", "George III", "YIVO", "FIFA World Cup", "Spanish", "foreplay, sexual conquests and how he picks up women", "Afghanistan", "Paul McCartney", "Francisco Pizarro"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6460193452380952}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11341", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-11068", "mrqa_searchqa-validation-15382", "mrqa_searchqa-validation-16692", "mrqa_searchqa-validation-747", "mrqa_searchqa-validation-11478", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-5517", "mrqa_searchqa-validation-5914", "mrqa_searchqa-validation-4574", "mrqa_searchqa-validation-15546", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-13088", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-8295", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-6307", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-15189", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-8294", "mrqa_newsqa-validation-2129"], "SR": 0.53125, "CSR": 0.5622767857142856, "EFR": 0.36666666666666664, "Overall": 0.5777418154761905}, {"timecode": 70, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-2535", "mrqa_hotpotqa-validation-256", "mrqa_hotpotqa-validation-2874", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-419", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-5769", "mrqa_hotpotqa-validation-762", "mrqa_hotpotqa-validation-986", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-167", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-2623", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2753", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-3896", "mrqa_naturalquestions-validation-442", "mrqa_naturalquestions-validation-4429", "mrqa_naturalquestions-validation-4643", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4883", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-6226", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6668", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6835", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6892", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-82", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-8443", "mrqa_naturalquestions-validation-8609", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1736", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1817", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1883", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-2359", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3449", "mrqa_newsqa-validation-3471", "mrqa_newsqa-validation-3538", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4049", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4181", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-863", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10147", "mrqa_searchqa-validation-10153", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-1047", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10649", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-10899", "mrqa_searchqa-validation-10995", "mrqa_searchqa-validation-11068", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-11161", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11220", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-11282", "mrqa_searchqa-validation-1141", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-11629", "mrqa_searchqa-validation-11679", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12047", "mrqa_searchqa-validation-12088", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12142", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12437", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12697", "mrqa_searchqa-validation-13281", "mrqa_searchqa-validation-13393", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13570", "mrqa_searchqa-validation-13628", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-13778", "mrqa_searchqa-validation-13782", "mrqa_searchqa-validation-13807", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-13872", "mrqa_searchqa-validation-13905", "mrqa_searchqa-validation-13923", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14036", "mrqa_searchqa-validation-1410", "mrqa_searchqa-validation-14120", "mrqa_searchqa-validation-1417", "mrqa_searchqa-validation-14174", "mrqa_searchqa-validation-14208", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-14272", "mrqa_searchqa-validation-1436", "mrqa_searchqa-validation-14463", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-14595", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14706", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14903", "mrqa_searchqa-validation-15058", "mrqa_searchqa-validation-15073", "mrqa_searchqa-validation-15122", "mrqa_searchqa-validation-15218", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-15470", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16042", "mrqa_searchqa-validation-16175", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16281", "mrqa_searchqa-validation-1646", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-1658", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16667", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-1799", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2135", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2277", "mrqa_searchqa-validation-228", "mrqa_searchqa-validation-2438", "mrqa_searchqa-validation-2597", "mrqa_searchqa-validation-2769", "mrqa_searchqa-validation-2878", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3058", "mrqa_searchqa-validation-309", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3126", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-323", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-3447", "mrqa_searchqa-validation-3552", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3724", "mrqa_searchqa-validation-3733", "mrqa_searchqa-validation-3745", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-3778", "mrqa_searchqa-validation-3780", "mrqa_searchqa-validation-3839", "mrqa_searchqa-validation-3973", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-4067", "mrqa_searchqa-validation-418", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-4492", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-4633", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4900", "mrqa_searchqa-validation-4943", "mrqa_searchqa-validation-5016", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-5192", "mrqa_searchqa-validation-5242", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5425", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5432", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5655", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-5934", "mrqa_searchqa-validation-5993", "mrqa_searchqa-validation-6014", "mrqa_searchqa-validation-612", "mrqa_searchqa-validation-6140", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-625", "mrqa_searchqa-validation-6271", "mrqa_searchqa-validation-6464", "mrqa_searchqa-validation-648", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6731", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-701", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-7866", "mrqa_searchqa-validation-7991", "mrqa_searchqa-validation-8045", "mrqa_searchqa-validation-8115", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-8132", "mrqa_searchqa-validation-8161", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8313", "mrqa_searchqa-validation-8354", "mrqa_searchqa-validation-8391", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-8973", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-9316", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9369", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9464", "mrqa_searchqa-validation-9585", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-9685", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-9791", "mrqa_squad-validation-10108", "mrqa_squad-validation-10363", "mrqa_squad-validation-1053", "mrqa_squad-validation-1207", "mrqa_squad-validation-1521", "mrqa_squad-validation-1774", "mrqa_squad-validation-1881", "mrqa_squad-validation-2014", "mrqa_squad-validation-2105", "mrqa_squad-validation-2122", "mrqa_squad-validation-2275", "mrqa_squad-validation-2546", "mrqa_squad-validation-2618", "mrqa_squad-validation-2642", "mrqa_squad-validation-2709", "mrqa_squad-validation-3000", "mrqa_squad-validation-3022", "mrqa_squad-validation-3146", "mrqa_squad-validation-3159", "mrqa_squad-validation-3552", "mrqa_squad-validation-3573", "mrqa_squad-validation-3642", "mrqa_squad-validation-3676", "mrqa_squad-validation-3858", "mrqa_squad-validation-396", "mrqa_squad-validation-4156", "mrqa_squad-validation-4204", "mrqa_squad-validation-425", "mrqa_squad-validation-4293", "mrqa_squad-validation-4646", "mrqa_squad-validation-4799", "mrqa_squad-validation-5185", "mrqa_squad-validation-5428", "mrqa_squad-validation-564", "mrqa_squad-validation-5944", "mrqa_squad-validation-6063", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6307", "mrqa_squad-validation-6366", "mrqa_squad-validation-6413", "mrqa_squad-validation-6423", "mrqa_squad-validation-659", "mrqa_squad-validation-673", "mrqa_squad-validation-6779", "mrqa_squad-validation-702", "mrqa_squad-validation-7065", "mrqa_squad-validation-7077", "mrqa_squad-validation-7314", "mrqa_squad-validation-7507", "mrqa_squad-validation-7519", "mrqa_squad-validation-7660", "mrqa_squad-validation-7745", "mrqa_squad-validation-7959", "mrqa_squad-validation-8073", "mrqa_squad-validation-8359", "mrqa_squad-validation-8426", "mrqa_squad-validation-8576", "mrqa_squad-validation-8980", "mrqa_squad-validation-8982", "mrqa_squad-validation-8987", "mrqa_squad-validation-9114", "mrqa_squad-validation-9205", "mrqa_squad-validation-9205", "mrqa_squad-validation-9336", "mrqa_squad-validation-937", "mrqa_squad-validation-9650", "mrqa_squad-validation-9691", "mrqa_squad-validation-9771", "mrqa_squad-validation-9802", "mrqa_squad-validation-9840", "mrqa_triviaqa-validation-1008", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1232", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-1935", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-2561", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2797", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3171", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-3344", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3980", "mrqa_triviaqa-validation-4008", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4043", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4257", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-573", "mrqa_triviaqa-validation-5811", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6016", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-6301", "mrqa_triviaqa-validation-6349", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-6433", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-6668", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6691", "mrqa_triviaqa-validation-6763", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7418", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-962"], "OKR": 0.697265625, "KG": 0.52421875, "before_eval_results": {"predictions": ["a malted", "Buckingham Palace", "Komodo dragon", "Pauley Pavilion", "Xaymaca", "the Lady of the Lake", "do", "the guillitine", "the polar bear", "Oslo", "Nigeria", "chicken", "a compound adjective", "cat", "Mao Zedong", "Pink", "tea", "ballpoint pen", "the guilder", "The Clouds", "Robert Louis Stevenson", "G.B.S.", "Rudyard Kipling", "China", "Lake Champlain", "Alaska", "Junior Walker", "the cedars", "iris", "a map", "the fulcrum", "vegetables", "The Five People You Meet in Heaven", "Spanish", "bluefin tuna", "a trapezoid", "neurons", "fudge", "Daniel Craig", "Ibex", "a Polaroid picture", "Canterbury", "Elizabeth I", "the Human Patient simulator", "The World According to Garp", "a \"mixing desk\"", "arsenic", "salivary glands", "Professor Higgins", "dogs", "Botswana", "Nashville, Tennessee, United States", "Cliff Richard", "Caleb", "January", "Moldova", "Her mother-in-law, Queen Mary,", "indoor ski slope", "Sir Edmund Barton, the first Prime Minister of Australia", "1620 to 1691", "Mexico", "J. Crew", "the United States, Japan, Russia, South Korea and China", "Giotto"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6297280844155845}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.5, 1.0, 0.0, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2927", "mrqa_searchqa-validation-13879", "mrqa_searchqa-validation-1447", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-6290", "mrqa_searchqa-validation-4758", "mrqa_searchqa-validation-12382", "mrqa_searchqa-validation-12949", "mrqa_searchqa-validation-14089", "mrqa_searchqa-validation-6866", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-2235", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-14591", "mrqa_searchqa-validation-11506", "mrqa_naturalquestions-validation-1904", "mrqa_triviaqa-validation-1085", "mrqa_triviaqa-validation-5437", "mrqa_hotpotqa-validation-4315", "mrqa_hotpotqa-validation-4650", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3353"], "SR": 0.59375, "CSR": 0.5627200704225352, "EFR": 0.4230769230769231, "Overall": 0.5977062736998917}, {"timecode": 71, "before_eval_results": {"predictions": ["Todd Bridges", "Charles Sherrington", "Thaddeus Rowe Luckinbill", "Copernicus", "when each of the variables is a perfect monotone function of the other", "Sanjana Ajay Pethewala", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "one body and one Spirit", "cells", "a cake or biscuit", "Fyedka", "1970", "Baaghi ( English : Rebel )", "New Zealand to New Guinea", "163", "the president", "the Caucasus region", "delta basin", "1999", "Kate Flannery", "lifts are another example, where the gondolas move on a closed ( continuous ) pulley system up and down the mountain", "1940 until 1973", "Andreas Vesalius", "Coroebus of Elis", "scythe", "a great deal on location", "Piedmont, Italy", "1983", "September 19 - 22, 2017", "1800s", "movement of some parts of the kinase domain gives free access to adenosine triphosphate ( ATP ) and the substrate to the active site", "2010", "push the food down the esophagus", "2003", "InterContinental Hotels & Resorts", "Paul Monti, whose son, Medal of Honor recipient Jared, was killed in Afghanistan while trying to save a fellow soldier", "13,000 astronomical units ( 0.21 ly )", "2001 -- 2002 season", "commissioned", "Anderton", "the root respiration", "2.45 billion years ago", "Pierre Carrier", "Maritime Provinces of Canada", "200 to 500 mg", "between 1939 and 1948", "Greenland", "Yuzuru Hanyu", "non-ferrous", "slavery", "fovea centralis", "John Logie Baird", "three", "Cilla Black", "dice", "1966", "The Grandmaster", "blind, the victim of an acid attack by a spurned suitor.", "Pakistan Taliban's chief in Punjab, according to Usman Anwar.", "Dan Parris, 25, and Rob Lehr, 26", "Winston Churchill", "Virginia", "tumbler", "Chris Evans"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5646154242697877}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.375, 0.1, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4210526315789474, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.23529411764705882, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-9697", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-2255", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-143", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-10520", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-7432", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-4067", "mrqa_naturalquestions-validation-10371", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-7358", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-5675", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-2294", "mrqa_searchqa-validation-1625"], "SR": 0.421875, "CSR": 0.5607638888888888, "EFR": 0.3783783783783784, "Overall": 0.5883753284534535}, {"timecode": 72, "before_eval_results": {"predictions": ["raven", "Istanbul", "Milton Friedman", "Oliver Goldsmith", "Susie", "Ann Widdecombe", "individual player projections and summing them up and trying, based on kind of proven mathematical models,", "Queen Elizabeth II", "Rio de Janeiro", "Eucalyptus", "Mussolini", "Croatia", "Whiskas", "cribbage", "Cameroon", "James Hanratty", "red top", "Denis Law", "Nicky Henderson", "The 'Erroneous' Number One", "Simpson", "Rebecca Adlington", "Boddington Bitter", "Pacific Ocean", "Surrealism", "Melpomene", "Cambodia", "Venezuela", "soccer", "skirts", "blue", "Van Wilder", "cancer", "a group", "the asthenosphere", "America", "The Guardian", "Turkey", "Erik Thorvaldson", "Nadia Comaneci", "Apollo", "The Match", "Exodus", "Robert Altman", "Silverstone", "Old Betsy", "Jamaica", "weekly", "Crimean Tatar", "The Kennel Club", "Rustle My Davies", "NCIS Special Agent in Charge", "Boy Meets Girl", "Superman", "English", "WAMC", "The Pogues", "Iran of trying to build nuclear bombs", "Zimbabwe's main opposition party", "Muslims around the globe, including the growing number of American Muslims, will do the same", "anvil", "the screen", "The Daily Dish", "Vienna"], "metric_results": {"EM": 0.5, "QA-F1": 0.5177083333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-280", "mrqa_triviaqa-validation-1416", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6073", "mrqa_triviaqa-validation-4331", "mrqa_triviaqa-validation-2031", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-4867", "mrqa_triviaqa-validation-3802", "mrqa_triviaqa-validation-4591", "mrqa_triviaqa-validation-2603", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-5978", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-8528", "mrqa_naturalquestions-validation-3182", "mrqa_hotpotqa-validation-2718", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1307", "mrqa_searchqa-validation-16763", "mrqa_searchqa-validation-12314"], "SR": 0.5, "CSR": 0.5599315068493151, "EFR": 0.15625, "Overall": 0.5437831763698631}, {"timecode": 73, "before_eval_results": {"predictions": ["Cold Comfort Farm", "duck", "Alfheim", "Phil Mickelson", "the USS Constitution Heavy Frigate Sailing Warship", "chat slang", "Kix", "Department of Farming and Rural Affairs", "Novak Djokovic", "Jupiter", "The Frighteners", "aircraft carrier", "bees", "Rapa Nui", "Emmy Awards", "Iceland", "red", "puffer fish", "Celsius", "tanks", "Mercury", "Terneuzen", "seven", "Pete Townshend", "Caernarfon", "Arizona Diamondbacks", "Pink Floyd", "August 1925", "Big Brother", "W. Wilson D.S.O., D.F.C.", "Yuri Alekseyevich Gagarin", "The Savoy - Hotel - London - The AA", "Jackie Robinson", "green", "Mary Poppins", "Acura", "Triboulet", "Dave", "'Hansel and Gretel' cottage", "Kempton", "Tonnabrix", "Barry Briggs", "the Jews", "Spain", "Joshua", "Jackie Kennedy", "1939", "Zephyros", "the Mongol Empire", "Jutt", "Dirty Dancing", "Eurasian Plate", "fresh nuclear fuel", "ice giants", "Michelle Anne Sinclair", "1980", "1903", "Sen. Evan Bayh", "Aryan Airlines Flight 1625", "U.S. Defense Secretary Robert Gates", "Transylvania", "Virginia", "oreos", "Haeftling"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5055059523809524}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2395", "mrqa_triviaqa-validation-2833", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-4126", "mrqa_triviaqa-validation-3712", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-2102", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-2733", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-4348", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-4667", "mrqa_triviaqa-validation-449", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-4091", "mrqa_triviaqa-validation-6529", "mrqa_triviaqa-validation-674", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-957", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-8653", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-1884", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1270", "mrqa_searchqa-validation-5920"], "SR": 0.46875, "CSR": 0.5586993243243243, "EFR": 0.17647058823529413, "Overall": 0.5475808575119236}, {"timecode": 74, "before_eval_results": {"predictions": ["Israel", "Ben Watson", "Roderic de Borja i Borja", "lily Allen", "Brian Sewell", "Karl Marx", "Lion of the Senate", "Getafix", "George III", "sulfur dioxide", "Tony Blair", "Stephen King", "sunshine after noon", "ecclesiastical", "van Diemenslandt", "Animals", "El Loco", "giraffe", "cuneiform", "18", "Peter Pan", "the Chancellor of the Exchequer", "Uranus", "divide from the other side, that is, dividing the 'two-thirds' side in half.", "blue", "Adam Smith", "saddler", "Milan", "Joanne Harris", "Matthew Boulton", "Ghana", "Thomas Cranmer", "Bush", "Emily Dickinson", "Jeremy Bates", "New Zealand", "Hadrian", "Charlie Drake", "mercury", "W. Wilson D.S.O., D.F.C.", "Doris Lessing", "sushi", "Paris", "Chile", "Scotland", "Birmingham", "origami", "Croatia", "Jules Verne", "jodhpurs", "India", "Eddie Van Halen", "Christopher Lloyd", "U.S. Supreme Court's decision in Chisholm v. Georgia", "Danny Elfman", "Humberside Airport", "Polish-Jewish", "Ronald Reagan UCLA Medical Center", "1996", "at checkposts and military camps in the Mohmand agency, part of the lawless Federally Administered Tribal Areas", "silk", "Madonna", "sponges", "New Jersey"], "metric_results": {"EM": 0.625, "QA-F1": 0.6764093137254902}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.823529411764706, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1483", "mrqa_triviaqa-validation-4461", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-3458", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-2490", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-2386", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-748", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-6224", "mrqa_naturalquestions-validation-82", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-4834", "mrqa_searchqa-validation-15941"], "SR": 0.625, "CSR": 0.5595833333333333, "EFR": 0.20833333333333334, "Overall": 0.5541302083333333}, {"timecode": 75, "before_eval_results": {"predictions": ["sheep", "blue-green", "Live and Let Die", "Elbe", "North Utsire", "Moldova", "jonquilla", "*N\u0101'\u0101lehu, Hawaii", "Sea of Galilee", "musical scale", "Greek", "1944", "scurvy", "Richard Krajicek", "Quentin Blake", "Chatsworth House", "1664", "the Mediterranean Sea", "Richard Curtis", "decorate", "Peter Blake", "Zoe Ball", "Albert Reynolds", "javelin throw", "Alice Cooper", "Squeeze", "Godfigu", "Brad Pitt", "San Francisco", "rustle My Davies", "Carly Simon", "the Swordfish", "Japanese silvergrass", "le Marseillaise", "Tripoli", "significant achievement", "30th anniversary", "bullfight", "george c Chesbro", "Charlie Chaplin", "Idaho", "the Edwin Smith papyrus", "neurons", "auction", "cosmic year", "March", "Louisiana", "Hestia", "varsol", "Touchmark Theatre, Guelph", "Stretford", "Las Vegas, Nevada", "1988", "Mr Carson", "Avoca Lodge", "14 December 1990", "Orange County", "improve health and beauty", "200", "Caylee Anthony", "hank", "Philadelphia P mint", "Clio", "50,000"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5807291666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-1711", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3652", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-1075", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-4256", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2819", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-5272", "mrqa_triviaqa-validation-5259", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-6443", "mrqa_triviaqa-validation-5730", "mrqa_naturalquestions-validation-8175", "mrqa_hotpotqa-validation-5500", "mrqa_newsqa-validation-1614", "mrqa_searchqa-validation-15714"], "SR": 0.53125, "CSR": 0.5592105263157895, "EFR": 0.23333333333333334, "Overall": 0.5590556469298246}, {"timecode": 76, "before_eval_results": {"predictions": ["Cornwall", "12", "investment spending", "Zulfikar Ali Bhutto", "Ed Miliband", "Charles Lindbergh", "red hot poker", "lirra", "Mariette", "romnis vitalba", "Pet Sounds", "Isambard Kingdom Brunel", "Crackerjack", "pool", "corrosion", "veal", "cricket", "Zephyr", "1879", "Florida", "Jerry Mouse", "Ron Paul", "12", "Philadelphia Eagles", "Fawlty Towers", "four", "egremont", "Soham", "social environment", "October", "south", "Spongebob", "Dee Caffari", "Edward FitzG Gerald", "Hogmanay", "car door", "South Dakota", "a cold-blooded terrorist.", "London", "Margaret Thatcher", "John", "Australia", "pulsar", "T.S. Eliot", "lemurs", "United", "Bosnia", "gold", "Eisenhower Executive Office Building", "Cleveland Brown", "George Lucas", "Gary Player", "Carol Ann Susi", "President of the United States", "Queensland", "Bain Capital", "Sissy Spacek", "2005", "if Turkey into any kind of engagement with the Taliban -- either as part of NATO or bilaterally -- would have much worse long-term consequences.", "Channel 4", "Mary", "Maryland", "table saw", "Neneh Cherry"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6842105263157896}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.4, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7894736842105263, 0.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2511", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-6918", "mrqa_triviaqa-validation-4596", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-4369", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-7714", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2724", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3621", "mrqa_hotpotqa-validation-5371", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-2017", "mrqa_searchqa-validation-11499"], "SR": 0.640625, "CSR": 0.5602678571428572, "EFR": 0.17391304347826086, "Overall": 0.5473830551242236}, {"timecode": 77, "before_eval_results": {"predictions": ["Cubism", "power blackout", "Kiss Me, Kate", "Helsinki", "Karl Pilkington", "Paul Keating", "Christine Keeler", "origami", "goslings", "beetle", "New York", "George VI", "skeLETON", "The Tenth Planet", "the Scot", "red", "bruise", "shoji doors", "Germany", "Catherine Cookson", "Donald Trump", "Moffitt", "Nuuk", "Oklahoma", "Ibrox Stadium", "bridge", "The Truman Show", "Paul Dukas", "the moon", "Turkey", "abacus", "Tom Waits", "porthmadog", "surrealism", "\"The Ensigns Amorial of the UK\"", "Scotland", "Tennessee Williams", "Sir Robert Walpole", "Australia", "Watchtower", "Haiti", "The Rocky and Bullwinkle Show", "Catherine of Aragon", "Robert Devereux", "Samolovich", "sledge of Sister Sledge", "snare drum", "skunk", "Joe Hart", "John Constable", "Euclid", "Michael Frankie", "Mike Alstott", "July 1, 1890", "16\u201321", "The Omega Man", "Assistant Secretary for Operations", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "marking of Ashura", "more than 100", "Lawrence Wien", "(George) Orwell", "Christopherrahimve", "two pages"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6510416666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.04761904761904762, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-5162", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-7066", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-4616", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-5282", "mrqa_triviaqa-validation-774", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-6733", "mrqa_triviaqa-validation-3956", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-310", "mrqa_triviaqa-validation-5283", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-2813", "mrqa_hotpotqa-validation-1628", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2477", "mrqa_searchqa-validation-12330", "mrqa_searchqa-validation-6268", "mrqa_searchqa-validation-4949"], "SR": 0.609375, "CSR": 0.5608974358974359, "EFR": 0.28, "Overall": 0.5687263621794871}, {"timecode": 78, "before_eval_results": {"predictions": ["unicorns", "Angela Merkel", "Montpelier", "David Copperfield", "friction", "Donald Sutherland", "st Pancras", "The Bulletin", "maidens", "Ghost", "Dutch", "lycanthropy", "Rowan Atkinson", "florida", "Sputnik", "copper", "caucausus", "Ukraine", "football", "Valentino", "maintiendrai", "Robert Plant", "a plaza in front of the Willard Hotel", "kolkheti", "Watchtower", "Agatha Christie", "steal", "she is played by Asuka Watts", "Evelyn Glennie", "light", "Sweden", "20 to 18", "Frank McCourt", "Abraham Lincoln", "The Jungle Book", "Mick Tully", "Ben Franklin", "Algeria", "United States", "John Wayne", "lace", "Bobby Vinton", "Amy Lawrence", "posh", "Venus flytrap", "liza Barth", "Right Said Fred", "Muffin Man", "frogs", "James Johnson", "Algiers", "1776", "Bo Jackson", "May 31, 2012", "the Cold War", "margarita glass", "Happy Death Day", "Franklin, Tennessee", "$1,500", "cities throughout Canada.", "Cromwell", "Abbeville", "Switzerland", "Antoni Patek"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6856770833333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2346", "mrqa_triviaqa-validation-2144", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-3059", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-2703", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-4087", "mrqa_triviaqa-validation-5833", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-4078", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-1795", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-2948", "mrqa_hotpotqa-validation-4677", "mrqa_hotpotqa-validation-2949", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3362", "mrqa_searchqa-validation-7655", "mrqa_searchqa-validation-3538"], "SR": 0.609375, "CSR": 0.5615110759493671, "EFR": 0.2, "Overall": 0.5528490901898735}, {"timecode": 79, "before_eval_results": {"predictions": ["Sicily", "Malm\u00f6", "John Constable", "Middleweight", "Henry Cooper", "1969", "eucalyptus", "New South Wales", "George Orwell", "finch", "\"Uncle Sam\"", "Northern Rock PLC", "peppercorn", "g Grover Washington Jr.", "cricket", "Budapest", "gypsum", "Adam Faith", "Zagreb", "Tanzania", "Sarawak", "Amsterdam", "Jackie Robinson", "Mark Rothko", "round brilliant", "five", "Kazakhstan", "Benedict Cumbersam and Martin Freeman", "Albert Einstein", "Aeschylus", "Maine", "horse racing", "Dunfermline Athletic", "George I, great-grandson of James I.", "burt", "Melissa Duck", "Pegasus", "Dirty Dancing", "k2", "denier", "Matalan", "Gaston Leroux", "Amnesty International", "Hugh Dowding", "kimutaingetich80", "potamos", "Mick Tully", "contact lenses", "VimtoVimto", "Cyprus", "celine's law", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s", "prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment", "$2 million", "Carol Ann Duffy", "Tim Whelan", "RAF Tangmere, West Sussex", "one of the most sought-after fugitive outside the country's rebel leaders.", "\"bystander effect\":", "cases presented by prosecutors", "log cabin", "Beijing & Lhasa", "The Cure", "Alexander Hamilton"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5943765664160401}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-4441", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-2290", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-10537", "mrqa_hotpotqa-validation-3282", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-815", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-12301"], "SR": 0.546875, "CSR": 0.561328125, "EFR": 0.27586206896551724, "Overall": 0.5679849137931035}, {"timecode": 80, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2535", "mrqa_hotpotqa-validation-256", "mrqa_hotpotqa-validation-2874", "mrqa_hotpotqa-validation-3043", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-419", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-5769", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-2623", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-3896", "mrqa_naturalquestions-validation-442", "mrqa_naturalquestions-validation-4429", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4883", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-6226", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6668", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6835", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6892", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-82", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-8443", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8609", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1736", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1817", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1883", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-2017", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2359", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4049", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4181", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-863", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10147", "mrqa_searchqa-validation-10153", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-1047", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10649", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-10899", "mrqa_searchqa-validation-10995", "mrqa_searchqa-validation-11068", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-11282", "mrqa_searchqa-validation-11363", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-11629", "mrqa_searchqa-validation-11679", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-12088", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12142", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12697", "mrqa_searchqa-validation-13281", "mrqa_searchqa-validation-13393", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13570", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-13778", "mrqa_searchqa-validation-13782", "mrqa_searchqa-validation-13807", "mrqa_searchqa-validation-13863", "mrqa_searchqa-validation-13872", "mrqa_searchqa-validation-13905", "mrqa_searchqa-validation-13923", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14036", "mrqa_searchqa-validation-1410", "mrqa_searchqa-validation-1417", "mrqa_searchqa-validation-14208", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-14272", "mrqa_searchqa-validation-1436", "mrqa_searchqa-validation-14463", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-14595", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14706", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14903", "mrqa_searchqa-validation-15058", "mrqa_searchqa-validation-15073", "mrqa_searchqa-validation-15122", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-15470", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-15941", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16042", "mrqa_searchqa-validation-16175", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16281", "mrqa_searchqa-validation-1646", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-1658", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16667", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-1799", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2135", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2277", "mrqa_searchqa-validation-228", "mrqa_searchqa-validation-2438", "mrqa_searchqa-validation-2597", "mrqa_searchqa-validation-2769", "mrqa_searchqa-validation-2878", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3126", "mrqa_searchqa-validation-323", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-3724", "mrqa_searchqa-validation-3724", "mrqa_searchqa-validation-3733", "mrqa_searchqa-validation-3745", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-3778", "mrqa_searchqa-validation-3780", "mrqa_searchqa-validation-3839", "mrqa_searchqa-validation-3973", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-4067", "mrqa_searchqa-validation-418", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4492", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-5016", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-5192", "mrqa_searchqa-validation-5242", "mrqa_searchqa-validation-5323", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5425", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5432", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5655", "mrqa_searchqa-validation-5736", "mrqa_searchqa-validation-5993", "mrqa_searchqa-validation-612", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-625", "mrqa_searchqa-validation-6271", "mrqa_searchqa-validation-648", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6731", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-6866", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-7655", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-7866", "mrqa_searchqa-validation-7991", "mrqa_searchqa-validation-8045", "mrqa_searchqa-validation-8115", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-8132", "mrqa_searchqa-validation-8161", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8313", "mrqa_searchqa-validation-8354", "mrqa_searchqa-validation-8391", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-8973", "mrqa_searchqa-validation-9316", "mrqa_searchqa-validation-9369", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9464", "mrqa_searchqa-validation-957", "mrqa_searchqa-validation-9585", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-9791", "mrqa_squad-validation-10108", "mrqa_squad-validation-10363", "mrqa_squad-validation-1053", "mrqa_squad-validation-1207", "mrqa_squad-validation-1774", "mrqa_squad-validation-1881", "mrqa_squad-validation-2014", "mrqa_squad-validation-2105", "mrqa_squad-validation-2275", "mrqa_squad-validation-2546", "mrqa_squad-validation-2618", "mrqa_squad-validation-2642", "mrqa_squad-validation-2709", "mrqa_squad-validation-3000", "mrqa_squad-validation-3022", "mrqa_squad-validation-3573", "mrqa_squad-validation-3642", "mrqa_squad-validation-3676", "mrqa_squad-validation-3858", "mrqa_squad-validation-396", "mrqa_squad-validation-4156", "mrqa_squad-validation-425", "mrqa_squad-validation-4293", "mrqa_squad-validation-4646", "mrqa_squad-validation-4799", "mrqa_squad-validation-5185", "mrqa_squad-validation-564", "mrqa_squad-validation-6063", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6307", "mrqa_squad-validation-6413", "mrqa_squad-validation-6423", "mrqa_squad-validation-659", "mrqa_squad-validation-6779", "mrqa_squad-validation-702", "mrqa_squad-validation-7065", "mrqa_squad-validation-7077", "mrqa_squad-validation-7314", "mrqa_squad-validation-7519", "mrqa_squad-validation-7745", "mrqa_squad-validation-7959", "mrqa_squad-validation-8073", "mrqa_squad-validation-8359", "mrqa_squad-validation-8426", "mrqa_squad-validation-8576", "mrqa_squad-validation-8980", "mrqa_squad-validation-8982", "mrqa_squad-validation-8987", "mrqa_squad-validation-9114", "mrqa_squad-validation-937", "mrqa_squad-validation-9650", "mrqa_squad-validation-9691", "mrqa_squad-validation-9802", "mrqa_squad-validation-9840", "mrqa_triviaqa-validation-1008", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1232", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-1909", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-2490", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2734", "mrqa_triviaqa-validation-2783", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2888", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3059", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3171", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-3304", "mrqa_triviaqa-validation-3344", "mrqa_triviaqa-validation-3464", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3980", "mrqa_triviaqa-validation-4008", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4043", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-4078", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4360", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-4564", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-4591", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5268", "mrqa_triviaqa-validation-5364", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5623", "mrqa_triviaqa-validation-5717", "mrqa_triviaqa-validation-5826", "mrqa_triviaqa-validation-5833", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-6049", "mrqa_triviaqa-validation-6062", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-6224", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-6301", "mrqa_triviaqa-validation-6349", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-6433", "mrqa_triviaqa-validation-6443", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-6668", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6691", "mrqa_triviaqa-validation-6763", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6918", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7066", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7227", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-7313", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7418", "mrqa_triviaqa-validation-7428", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7466", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-962", "mrqa_triviaqa-validation-974"], "OKR": 0.689453125, "KG": 0.53046875, "before_eval_results": {"predictions": ["Beethoven", "Margaret Thatcher", "Italy", "glass", "National Prohibition Act", "banche", "geometry", "Motel 6", "petticoat", "endosperm", "Peter Blake", "phylloxera", "1924", "Bromley-By- Bowen", "Mason Williams", "KEXP", "Moffitt", "skin care", "The Norwegian Dawn", "ear canal", "Bleak House", "100", "vince Lombardi", "Loch Morar", "Canada", "Ralph Vaughan Williams", "little Lord Fauntleroy: A Drama in Three Acts", "Outkast", "the Vulture Squadron", "technetium", "sloe berry", "Wooden Heart", "Popeye", "Eddie Shah", "meteoroids", "small gifts or knicks-knacks", "Bulldog Drummond", "Carrie", "posters", "Canada", "Sodor", "Rudyard Kipling", "igneous", "Baltic Sea", "West Sussex", "Runcorn", "Texas Governor", "St. Paul\u2019s", "rockin", "an enigma", "oleanol", "displacement", "Frank Langella", "the Vital Records Office of the states, capital district, territories and former territories", "Boulder High School in Boulder, Colorado", "6", "Ryan Seacrest", "the Southeast, with a cold high-pressure center expected to remain over the area through Saturday", "Missouri", "remains committed to British sovereignty", "Maurice Jarre", "Ireland", "a dog to pick up and hold until released, any object pointed out to him", "Taeko Ikeda"], "metric_results": {"EM": 0.5, "QA-F1": 0.5790006115047233}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.625, 0.6666666666666666, 0.0, 1.0, 0.15384615384615385, 0.0, 0.21052631578947364, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-84", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-3335", "mrqa_triviaqa-validation-316", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-5062", "mrqa_triviaqa-validation-6412", "mrqa_triviaqa-validation-3230", "mrqa_triviaqa-validation-4312", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-2648", "mrqa_triviaqa-validation-5843", "mrqa_triviaqa-validation-2400", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-6655", "mrqa_naturalquestions-validation-6998", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-4947", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-4865", "mrqa_hotpotqa-validation-1384"], "SR": 0.5, "CSR": 0.560570987654321, "EFR": 0.34375, "Overall": 0.5826610725308642}, {"timecode": 81, "before_eval_results": {"predictions": ["v", "Ohio", "Rochdale", "fungi", "hesse", "Adam Ant", "Amanda Barrie", "Hyderabad", "vince Lombardi", "Eisenhower", "smith", "steal", "Roy Plomley", "Japan", "Bodhidharma", "lorry", "Budapest", "Touchmark Theatre, Guelph", "Nixon", "fencing", "fish", "Tomb", "popped In Souled Out", "handball", "Swiss", "China", "John McCarthy", "woe", "raw beef", "vespa", "black apple", "Catherine Parr", "photography", "Munich", "Hector BERLIOZ", "falafel", "Saskatchewan", "Netherlands", "Village People", "birds", "Darwin", "Hedonismbot", "James Stewart", "cogito ergo sum", "Albert Square", "Hokkoro", "Scooby-Doo", "Timothy", "oboe", "Scooby-Doo", "nine", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "James Watson and Francis Crick", "over $1.84 billion", "Orlando\u2013Kissimmee\u2013Sanford, Florida Metropolitan Statistical Area", "Bill Curry", "The Prodigy", "psychotropic drugs", "Marines", "the \"surge\" strategy he implemented last year.", "petruchio", "potato chip", "commune", "1999"], "metric_results": {"EM": 0.5, "QA-F1": 0.5740597943722943}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.4, 0.33333333333333337, 0.5, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-1444", "mrqa_triviaqa-validation-110", "mrqa_triviaqa-validation-6443", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-2175", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-6816", "mrqa_triviaqa-validation-4636", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-4497", "mrqa_triviaqa-validation-4449", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-1294", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-4519", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-3127", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-158", "mrqa_searchqa-validation-14151", "mrqa_searchqa-validation-306"], "SR": 0.5, "CSR": 0.5598323170731707, "EFR": 0.28125, "Overall": 0.5700133384146341}, {"timecode": 82, "before_eval_results": {"predictions": ["Australia", "lirra", "Haiti", "V\u00e1clav Havel", "nitric acid", "mulberry", "jelly roll Morton", "bajec-Lapajne", "Nadia Comaneci", "Gettysburg", "Ellen DeGeneres", "bannatt", "Alan Freeman", "Motel 6", "Goldtrail", "Cilla Black", "yellow", "James Cameron", "India", "The Savoy - Hotel - London - The AA", "the Soviet Union", "The Potteries", "Annie Lennox", "12", "three", "a book", "mercury", "Jamaica", "orbit around the sun", "teeth", "Bill Bryson", "Thomas Chippendale", "biathlon", "Geoffrey Chaucer", "Jupiter", "John Steinbeck", "Tunisia", "witte de With", "diffusion", "chance", "Bjorn Borg", "Iceland", "France", "Akon", "Monaco Historic Friday 13th 14th 15th May 2016 Grand Prix", "lite d'Ivoire, Luxembourg, and Netherlands", "Little Eleanor", "jolly band linking arms and belting Italian arias", "Amy Johnson", "caiaphas", "x", "Russell Huxtable", "noble patrilineality", "benzod", "Bayern Munich", "Steve Carell", "Ten Walls", "Dr. Cade", "John Lennon and George Harrison", "Karthik Rajaram", "anthropology", "Ian Fleming", "Johnny Appleseed", "Georgia Groome"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6805194805194805}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-4348", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-2829", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-7679", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-4962", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-3270", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-4442", "mrqa_hotpotqa-validation-5791", "mrqa_searchqa-validation-10367", "mrqa_naturalquestions-validation-4784"], "SR": 0.65625, "CSR": 0.5609939759036144, "EFR": 0.045454545454545456, "Overall": 0.5230865792716319}, {"timecode": 83, "before_eval_results": {"predictions": ["squash", "collapsible support assembly", "jack-in-the-box", "World Bank", "Mendip", "Guinea", "copper", "transvestite", "Jack Mills", "Ottorino Respighi", "Melbourne", "Utrecht", "spark-ignition", "Raul Castro", "Gaston Leroux", "Kate Smith", "commitment", "Abraham Lincoln", "Belgium", "Swansea", "2000", "dantzic", "Olympic Games", "Zachary Taylor", "igneous", "beer", "Timothy Dalton", "humber", "Tina Turner", "France", "63 to 144 inches", "the Adriatic Sea", "l\u00f8gting", "Chicago", "pharynx", "pianoforte", "Meat Loaf", "sclera", "The Lion King", "\"Sugar Baby Love\"", "blood", "seven", "Paul Gauguin", "Prokofiev", "tETER-TOTTERER", "sports which are no longer present on the current program,", "Kansas City", "Sudan", "Cannes Film Festival", "Muhammad Ali", "Cambodia", "season five", "2015", "Amanda Fuller", "Columbia, Maryland", "1941", "UEFA", "Melbourne", "Afghanistan's restive provinces", "11th anniversary of the September 11, 2001, terror attacks", "stem cells", "wind-walls", "oxygen", "Tonga"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6383928571428571}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7222", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-7621", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-650", "mrqa_triviaqa-validation-1943", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-2047", "mrqa_triviaqa-validation-6373", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-3464", "mrqa_triviaqa-validation-2316", "mrqa_triviaqa-validation-2355", "mrqa_triviaqa-validation-2996", "mrqa_triviaqa-validation-3749", "mrqa_triviaqa-validation-232", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-7424", "mrqa_naturalquestions-validation-7239", "mrqa_hotpotqa-validation-5472", "mrqa_newsqa-validation-2216", "mrqa_searchqa-validation-92"], "SR": 0.59375, "CSR": 0.5613839285714286, "EFR": 0.3076923076923077, "Overall": 0.5756121222527473}, {"timecode": 84, "before_eval_results": {"predictions": ["ringway Airport", "Llandudno", "tommy Mulligan", "hula hoop", "Eddie Redmayne", "elephants", "Annie Lennox", "rugby", "yellow", "Isle of Skye", "bat", "Scharnhorst", "Southampton, England", "Cape Town", "Bristol", "Stephen Fry", "steel", "Neighbours", "Malm\u00f6", "Love Scottish", "Valentine Dyall", "winnie Mae", "Rome and Carthage", "almond", "Spain", "Strawberries", "antelope", "March 1", "Paris", "1961", "Brat Pack", "Pamplona", "William Shatner", "Becher's Brook", "pool", "tennis", "Martin Scorsese", "The Last King of Scotland", "Nile River", "Robert Taylor", "Ub Iwerks", "bridge", "The Virgin Spring", "United States", "Buckingham Palace", "David Griffiths", "congruent", "hearts", "local Defence Volunteers", "Brazil", "Cuba", "enlisted", "Emma Watson, Dan Stevens, Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Nathan Mack, Ian McKellen, and Emma Thompson", "helps scientists better understand the spread of pollution around the globe", "Terrence Jones", "Warrington Town", "$10.5 million", "Deputy Treasury Secretary", "Samoa", "Pakistan's border with Afghanistan", "Bridges of Madison County", "black hole", "Department of Energy", "senior International Correspondent Matthew Chance"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7663690476190476}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715]}}, "before_error_ids": ["mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7040", "mrqa_triviaqa-validation-5166", "mrqa_triviaqa-validation-1090", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-3929", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-5160", "mrqa_triviaqa-validation-3584", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-10161", "mrqa_hotpotqa-validation-3518", "mrqa_searchqa-validation-1337", "mrqa_newsqa-validation-4117"], "SR": 0.71875, "CSR": 0.5632352941176471, "EFR": 0.2222222222222222, "Overall": 0.5588883782679739}, {"timecode": 85, "before_eval_results": {"predictions": ["resuscitation", "yodeling", "whale", "koala", "cracker jack", "HDMI", "Thunderbirds", "Paul Simon", "Suez Canal", "Canada", "beer", "Vermont", "Jayhawk", "Hirohito", "dixiecrat", "think Big", "Croatia", "kings", "Victor Hugo", "George W. Bush", "Valium", "diCaprio", "a lighthouse", "Leatherheads", "Charleston", "coins", "chandigarh", "Ken Russell", "Queen Wilhelmina", "Alexander the Great", "chase", "goose", "rabbit", "Wu-Tang", "palaiologos", "Missouri", "cutlery", "past performance", "Salt Lake City", "A Doll's House", "love", "Blondin", "Qwerty", "smell", "bAND-AID", "diva shankar", "Juan Carlos I of Spain", "Spock", "before it starts before I begin", "Chile", "poses", "George Strait", "1999", "May 16, 2017", "La traviata", "Bleak House", "Thor", "Private Secretary and Treasurer", "400 MW", "Polihale State Park", "Nearly eight in 10", "\"Reed Between the Lines\"", "at least $20 million to $30 million", "cauliflower"], "metric_results": {"EM": 0.625, "QA-F1": 0.6527777777777778}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13767", "mrqa_searchqa-validation-14828", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-8956", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-10533", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-1981", "mrqa_searchqa-validation-4084", "mrqa_searchqa-validation-14187", "mrqa_searchqa-validation-3561", "mrqa_searchqa-validation-14072", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-8199", "mrqa_searchqa-validation-4796", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-13426", "mrqa_searchqa-validation-12565", "mrqa_searchqa-validation-3623", "mrqa_searchqa-validation-11271", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-948", "mrqa_newsqa-validation-2601"], "SR": 0.625, "CSR": 0.563953488372093, "EFR": 0.4166666666666667, "Overall": 0.5979209060077519}, {"timecode": 86, "before_eval_results": {"predictions": ["parking meters", "West Point", "Bonnie and Clyde", "engine", "Florida", "USA Today", "Mau Mau Uprising", "Warsaw", "Niccol Machiavelli", "polo", "Pennsylvania", "eggplant", "Prussia", "Vespa", "fibula", "Columbus", "Alsace", "brush", "screwdrivers Vodka", "hoof wall", "Babe Ruth", "reaping machine", "Gus Grissom", "Limerick", "1863", "Hawaii", "Dylan Thomas", "\"What hath God wrought\"", "tap", "smut smut", "Sadat", "Graceland", "abundance", "Voltaire", "Ross Perot", "pants", "Love", "Pan Am", "Supernatural", "Iran", "Conoco", "Tom DeLay", "Stephen Hawking", "Led Zeppelin", "Gollum", "Telstar 1", "Denmark", "angels", "shampoos", "David Beckham", "Chicago", "Thomas Jefferson", "Mercedes -Benz Stadium in Atlanta, Georgia", "two installments", "13", "papal state", "warblers", "Humvee", "True Williams", "Candice Susan Swanepoel", "overturned", "\"the most dangerous precedent in this country, violating all of our due process,\"", "President Bush", "Brooklyn, New York"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7691287878787878}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-7542", "mrqa_searchqa-validation-1096", "mrqa_searchqa-validation-4021", "mrqa_searchqa-validation-15618", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7092", "mrqa_searchqa-validation-5567", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-3247", "mrqa_searchqa-validation-8954", "mrqa_searchqa-validation-4766", "mrqa_searchqa-validation-6151", "mrqa_searchqa-validation-9780", "mrqa_searchqa-validation-8579", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-5185", "mrqa_triviaqa-validation-6679", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-3782", "mrqa_newsqa-validation-3584"], "SR": 0.671875, "CSR": 0.5651939655172413, "EFR": 0.42857142857142855, "Overall": 0.6005499538177339}, {"timecode": 87, "before_eval_results": {"predictions": ["Jack Shephard", "portico", "fluid dynamics", "grapefruit", "Thailand", "William Shakespeare", "9 Things You Don't Know About Tavern On the Green", "maps", "R", "pina colada dipping sauce", "varsity.com", "Judges", "Gustav", "Jordan", "Whitehorse", "DNA sequencing", "d'Artagnan", "Joseph", "shoes", "cricket", "Grover's Corner", "the Byzantine Empire", "a meal ready to eat", "parapet", "Max Planck", "wormholes", "Stalin", "ballpoint pen", "Norway", "Chevy Chase", "Annapolis", "laurie Corbett", "taffy", "Kermit Roosevelt", "Hannah Montana", "Gaelic", "a blue mussel", "Kevin Costner", "transitive", "a bolt", "fudge", "king cobra", "Priceline.com", "the Jungle Book", "scuplture", "Manhattan", "The Crow", "splinting", "McMillan & wife", "Richard Nixon", "James Hepburn", "Warren Buffett", "on a page before the start of a written work", "James I", "Seine", "Oscar Wilde", "Prussia", "1482", "Skyscraper", "George Cayley", "Osama bin Laden", "SSM Cardinal Glennon Children's Medical Center", "prostate cancer", "\"Slow\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.658984375}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8750000000000001, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16550", "mrqa_searchqa-validation-9546", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-3123", "mrqa_searchqa-validation-10951", "mrqa_searchqa-validation-10610", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-10382", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4390", "mrqa_searchqa-validation-7188", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-1699", "mrqa_searchqa-validation-8728", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-16448", "mrqa_searchqa-validation-14765", "mrqa_searchqa-validation-7145", "mrqa_searchqa-validation-14488", "mrqa_searchqa-validation-16480", "mrqa_searchqa-validation-8070", "mrqa_naturalquestions-validation-10619", "mrqa_naturalquestions-validation-9209", "mrqa_triviaqa-validation-259", "mrqa_hotpotqa-validation-4785", "mrqa_newsqa-validation-505", "mrqa_newsqa-validation-352"], "SR": 0.5625, "CSR": 0.5651633522727273, "EFR": 0.42857142857142855, "Overall": 0.6005438311688313}, {"timecode": 88, "before_eval_results": {"predictions": ["bees", "camels", "Ethiopian", "whimper", "voulge", "Epstein-Barr", "red wine", "mince pie", "balloon", "Barcelona", "Milton Glaser", "Lyme disease", "Enola Gay", "Canada", "raccoons", "the Parliamentarian cavalry", "library", "John Paul Jones", "eating poo", "depression", "1998", "comet Tempel 1", "Genesis 1:3-5", "Achaemenid", "270", "the General Electric Company", "The Crow", "hog", "a large wading bird", "National Gallery of Art", "The Waianae Mountains", "Barnard", "Once", "doughy", "America's Best Dance Crew", "The Drowsy Chaperone", "from each according to his ability", "Quebec", "Skye", "The 39 Steps", "Texas Rangers", "an APA overview", "Ocean's Twelve", "malaria", "Copacabana", "Oliver Cromwell", "Sammy Davis", "the Amazon River", "vanilla", "Nigeria", "Tennessee Williams", "Fertile Crescent ( Mesopotamia and Ancient Egypt )", "Auburn Tigers football team", "ninth w\u0101", "35", "North by Northwest", "Vancouver Island", "Taeko Ikeda", "the Port of Boston", "Christophe Lourdelet", "\"underwear bomber\" Umar Farouk AbdulMutallab", "Reid's dismissal", "to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Ethiopia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6061197916666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.12500000000000003, 0.0, 0.13333333333333333, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2983", "mrqa_searchqa-validation-15679", "mrqa_searchqa-validation-6936", "mrqa_searchqa-validation-8256", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-11078", "mrqa_searchqa-validation-10721", "mrqa_searchqa-validation-648", "mrqa_searchqa-validation-14528", "mrqa_searchqa-validation-15368", "mrqa_searchqa-validation-16503", "mrqa_searchqa-validation-16965", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-174", "mrqa_searchqa-validation-3363", "mrqa_searchqa-validation-4790", "mrqa_searchqa-validation-1985", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-4511", "mrqa_searchqa-validation-3477", "mrqa_naturalquestions-validation-5599", "mrqa_triviaqa-validation-467", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-919", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-4168"], "SR": 0.546875, "CSR": 0.5649578651685394, "EFR": 0.27586206896551724, "Overall": 0.5699608618268114}, {"timecode": 89, "before_eval_results": {"predictions": ["Punch", "West Point", "Black Beauty", "Forest Lawn", "Ralph Lauren", "opium", "Gabriele Falloppio", "Walt Disney World", "a gastropod shell", "Darfur", "Colonel Robert Hogan", "Judy Garland", "Spain and Portugal", "a riot", "Muhammad Ali", "minister of telecommunications, mines, and equipment", "Ronald Reagan Presidential Library", "the Congo", "Alsace", "Cherry Jones", "Liliuokalani", "ticker tape", "Richard M. Nixon", "comet", "Spain", "sheep", "Scrabble", "three", "Slayer", "Cherokee", "30%", "H-4 Hercules", "Joe Lieberman", "Atolls", "the College of William and. Mary", "Pullman", "apologia", "Rudyard Kipling", "brakes", "white", "tendonitis", "Universal Studios", "Union Carbide", "Idaho", "Colombia", "Salem witch trials", "Thornton Wilder", "Louisiana", "(P Pablo) Neruda", "Oklahoma", "the Revolutionary War", "the Eurasian Plate", "China in the 8th century ( Nara Period )", "Ralph Northam", "rivers", "Theodore Roosevelt", "Charlotte", "Jerry Michael Glanville", "Ardeth Bay", "Lakshmibai", "President Bill Clinton", "an antihistamine and an epinephrine auto-injector for emergencies", "1519", "beetle"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5952537593984962}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.21052631578947367, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.2857142857142857, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-3767", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-2907", "mrqa_searchqa-validation-8261", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11219", "mrqa_searchqa-validation-13619", "mrqa_searchqa-validation-13632", "mrqa_searchqa-validation-5330", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-15071", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2658", "mrqa_searchqa-validation-3470", "mrqa_searchqa-validation-103", "mrqa_searchqa-validation-1141", "mrqa_searchqa-validation-6052", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-9110", "mrqa_naturalquestions-validation-10648", "mrqa_triviaqa-validation-1925", "mrqa_hotpotqa-validation-43", "mrqa_hotpotqa-validation-3064", "mrqa_hotpotqa-validation-1664", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-476"], "SR": 0.484375, "CSR": 0.5640625, "EFR": 0.30303030303030304, "Overall": 0.5752154356060606}, {"timecode": 90, "UKR": 0.7734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2535", "mrqa_hotpotqa-validation-256", "mrqa_hotpotqa-validation-2874", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-419", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-5244", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5769", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-2623", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3323", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-3896", "mrqa_naturalquestions-validation-442", "mrqa_naturalquestions-validation-4429", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4883", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-6226", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6668", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6835", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6892", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8105", "mrqa_naturalquestions-validation-82", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-8443", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8609", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1736", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1817", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-2017", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2359", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2581", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-4049", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4181", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-863", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10153", "mrqa_searchqa-validation-10200", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1047", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-10610", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10649", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-10995", "mrqa_searchqa-validation-11068", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-11282", "mrqa_searchqa-validation-11363", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-11629", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-12088", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12142", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12697", "mrqa_searchqa-validation-13281", "mrqa_searchqa-validation-13352", "mrqa_searchqa-validation-13393", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-13782", "mrqa_searchqa-validation-13807", "mrqa_searchqa-validation-13872", "mrqa_searchqa-validation-13905", "mrqa_searchqa-validation-13923", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14036", "mrqa_searchqa-validation-14072", "mrqa_searchqa-validation-1410", "mrqa_searchqa-validation-1417", "mrqa_searchqa-validation-14208", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-14272", "mrqa_searchqa-validation-1436", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-14463", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14706", "mrqa_searchqa-validation-15058", "mrqa_searchqa-validation-15071", "mrqa_searchqa-validation-15073", "mrqa_searchqa-validation-15122", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-15470", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-15936", "mrqa_searchqa-validation-15941", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16175", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16281", "mrqa_searchqa-validation-1646", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16667", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-174", "mrqa_searchqa-validation-1799", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2277", "mrqa_searchqa-validation-228", "mrqa_searchqa-validation-2438", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2769", "mrqa_searchqa-validation-2878", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3126", "mrqa_searchqa-validation-323", "mrqa_searchqa-validation-3247", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-3724", "mrqa_searchqa-validation-3733", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-3778", "mrqa_searchqa-validation-3780", "mrqa_searchqa-validation-3839", "mrqa_searchqa-validation-3973", "mrqa_searchqa-validation-4067", "mrqa_searchqa-validation-418", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-4492", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-5016", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-5192", "mrqa_searchqa-validation-5242", "mrqa_searchqa-validation-5323", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5425", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5432", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5624", "mrqa_searchqa-validation-5655", "mrqa_searchqa-validation-5940", "mrqa_searchqa-validation-6052", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-625", "mrqa_searchqa-validation-6271", "mrqa_searchqa-validation-648", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6679", "mrqa_searchqa-validation-6731", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-6835", "mrqa_searchqa-validation-6866", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-7866", "mrqa_searchqa-validation-7991", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8045", "mrqa_searchqa-validation-8115", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-8132", "mrqa_searchqa-validation-8161", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8313", "mrqa_searchqa-validation-8354", "mrqa_searchqa-validation-8391", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-8954", "mrqa_searchqa-validation-8973", "mrqa_searchqa-validation-9316", "mrqa_searchqa-validation-9369", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-9464", "mrqa_searchqa-validation-9481", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-957", "mrqa_searchqa-validation-9585", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-9791", "mrqa_searchqa-validation-9951", "mrqa_squad-validation-10363", "mrqa_squad-validation-1207", "mrqa_squad-validation-1774", "mrqa_squad-validation-1881", "mrqa_squad-validation-2014", "mrqa_squad-validation-2105", "mrqa_squad-validation-2275", "mrqa_squad-validation-2546", "mrqa_squad-validation-2618", "mrqa_squad-validation-2642", "mrqa_squad-validation-2709", "mrqa_squad-validation-3000", "mrqa_squad-validation-3022", "mrqa_squad-validation-3573", "mrqa_squad-validation-3642", "mrqa_squad-validation-3676", "mrqa_squad-validation-3858", "mrqa_squad-validation-396", "mrqa_squad-validation-4156", "mrqa_squad-validation-425", "mrqa_squad-validation-4646", "mrqa_squad-validation-4799", "mrqa_squad-validation-5185", "mrqa_squad-validation-6063", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6307", "mrqa_squad-validation-6413", "mrqa_squad-validation-6423", "mrqa_squad-validation-659", "mrqa_squad-validation-6779", "mrqa_squad-validation-702", "mrqa_squad-validation-7065", "mrqa_squad-validation-7077", "mrqa_squad-validation-7519", "mrqa_squad-validation-7745", "mrqa_squad-validation-7959", "mrqa_squad-validation-8073", "mrqa_squad-validation-8359", "mrqa_squad-validation-8426", "mrqa_squad-validation-8980", "mrqa_squad-validation-8982", "mrqa_squad-validation-8987", "mrqa_squad-validation-9114", "mrqa_squad-validation-937", "mrqa_squad-validation-9802", "mrqa_squad-validation-9840", "mrqa_triviaqa-validation-1008", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-110", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1232", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-1671", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1859", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-2490", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2783", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2888", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3059", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3139", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-3304", "mrqa_triviaqa-validation-3344", "mrqa_triviaqa-validation-3464", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3980", "mrqa_triviaqa-validation-4008", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4043", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-4078", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4564", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-4666", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5166", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-5479", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5623", "mrqa_triviaqa-validation-5717", "mrqa_triviaqa-validation-5826", "mrqa_triviaqa-validation-5833", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-6049", "mrqa_triviaqa-validation-6062", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-6301", "mrqa_triviaqa-validation-6349", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-6433", "mrqa_triviaqa-validation-6443", "mrqa_triviaqa-validation-6443", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-6668", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6691", "mrqa_triviaqa-validation-6763", "mrqa_triviaqa-validation-6774", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6878", "mrqa_triviaqa-validation-6918", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7066", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7110", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7181", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7227", "mrqa_triviaqa-validation-7231", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7402", "mrqa_triviaqa-validation-7428", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7466", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-817", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-962", "mrqa_triviaqa-validation-974"], "OKR": 0.689453125, "KG": 0.528125, "before_eval_results": {"predictions": ["My Girls", "Oklahoma City", "kookaburra", "Catch Me if You Can", "Vancouver Island", "Ghirardelli", "Emancipation Proclamation", "Cleopatra's Needle", "the Iberas del Estero", "Stalingrad", "the Unknown Soldier", "Grand Canal", "M16A2", "pioneers", "Samuel Johnson", "the Blue Ridge Mountains", "Pearl Jam", "John F. Kennedy", "heron", "lights", "erosion", "Warren Burger", "peace", "Jamaica", "Civil War", "a final contest", "Kentucky Wildcats", "Aegean Sea", "tannins", "Bottlenose Dolphin", "Princess Anne", "CAKES", "us", "Peter Piper", "Borneo", "a trumpet", "South Africa", "fish", "Green Mountains", "Queens", "collard Greens", "Cloverfield", "Ohio State", "HIV/AIDS", "Maroc Soir", "Hudson", "Mitch Albom", "topless doughnut shop", "Panama Canal", "Mount Kenya", "midget", "19 July 1990", "benzodidatesines", "annuity", "Winnie Mae", "Japan", "willow", "Atl\u00e9tico Madrid", "Venice", "Al-Masjid an-Nabawi", "U.S. senators", "pesos", "war crimes and crimes against humanity", "Wars of the Roses"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6407986111111111}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.4444444444444444, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-4157", "mrqa_searchqa-validation-4906", "mrqa_searchqa-validation-13230", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-3798", "mrqa_searchqa-validation-5374", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-6936", "mrqa_searchqa-validation-12684", "mrqa_searchqa-validation-16203", "mrqa_searchqa-validation-1110", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-6521", "mrqa_searchqa-validation-566", "mrqa_searchqa-validation-16743", "mrqa_searchqa-validation-15031", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-3789", "mrqa_triviaqa-validation-873", "mrqa_hotpotqa-validation-3384", "mrqa_hotpotqa-validation-2460", "mrqa_hotpotqa-validation-4413", "mrqa_newsqa-validation-2114"], "SR": 0.578125, "CSR": 0.564217032967033, "EFR": 0.25925925925925924, "Overall": 0.5628983834452586}, {"timecode": 91, "before_eval_results": {"predictions": ["earreach", "New Zealand", "Are from Venus: The Classic Guide to Understanding the Opposite Sex", "tarantula", "Philadelphia", "Europe", "(Giacomo) Puccini", "\"An Old-Fashioned Wedding,\"", "Greece", "shrews", "cardiology & Heart surgery", "the Terracotta Army", "honey", "#jonlindsey on Tofo.me", "J! Archive", "the Cape of Good Hope", "Constantine", "Switzerland", "Van Halen", "beef", "Archer Daniels Midland", "Sigmund Freud", "baleen", "Pompeii", "Road to Perdition", "Mussolini", "Tuesday", "Tiger Woods", "John Sedges", "New York", "American Motors", "miners", "Circus Animal cookies", "CANDU reactor", "Luxembourg", "Mississippi", "John Adam Belushi", "butterfly", "sea turtles", "Complexion", "1712-1786", "spider", "The Bingo Kids Sing", "napalm", "white wine", "World War II", "batteries", "plasma", "How Green Was My Valley", "John Lennon", "performance", "Jesse Frederick James Conaway", "Kelli Goss", "The British later lost the Battle of Sainte - Foy west of Quebec ( 1760 )", "Susie Dent", "Thailand", "Stockholm", "1992", "The Tuskegee Study of Untreated Syphilis in the Negro Male", "David Starkey", "skull", "Pew Research Center", "1620", "Nirvana and Kiss"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6419730392156863}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11370", "mrqa_searchqa-validation-2184", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10059", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-13938", "mrqa_searchqa-validation-10985", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-1720", "mrqa_searchqa-validation-9030", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-1818", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-16071", "mrqa_searchqa-validation-16270", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-923", "mrqa_hotpotqa-validation-1733", "mrqa_newsqa-validation-3729", "mrqa_triviaqa-validation-3545"], "SR": 0.609375, "CSR": 0.5647078804347826, "EFR": 0.32, "Overall": 0.5751447010869566}, {"timecode": 92, "before_eval_results": {"predictions": ["Golf", "Spielberg", "corporal punishment", "Margaret Mitchell", "the Globe", "Channel Tunnel", "John Adams", "the Fur trapper", "Glasgow", "lobotomy", "the South", "Nothing without Providence", "Mississippi River", "abacus", "Chekhov", "Roger Maris", "Estonia, Latvia and Lithuania", "Japan", "Robber barons", "the Constitution", "\"Twelfth Night\"", "El Zorro", "Federalist Papers", "Molson", "gymnastics", "Foot Locker", "homeostasis", "Iceland", "Ireland", "President George W. Bush", "disaccharides", "the leaves", "size zero", "Egypt", "Alabama", "Cincinnati", "the United States Department of Transportation", "Meyer Lansky", "South Carolina", "Uranus", "gangrene", "Falconer", "van der Rohe", "Kyoto", "American Sign Language", "8 ball pool", "horses", "the Wasatch Mountains", "hot milk", "Munich", "Australia and the Anzacs", "herd maintenance", "Pure Java driver", "Robert Hooke", "1879", "ear", "Faversham", "aging issues", "Anabolic steroids", "Blue Grass Airport", "the cancellation of more than 650 flights at London's Heathrow airport", "Turkey", "3 to 17", "Ralph Cifaretto"], "metric_results": {"EM": 0.59375, "QA-F1": 0.66796875}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3201", "mrqa_searchqa-validation-7356", "mrqa_searchqa-validation-5581", "mrqa_searchqa-validation-11371", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-3872", "mrqa_searchqa-validation-6121", "mrqa_searchqa-validation-8535", "mrqa_searchqa-validation-16778", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-4428", "mrqa_searchqa-validation-10435", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-5560", "mrqa_searchqa-validation-14403", "mrqa_searchqa-validation-6288", "mrqa_searchqa-validation-10825", "mrqa_searchqa-validation-16355", "mrqa_searchqa-validation-9712", "mrqa_searchqa-validation-650", "mrqa_searchqa-validation-2548", "mrqa_naturalquestions-validation-3732", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-185"], "SR": 0.59375, "CSR": 0.5650201612903225, "EFR": 0.2692307692307692, "Overall": 0.5650533111042183}, {"timecode": 93, "before_eval_results": {"predictions": ["Levi's", "Hospital Corporation of America", "Qatar", "Gonzales County", "Turin", "( Jose de San) Martin", "half-staff", "the Chablis", "Tel Aviv", "Sufism", "Torah", "Lincoln", "a volcano", "Flyleaf", "superde delegates", "Norway", "the Green Mountains", "Hiroshima", "justice", "Sabrina", "the Communist Party", "Martha Washington", "Gavin MacLeod", "William Tell", "Professor", "bananas", "Little Men", "Buenos Aires", "dawdle", "William IV, Duke of Clarence", "nasi lemak", "\"Hey Joe\"", "General Lee", "Ann Richards", "Compensatory Damages", "Five Easy pieces", "oreos", "guitar", "Crossword Solver", "Switzerland", "Ayn Rand", "Sue Miller", "New Kids on the Block", "TOLEDO", "deckhand", "Pablo Escobar", "Ricardo Sanchez Robert Gates", "rabbit", "New Kids on the Block", "Jason", "superstring", "The Only difference Between Martyrdom and Suicide Is Press Coverage", "a stray wandering the streets of Moscow. Soviet scientists chose to use Moscow strays since they assumed that such animals had already learned to endure conditions of extreme cold and hunger", "Yuzuru Hanyu", "Easter", "Dick Whittington", "Albania", "Borwick railway station", "Bea Arthur", "110 miles (177 km)", "84-year", "Starz", "Michael Jackson", "pine-apple"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6088541666666667}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15313", "mrqa_searchqa-validation-15111", "mrqa_searchqa-validation-14999", "mrqa_searchqa-validation-12583", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-2427", "mrqa_searchqa-validation-10114", "mrqa_searchqa-validation-16408", "mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-6253", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-6496", "mrqa_searchqa-validation-3202", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-8153", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-12044", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-10621", "mrqa_naturalquestions-validation-2857", "mrqa_naturalquestions-validation-1443", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-1021", "mrqa_newsqa-validation-1135"], "SR": 0.5625, "CSR": 0.5649933510638299, "EFR": 0.25, "Overall": 0.561201795212766}, {"timecode": 94, "before_eval_results": {"predictions": ["tccic", "Wodehouse", "Viggo Mortensen", "Cana of Galilee", "the Prince of Denmark", "dinoflagellates", "Laurence", "Panama Canal", "Informatics", "Gary Coleman", "Mount Moriah", "Oman", "a guitar", "60", "the Argonauts", "President Lincoln", "Jenna Bush", "a stop sign", "Hillary Clinton", "oysters", "Elvis Presley", "Slovakia", "a battalion", "Milan", "Princess Grace", "electric clothes dryer", "Constantine", "The Trump International Hotel & Tower", "Kennedy Space Center", "James Boswell", "liver cancer", "Elizabeth Taylor", "the Rhine & the Main", "Hiroshima", "the beehive", "Marathon Man", "Taxi Driver", "Tombs of Kobol", "Calvin Coolidge", "Mussolini", "time", "Islamabad", "Kinshasa", "ice cream sundae", "a lynx", "a Permanent Select Committee", "David Lynch", "large", "Beethoven", "Scooter Libby", "a condor", "Bartolomeu Dias", "Carpenter", "Marie Van Brittan Brown", "human rights lawyer", "Clive Hornby", "William Boyd", "Tainted Love", "H. R. Giger", "current chair", "Sonia Sotomayor, Sonia's father", "opium has accounted for more than half of Afghanistan's gross domestic product in 2007", "Somali", "Amanda Fuller"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6634415064102563}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.75, 0.07692307692307691, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5720", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-692", "mrqa_searchqa-validation-9901", "mrqa_searchqa-validation-15742", "mrqa_searchqa-validation-12240", "mrqa_searchqa-validation-9396", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-10423", "mrqa_searchqa-validation-4549", "mrqa_searchqa-validation-9514", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-12029", "mrqa_searchqa-validation-14259", "mrqa_searchqa-validation-3300", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-11273", "mrqa_searchqa-validation-10740", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-9675", "mrqa_hotpotqa-validation-48", "mrqa_hotpotqa-validation-3888", "mrqa_newsqa-validation-2529", "mrqa_newsqa-validation-2172", "mrqa_newsqa-validation-536"], "SR": 0.5625, "CSR": 0.564967105263158, "EFR": 0.42857142857142855, "Overall": 0.5969108317669173}, {"timecode": 95, "before_eval_results": {"predictions": ["Arkansas", "ginger", "Jodocus", "cramming", "Odysseus", "Gregorian", "parchment", "1863", "barcode", "rings", "TaB", "troll", "Ivanhoe", "the Shar'ia penal code", "the General Assembly", "Chief of Staff", "Red", "Chile", "Charles Manson", "Coldplay", "Central Park Zoo", "choke Point", "Tales from the Vienna Woods", "Rhode Island", "the grapevine", "Spain", "ultralight", "Rodgers & Hammerstein", "17th", "Mr. chips", "Raggedy Ann", "The Naked Gun", "Baton Rouge", "Canada", "Yuri Gagarin", "lazo", "Planned Parenthood", "S.W.A.T", "Frank Sinatra", "bronchodilator", "alveoli", "Kbec", "Melissa Etheridge", "Crimea", "Bob Falfa", "a duke", "Sappho", "Mary, Queen of Scots", "glucocorticoids", "Don Diego de la Vega", "Minerva", "October 2008", "Exodus 20 : 7", "Big Boi and Sleepy Brown", "Samoa", "John Sullivan", "Hard Candy Fitness", "Miss Universe", "December 24, 1973", "sarod", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "Haeftling", "Fargo, North Dakota", "Surtsey"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7712282509157509}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-15502", "mrqa_searchqa-validation-9121", "mrqa_searchqa-validation-4254", "mrqa_searchqa-validation-5282", "mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-863", "mrqa_searchqa-validation-1794", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4992", "mrqa_searchqa-validation-5316", "mrqa_searchqa-validation-9867", "mrqa_searchqa-validation-5131", "mrqa_searchqa-validation-6816", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-7138", "mrqa_hotpotqa-validation-3679", "mrqa_newsqa-validation-4032"], "SR": 0.71875, "CSR": 0.5665690104166667, "EFR": 0.3333333333333333, "Overall": 0.5781835937500001}, {"timecode": 96, "before_eval_results": {"predictions": ["Peter Pan", "Walter Raleigh", "William Henry Harrison", "Mario Puzo", "Bartholomew Cubbins", "Bones", "The Polar Express", "English", "Will Rogers", "the Science", "Nevermind", "Satan", "the Me Me Me Generation", "a prophet of God", "prisonthe", "Pearl", "The Twinkle Tales", "the Apennines Mountains", "\"The \"T\"", "Fidel Castro", "Mikhail Baryshinski", "a colloid", "Fettuccine", "Fahrenheit 451", "KLM", "pour some sugar on me", "Spanglish", "the Great Wall of Rhodes", "the beurre mani", "General William Devereaux", "the Indigo Girls", "can we get along?", "Buddhism", "Casablanca", "Herbert George Wells", "impact basin", "Madonna", "Benjamin Harrison", "Dirty Rotten Scoundrels", "porcupines", "the Boston Tea Party", "State the Chemical Element", "Neeson", "Puccini", "the green-eyed monster", "claymore", "nuclear", "Plutarch", "Steven Wright", "cantatas", "New Balance", "United Nations Peacekeeping Operations", "Sebastian V tyres", "Castleford is a town in the metropolitan borough of Wakefield, West Yorkshire, England", "curling", "the Dalton Gang", "Kiki", "the late 1960s and early 1970s", "DJ Premier, engineer, Junior Makhno, Snowgoons", "Che Guevara", "Amber Room", "Hurricane Gustav", "Raymond Thomas", "Owen Vaccaro"], "metric_results": {"EM": 0.5, "QA-F1": 0.5789930555555556}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.16666666666666669, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11364", "mrqa_searchqa-validation-6719", "mrqa_searchqa-validation-1275", "mrqa_searchqa-validation-15715", "mrqa_searchqa-validation-3746", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-15894", "mrqa_searchqa-validation-39", "mrqa_searchqa-validation-3643", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-10843", "mrqa_searchqa-validation-11277", "mrqa_searchqa-validation-6630", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-2852", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-3225", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-5011", "mrqa_searchqa-validation-597", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-8757", "mrqa_hotpotqa-validation-1380", "mrqa_hotpotqa-validation-3536", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1965"], "SR": 0.5, "CSR": 0.5658827319587629, "EFR": 0.375, "Overall": 0.5863796713917526}, {"timecode": 97, "before_eval_results": {"predictions": ["tongue", "football", "darts", "Holiday Inn", "William Pitt the Younger", "rugby", "an Inuit", "Columbus Day", "Hernando de Soto", "Thomas Jefferson", "cases", "porcupines", "a baffle", "a kitchen", "signatures", "arteries", "leeward", "BMW", "Legally Blonde", "Jimmy Carter", "Val Kilmer", "The Crucible", "arrested Development", "U.S. Army Lieutenant William Eaton", "Liberty", "David Geffen", "the Sacred Cod", "1/3", "licorice stick", "liquid crystal displays", "John Carpenter", "the Sarajevo Haggadah", "the U.S. battleship Maine", "a call option", "hollandaise sauce", "nag", "the North Canadian River", "Atlas Shrugged", "Dermatology", "The Star-Spangled Banner", "Bunker Hill", "Dreamgirls", "a vessel", "masks", "Eliza Doolittle", "FIFA World Cup", "Celine Dion", "Adam", "a bazooka", "Buenos Aires Herald", "John Glenn", "Sylvester Stallone", "eleven", "228 minutes", "Jack Nicholson", "tera", "Bonnie and Clyde", "Germany", "1958", "1948", "Samson D'Souza", "future relations between the Middle East and Washington", "Adam Lambert and Kris Allen", "1996"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6231770833333333}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6263", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-6585", "mrqa_searchqa-validation-12276", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-3225", "mrqa_searchqa-validation-14959", "mrqa_searchqa-validation-14937", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-9636", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-7119", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-7142", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-6512", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-327", "mrqa_newsqa-validation-576"], "SR": 0.578125, "CSR": 0.5660076530612245, "EFR": 0.48148148148148145, "Overall": 0.6077009519085411}, {"timecode": 98, "before_eval_results": {"predictions": ["a computer", "blue blood", "Quasimodo", "King Philip", "U.S. Department of the Interior", "the Nazi Olympics Berlin 1936", "coelacanth", "the Crystal", "Skull and crossbones", "the Cape Hatteras Lighthouse", "chiaroscuro", "The Communist Manifesto", "Haile Selassie", "the distillation", "a scalene triangle", "a sub-adult raccoon", "Little Miss Sunshine", "Applebee's", "Goodyear", "the small intestine", "the International Space Station", "Roman Polanski", "a ruby", "New South Wales", "Duncan", "Nancy Lopez", "Matthew", "Blofeld", "The Age of Innocence", "a pearl", "cholera", "PR", "James McConkey", "Yentl", "Neptune", "a duvet", "Sergio Garcia", "Bo Diddley", "Victoria Victoria", "Matt Leinart", "Rodents", "Georgia-Pacific", "Warsaw", "Buckingham Palace", "Roger Ebert", "Tammy Wynette", "the Coast Guard Reserve Home Page", "Miami", "the United Nations Atomic Energy Commission (UNAEC)", "alex", "Balaam", "ideology", "in the vascular bundles", "accomplish the objectives of the organization", "5 through 9", "Ray Combs", "Anita Roddick", "Preston, Lancashire, UK", "Helensvale", "Over forty", "Pakistani officials", "renew registration until the manufacturer's fix has been made", "London Health Sciences Centre", "AS Roma beat Lecce 3-2"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6629464285714286}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_searchqa-validation-10956", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-16582", "mrqa_searchqa-validation-14429", "mrqa_searchqa-validation-3706", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-13423", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-11665", "mrqa_searchqa-validation-11401", "mrqa_searchqa-validation-6709", "mrqa_searchqa-validation-9695", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-8811", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-11186", "mrqa_searchqa-validation-15560", "mrqa_naturalquestions-validation-8220", "mrqa_triviaqa-validation-4230", "mrqa_triviaqa-validation-38", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3930", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-2513"], "SR": 0.5625, "CSR": 0.5659722222222222, "EFR": 0.39285714285714285, "Overall": 0.589968998015873}, {"timecode": 99, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1265", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-2493", "mrqa_hotpotqa-validation-2535", "mrqa_hotpotqa-validation-256", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2874", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-419", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5200", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-5762", "mrqa_hotpotqa-validation-5769", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1028", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2504", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-3896", "mrqa_naturalquestions-validation-442", "mrqa_naturalquestions-validation-4429", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4883", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-6226", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6668", "mrqa_naturalquestions-validation-6765", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-6835", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-6892", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7359", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8105", "mrqa_naturalquestions-validation-82", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-8443", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8609", "mrqa_naturalquestions-validation-861", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-1192", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1736", "mrqa_newsqa-validation-1773", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1817", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-2017", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2359", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2581", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3603", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-4049", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4181", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-863", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-10200", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1047", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-10610", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10649", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11068", "mrqa_searchqa-validation-11074", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-11282", "mrqa_searchqa-validation-11363", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-1159", "mrqa_searchqa-validation-11629", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-11676", "mrqa_searchqa-validation-11778", "mrqa_searchqa-validation-12041", "mrqa_searchqa-validation-12088", "mrqa_searchqa-validation-12108", "mrqa_searchqa-validation-12142", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12301", "mrqa_searchqa-validation-12324", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12691", "mrqa_searchqa-validation-12697", "mrqa_searchqa-validation-13352", "mrqa_searchqa-validation-13393", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13470", "mrqa_searchqa-validation-13494", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-13726", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-13782", "mrqa_searchqa-validation-13872", "mrqa_searchqa-validation-13905", "mrqa_searchqa-validation-13923", "mrqa_searchqa-validation-14036", "mrqa_searchqa-validation-14059", "mrqa_searchqa-validation-14072", "mrqa_searchqa-validation-1410", "mrqa_searchqa-validation-1417", "mrqa_searchqa-validation-14190", "mrqa_searchqa-validation-14208", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-14272", "mrqa_searchqa-validation-1436", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14706", "mrqa_searchqa-validation-15058", "mrqa_searchqa-validation-15071", "mrqa_searchqa-validation-15073", "mrqa_searchqa-validation-15122", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15267", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15309", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-15470", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-15742", "mrqa_searchqa-validation-15894", "mrqa_searchqa-validation-15936", "mrqa_searchqa-validation-15941", "mrqa_searchqa-validation-160", "mrqa_searchqa-validation-16175", "mrqa_searchqa-validation-16234", "mrqa_searchqa-validation-16281", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-1646", "mrqa_searchqa-validation-16464", "mrqa_searchqa-validation-16553", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16675", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16791", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-174", "mrqa_searchqa-validation-1799", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-204", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2277", "mrqa_searchqa-validation-228", "mrqa_searchqa-validation-2434", "mrqa_searchqa-validation-2438", "mrqa_searchqa-validation-2503", "mrqa_searchqa-validation-2769", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3126", "mrqa_searchqa-validation-323", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-3339", "mrqa_searchqa-validation-3623", "mrqa_searchqa-validation-3643", "mrqa_searchqa-validation-3724", "mrqa_searchqa-validation-3733", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-3778", "mrqa_searchqa-validation-3780", "mrqa_searchqa-validation-3839", "mrqa_searchqa-validation-3973", "mrqa_searchqa-validation-4067", "mrqa_searchqa-validation-4173", "mrqa_searchqa-validation-418", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4492", "mrqa_searchqa-validation-4616", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-4814", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-5016", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-5192", "mrqa_searchqa-validation-5242", "mrqa_searchqa-validation-5252", "mrqa_searchqa-validation-5369", "mrqa_searchqa-validation-5425", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5432", "mrqa_searchqa-validation-5481", "mrqa_searchqa-validation-5504", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-5624", "mrqa_searchqa-validation-5655", "mrqa_searchqa-validation-5779", "mrqa_searchqa-validation-5940", "mrqa_searchqa-validation-6052", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-6240", "mrqa_searchqa-validation-625", "mrqa_searchqa-validation-648", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6679", "mrqa_searchqa-validation-6705", "mrqa_searchqa-validation-6731", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-6866", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-7119", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7277", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-7866", "mrqa_searchqa-validation-7991", "mrqa_searchqa-validation-8011", "mrqa_searchqa-validation-8045", "mrqa_searchqa-validation-8115", "mrqa_searchqa-validation-8120", "mrqa_searchqa-validation-8153", "mrqa_searchqa-validation-8161", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8313", "mrqa_searchqa-validation-8354", "mrqa_searchqa-validation-8391", "mrqa_searchqa-validation-8392", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8535", "mrqa_searchqa-validation-8572", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8707", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-8835", "mrqa_searchqa-validation-8954", "mrqa_searchqa-validation-8973", "mrqa_searchqa-validation-9297", "mrqa_searchqa-validation-9316", "mrqa_searchqa-validation-9464", "mrqa_searchqa-validation-9481", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-957", "mrqa_searchqa-validation-9585", "mrqa_searchqa-validation-9629", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-9695", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-9791", "mrqa_searchqa-validation-9914", "mrqa_searchqa-validation-9951", "mrqa_searchqa-validation-9992", "mrqa_squad-validation-10363", "mrqa_squad-validation-1207", "mrqa_squad-validation-1774", "mrqa_squad-validation-1881", "mrqa_squad-validation-2014", "mrqa_squad-validation-2105", "mrqa_squad-validation-2275", "mrqa_squad-validation-2546", "mrqa_squad-validation-2618", "mrqa_squad-validation-2642", "mrqa_squad-validation-2709", "mrqa_squad-validation-3000", "mrqa_squad-validation-3022", "mrqa_squad-validation-3573", "mrqa_squad-validation-3642", "mrqa_squad-validation-3676", "mrqa_squad-validation-3858", "mrqa_squad-validation-396", "mrqa_squad-validation-4156", "mrqa_squad-validation-4646", "mrqa_squad-validation-4799", "mrqa_squad-validation-5185", "mrqa_squad-validation-6063", "mrqa_squad-validation-6175", "mrqa_squad-validation-6197", "mrqa_squad-validation-6307", "mrqa_squad-validation-6413", "mrqa_squad-validation-6423", "mrqa_squad-validation-659", "mrqa_squad-validation-6779", "mrqa_squad-validation-702", "mrqa_squad-validation-7065", "mrqa_squad-validation-7077", "mrqa_squad-validation-7745", "mrqa_squad-validation-7959", "mrqa_squad-validation-8073", "mrqa_squad-validation-8359", "mrqa_squad-validation-8426", "mrqa_squad-validation-8980", "mrqa_squad-validation-8982", "mrqa_squad-validation-8987", "mrqa_squad-validation-9114", "mrqa_squad-validation-9802", "mrqa_squad-validation-9840", "mrqa_triviaqa-validation-1008", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-110", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1232", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-1359", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1505", "mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-1671", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-2476", "mrqa_triviaqa-validation-2490", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2783", "mrqa_triviaqa-validation-2864", "mrqa_triviaqa-validation-2888", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3059", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-3106", "mrqa_triviaqa-validation-3139", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-3304", "mrqa_triviaqa-validation-3344", "mrqa_triviaqa-validation-3464", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3980", "mrqa_triviaqa-validation-4008", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4043", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-4078", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-4253", "mrqa_triviaqa-validation-4298", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4564", "mrqa_triviaqa-validation-4587", "mrqa_triviaqa-validation-4666", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-4752", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5122", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5187", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-5479", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-5623", "mrqa_triviaqa-validation-5717", "mrqa_triviaqa-validation-5826", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6000", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-6049", "mrqa_triviaqa-validation-6062", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6209", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-6301", "mrqa_triviaqa-validation-6349", "mrqa_triviaqa-validation-6367", "mrqa_triviaqa-validation-6405", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-6433", "mrqa_triviaqa-validation-6443", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6582", "mrqa_triviaqa-validation-6598", "mrqa_triviaqa-validation-6668", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6691", "mrqa_triviaqa-validation-6763", "mrqa_triviaqa-validation-6774", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6878", "mrqa_triviaqa-validation-6918", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7066", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7110", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-7200", "mrqa_triviaqa-validation-7231", "mrqa_triviaqa-validation-7270", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7428", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-7466", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-817", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-962", "mrqa_triviaqa-validation-974"], "OKR": 0.705078125, "KG": 0.5375, "before_eval_results": {"predictions": ["Rossini", "change color", "the slide", "a Bachelor's degree and a Master's degree in Accounting", "Jane Eyre", "a vessel", "Sri Lanka", "major", "the red fox", "Band of Brothers", "Frogs", "Armistice", "the great horned owl", "the asylum", "the North Atlantic Treaty Organization", "vacuum tubes", "Deviled Eggs", "Jane Eyre", "indirect printing", "Palatine", "Martin Luther", "Sweeney Todd: The Demon Barber of Fleet Street", "Jaws", "Jekyll", "fauxpas", "morphine", "a Armageddon", "the Black Forest", "Sarah Hughes", "the Viking Ship Museum", "the hypothalamus", "Apple", "Heated", "Constantine", "Rosa Parks", "Anne Moore", "Mars", "Latter-day", "Alcoa", "Fort Knox", "Pennsylvania", "Ernest Lawrence", "Edward Hopper", "malaria", "Zanzibar", "Wizard of Oz", "Uganda", "Ted", "Heinrich Olbers", "YEAH", "Eminem", "California, Utah and Arizona", "the Chicago metropolitan area", "Norman origin", "a hobby", "George III", "Augustus", "Paris", "1998", "nine", "Australia and New Zealand", "Greeley, Colorado", "grizzly bear", "various"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6182291666666666}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-5617", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-8927", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1685", "mrqa_searchqa-validation-7368", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8584", "mrqa_searchqa-validation-2198", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-5630", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-5763", "mrqa_searchqa-validation-8251", "mrqa_searchqa-validation-9400", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-11387", "mrqa_naturalquestions-validation-440", "mrqa_triviaqa-validation-7014", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-3780"], "SR": 0.546875, "CSR": 0.56578125, "EFR": 0.27586206896551724, "Overall": 0.5746567887931036}]}